<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd"><html><head><meta charset="windows-1252" /><meta http-equiv="Content-Type" content="text/html; charset=windows-1252"><meta name="viewport" content="width=device-width, initial-scale=1"><title>SC18 Proceedings</title><link href="includes/css/jquery-ui.css" rel="stylesheet" type="text/css" /><link href="includes/css/shared_styles.css" rel="stylesheet" type="text/css" /><link href="includes/css/block_styles.css?v=1" rel="stylesheet" type="text/css" /><link href="includes/css/jquery.qtip.min.css" rel="stylesheet" type="text/css" /><link href="includes/css/font-awesome-4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css" /><link href="includes/css/user_generated.css" rel="stylesheet" type="text/css" /><link href="archive_styles.css" rel="stylesheet" type="text/css" /><style>

        .tabs {
            background: #005EB8;
        }
        .tabs .divider,
        .tabs .bg_tab {
            background-color: #005EB8;
            border-top-color: #005EB8;
            color: #ffffff;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        .tabs .fg_tab {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            color: #00205B;
            text-transform: none;
        }
        .tab_menu_label, .tab_no_menu_label {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .qtip.qtip-rm-tab-menu {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        


        .filter_bar,
        .filter_bar_w_legend {
            background-color: #ffffff;
        }
        .filter_bar_w_legend .instr,
        .filter_bar .instr {
            background-color: #deedf7;
        }
        


        .role_stype_bar {
            background-color: #00205B;
            color: #FFFFFF;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        


        #footer {
            background-color: #A6192E;
            color: #ffffff;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        #footer a {
            color: #92C1E9;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        


        div.banner_top,
        div.banner_top .site_title,
        div.banner_top .no_logo_banner_right,
        div.logo_banner,
        div.logo_banner .user_name,
        #header {
            background-color: #92C1E9;
            color: #00205B;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        


        a:link,
        a:visited,
        a:active,
        .clickable,
        a.clickable,
        a.clickable:link,
        a.clickable:visited,
        a.clickable:active,
        .ttip_object_info_blue,
        .ttip_object_info_blue_no_clone,
        .ttip_object_info_blue_wide,
        .ttip_object_info_blue_wide_no_clone,
        .ttip_object_info_blue_very_wide,
        .ttip_object_info_blue_very_wide_no_clone,
        .ttip_object_info_blue_extra_wide,
        .ttip_object_info_blue_extra_wide_no_clone,
        .ttip_object_info_blue_modal,
        .ttip_object_info_blue_modal_no_clone,
        .colorbox_object_info,
        span.menu_item_label,
        .page_box_print .contents A,
        .page_box_print #footer a   { color: #0000EE; }

        /* Light Links */
        .light_link a,
        .light_arrow,
        .light_link a:link,
        .light_link a:active,
        .light_link a:visited,
        .light_clickable,
        a.light_clickable,
        a.light_clickable:link,
        a.light_clickable:active,
        a.light_clickable:visited   { color: #5088F0; }

        /* user hovers     */
        a:hover,
        .light_link a:hover,
        .light_arrow:hover,
        .light_clickable:hover,
        a.light_clickable:hover,
        .hover_link:hover,
        .ttip_object_info_blue:hover,
        .ttip_object_info_blue_no_clone:hover,
        .ttip_object_info_blue_wide:hover,
        .ttip_object_info_blue_wide_no_clone:hover,
        .ttip_object_info_blue_very_wide:hover,
        .ttip_object_info_blue_very_wide_no_clone:hover,
        .ttip_object_info_blue_extra_wide:hover,
        .ttip_object_info_blue_extra_wide_no_clone:hover,
        .ttip_object_info_blue_modal:hover,
        .ttip_object_info_blue_modal_no_clone:hover,
        .ttip_object_info:hover,
        .ttip_object_info_no_clone:hover,
        .ttip_object_info_wide:hover,
        .ttip_object_info_wide_no_clone:hover,
        .ttip_object_info_very_wide:hover,
        .ttip_object_info_very_wide_no_clone:hover,
        .ttip_object_info_extra_wide:hover,
        .ttip_object_info_extra_wide_no_clone:hover,
        .ttip_object_info_modal:hover,
        .ttip_object_info_modal_no_clone:hover,
        .colorbox_object_info:hover,
        .subtabs .fg_tab:hover div,
        .subtabs .fg_tab:hover A,
        .subtabs .bg_tab:hover,
        .subtabs .bg_tab:hover A        { color: #0000EE; }

        ul.rm_mega_menu li.mega > div,
        ul.rm_mega_menu > li.mega-link > a:hover,
        .disp_details_header,
        .disp_details_sub_header,
        .disp_details I,    /* This is deprecated, since it clashes with font awesome using I tags. */
        .disp_red,
        .disp_label {
            color: #A6192E;
        }


        


        #related_col .block-title {
            background-color: #005EB8;
            color: #ffffff;
        }
        #related_col .block-title a {
            color: #0000FF;
        }
        #related_col .block-content {
            background-color: #ffffff;
        }
        #related_col .block-content .instr {
            background-color: #f2f8fc;
        }
        #related_col .block-content .odd {
            background-color: #e6eefa;
        }
        #related_col .block-content .even {
            background: #c9ddf8;
        }
        


        .contents .output_box .title {
            background-color: #005EB8;
        }
        .block-content,
        .block2-content,
        .contents .output_box table tr th,
        .contents .output_box {
            background-color: #FFFFFF;
        }
        .output_box_instr,
        .contents .output_box .instr,
        .block .instr,
        .block-content .instr {
            background-color: #f2f8fc;
        }
        .odd,
        .contents .output_box .odd,
        .block-content .odd {
            background-color: #e6eefa;
        }
        .even,
        .contents .output_box .even,
        .block-content .even {
            background-color: #c9ddf8;
        }
        


        .tabs .fg_tab {
            background-color: #92C1E9;
            color: #00205B;
            border-bottom-color: #92C1E9;
        }
        .tab_menu_label:hover, .active .tab_menu_label,
        .tab_no_menu_label:hover,
        .tab_no_menu_label:hover a {
            color: #00205B;
        }
        .subtabs {
            background-color: #92C1E9;
            color: #00205B;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        .subtabs .divider,
        .subtabs .bg_tab,
        .subtabs .bg_tab a {
            background-color: #92C1E9;
            border-top-color: #92C1E9;
            color: #00205B;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        .subtabs .fg_tab {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            color: #00205B;
            text-transform: none;
        }

        /*
        uncomment this to make the subtabs follow the selected tab color instead
        of the link color

        .subtabs .bg_tab:hover a {
            color: #00205B;
        }
        .subtabs .fg_tab:hover a, {
            color: #00205B;
        }
        */

        .subtabs .fg_tab a {
            color: #00205B;
        }
        


        .documentation_box {
            background: #ffffff;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        


        body.in_iframe {
            background-color: #f2f8fc;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .pagedoc {
            background-color: #f2f8fc;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .page_box {
            background-color: #f2f8fc;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .page_box_in_iframe {
            background-color: #f2f8fc;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .contents {
            background-color: #f2f8fc;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .contents_options {
            background-color: #f2f8fc;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        #top-links {
            background-color: #f2f8fc;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .fullscreen {
            background-color: #f2f8fc;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .subtabs .fg_tab {
            border-bottom-color: #f2f8fc;
        }
        .subtabs .fg_tab div {
            background-color: #f2f8fc;
            border-bottom-color: #f2f8fc;
        }
        .fullscreen_schedule {
            background-color: #f2f8fc;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .contents input,
        .contents input_box,
        .contents textarea {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .qtip.rm-qtip {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        #cboxContent {
            background-color: #f2f8fc;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        /* For now, use the main site background color for tool tips. */
        .qtip.qtip-rm,
        .qtip.qtip-rm .qtip-titlebar {
            background-color: #f2f8fc;
        }

        /* Not sure where this should live. */
        #actions_col .block-title-text {
            font-size: 15px;
        }
        #related_col .block-title-text {
            font-size: 15px;
        }

        /* For jquery-ui. */
        .ui-widget {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        .arrow-slidedown {
            background-color: #f2f8fc;
            color: #000000;
        }
        


        .contents .input .title,
        .contents .input_box .title {
            background-color: #005EB8;
        }
        .contents .input,
        .contents .input_box,
        .contents .input table tr th,
        .contents .input_box table tr th,
        .form .block-content {
            background-color: #FFFFFF;
        }
        .contents .input .instr,
        .contents .input_box .instr,
        .multi_block_button,
        .form .block .instr {
            background-color: #f2f8fc;
        }
        .contents .input .odd,
        .contents .input_box .odd,
        .form .block-content .odd {
            background-color: #e6eefa;
        }
        .contents .input .even,
        .contents .input_box .even,
        .form .block-content .even {
            background-color: #c9ddf8
        }
        


        div.rm_mega_menus_container,
        ul.rm_mega_menu.darker,
        ul.rm_mega_menu > li.mega > a,
        ul.rm_mega_menu.darker > li.mega > a,
        ul.rm_mega_menu > li.mega-link > a,
        ul.rm_mega_menu.darker > li.mega-link > a,
        ul.rm_mega_menu > li.mega-label > span {
            background-color: #A6192E;
            border-color: #A6192E;
            color: #FFFFFF;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        ul.rm_mega_menu > li.mega.selected > a {
            background-color: #A6192E;
            border-color: #00205B;
            color: #FFFFFF;
        }
        ul.rm_mega_menu > li.mega:hover > a {
            color: #00205B;
        }
        div.rm_mega_menus_container ul.rm_mega_menu > li.mega > div.menu_dropdown {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .contents input,
        .contents input_box,
        .contents textarea {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        


        #actions_col .block-title {
            background-color: #005EB8;
            color: #FFFFFF;
        }
        #actions_col .block-title a {
            color: #0000FF;
        }
        #actions_col .block-content {
            background-color: #FFFFFF;
        }
        #actions_col .block-content .instr {
            background-color: #f2f8fc;
        }
        #actions_col .block-content .odd {
            background-color: #e6eefa;
        }
        #actions_col .block-content .even {
            background: #c9ddf8;
        }
        </style><script src="includes/jquery/jquery-1.8.3.min.js" type="text/javascript"></script><script src="includes/jquery/jquery-ui.min.js" type="text/javascript"></script><script src="includes/jquery/jquery.hoverIntent.minified.js" type="text/javascript"></script><script src="includes/jquery/basic.js" type="text/javascript"></script><script src="includes/jquery/jquery.qtip.min.js" type="text/javascript"></script><script src="includes/jquery/jquery.highlight-4.js"></script><script src="includes/jquery/jquery.instaFilter.js"></script><script type="text/javascript">$(function() { setup_info_links(); });</script><script src="includes/jquery/user_generated.js"></script><script type="text/javascript">
    $(document).ready(function(){
        $program_sections = $("#sections-container");
        $("#program_filter").InstaFilter($program_sections, {
            typing_pause: 500,
            search_unit_selector: ".slot-entry",
            search_unit_ancestor_fields_selector: ".session-title, .session-chair, .room-name"
        });

        // this ensures there is enough vertical space below the search field to keep the
        // page from scrolling during a search
        $program_sections.css("min-height", $(window).height());
    });
    </script></head><body><a name="top"></a><div class="centered"><div><div style="float: left;"></div><div style="float: left; margin-top: 15px; height: 80px;"><span class="page-title">SC18 Proceedings</span></div><div style="clear: both;"></div></div></div><br /><div class="centered"><br /><span class="page-links"><a href="at_a_glance.html">Overview</a></span> | <span class="page-links"><a href="by_sub_type.html">By Event Type</a></span> | <span class="page-links">By Tag</span> | <span class="page-links"><a href="by_auth.html">Author Index</a></span><br /></div><br /><div id="main-content-box"><div class="centered"><table class="cellspacing10px"><tr><td><div class="anchor-link"><a href="#ptrack104">Algorithms</a></div></td><td><div class="anchor-link"><a href="#ptrack125">Graph Algorithms</a></div></td><td><div class="anchor-link"><a href="#ptrack151">Resiliency</a></div></td></tr><tr><td><div class="anchor-link"><a href="#ptrack105">Applications</a></div></td><td><div class="anchor-link"><a href="#ptrack129">I/O</a></div></td><td><div class="anchor-link"><a href="#ptrack185">Resource Management</a></div></td></tr><tr><td><div class="anchor-link"><a href="#ptrack106">Architectures</a></div></td><td><div class="anchor-link"><a href="#ptrack132">Linear Algebra</a></div></td><td><div class="anchor-link"><a href="#ptrack172">Scheduling</a></div></td></tr><tr><td><div class="anchor-link"><a href="#ptrack108">Clouds and Distributed Computing</a></div></td><td><div class="anchor-link"><a href="#ptrack133">Machine Learning</a></div></td><td><div class="anchor-link"><a href="#ptrack155">Scientific Computing</a></div></td></tr><tr><td><div class="anchor-link"><a href="#ptrack112">Compiler Analysis and Optimization</a></div></td><td><div class="anchor-link"><a href="#ptrack134">Memory</a></div></td><td><div class="anchor-link"><a href="#ptrack156">Security</a></div></td></tr><tr><td><div class="anchor-link"><a href="#ptrack171">Computational Biology</a></div></td><td><div class="anchor-link"><a href="#ptrack186">MPI</a></div></td><td><div class="anchor-link"><a href="#ptrack158">Sparse Computation</a></div></td></tr><tr><td><div class="anchor-link"><a href="#ptrack173">Computational Physics</a></div></td><td><div class="anchor-link"><a href="#ptrack135">NVRAM</a></div></td><td><div class="anchor-link"><a href="#ptrack159">State of the Practice</a></div></td></tr><tr><td><div class="anchor-link"><a href="#ptrack191">Cosmology</a></div></td><td><div class="anchor-link"><a href="#ptrack137">Networks</a></div></td><td><div class="anchor-link"><a href="#ptrack160">Storage</a></div></td></tr><tr><td><div class="anchor-link"><a href="#ptrack115">Data Analytics</a></div></td><td><div class="anchor-link"><a href="#ptrack190">OpenMP</a></div></td><td><div class="anchor-link"><a href="#ptrack163">System Software</a></div></td></tr><tr><td><div class="anchor-link"><a href="#ptrack116">Data Management</a></div></td><td><div class="anchor-link"><a href="#ptrack140">Parallel Programming Languages, Libraries, and Models</a></div></td><td><div class="anchor-link"><a href="#ptrack170">Tools</a></div></td></tr><tr><td><div class="anchor-link"><a href="#ptrack118">Deep Learning</a></div></td><td><div class="anchor-link"><a href="#ptrack141">Performance</a></div></td><td><div class="anchor-link"><a href="#ptrack166">Visualization</a></div></td></tr><tr><td><div class="anchor-link"><a href="#ptrack123">File Systems</a></div></td><td><div class="anchor-link"><a href="#ptrack142">Power</a></div></td><td><div class="anchor-link"><a href="#ptrack168">Workflows</a></div></td></tr><tr><td><div class="anchor-link"><a href="#ptrack188">Floating Point</a></div></td><td><div class="anchor-link"><a href="#ptrack189">Precision</a></div></td><td><div class="anchor-link"><a href="#ptrack179">Tech Program Reg Pass</a></div></td></tr><tr><td><div class="anchor-link"><a href="#ptrack183">GPUs</a></div></td><td><div class="anchor-link"><a href="#ptrack147">Programming Systems</a></div></td><td><div class="anchor-link"><a href="#other">Other</a></div></td></tr></table><br /><hr /></div><div class="righted"><input id="program_filter" name="program_filter" placeholder="search" size="40" type="text" /></div><div class="centered" id="sections-container"><table><tr><td align="left"><div class="area-section"><div class="centered"><a name="ptrack104"></a><div class="section-title">Algorithms</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Computational Biology, Scientific Computing, Tech Program Reg Pass</span><br /><div class="session-title">Biology Applications</div><div class="slot-entry"><a name="pap410"></a><div class="slot-title">Extreme Scale De Novo Metagenome Assembly</div><div><span class="BP award">Best Paper Finalists</span></div><div class="slot-authors">Evangelos Georganas (Intel Corporation) and Rob Egan, Steven Hofmeyr, Eugene Goltsman, Bill Arndt, Andrew Tritt, Aydin Buluc, Leonid Oliker, and Katherine Yelick (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_150_1539825877_66" onclick="$('#vhsjs_view_150_1539825877_66').hide();
                $('#vhsjs_hide_150_1539825877_66').show();
                $('#149_1539825877_66').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_150_1539825877_66" onclick="$('#149_1539825877_66').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_150_1539825877_66').hide();
                $('#vhsjs_view_150_1539825877_66').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="150_1539825877_66" id="149_1539825877_66" style="display: none"><div class="arrow-slidedown"><blockquote>Metagenome assembly is the process of transforming a set of short, overlapping, and potentially erroneous DNA segments from environmental samples into the accurate representation of the underlying microbiomes's genomes. State-of-the-art tools require large shared memory machines and cannot handle contemporary metagenome datasets that exceed terabytes in size. In this paper, we introduce the metaHipMer pipeline, a high-quality and high-performance metagenome assembler that employs an iterative de Bruijn graph approach. MetaHipMer leverages a specialized scaffolding algorithm that produces long scaffolds and accommodates the idiosyncrasies of metagenomes. MetaHipMer is end-to-end parallelized using the Unified Parallel C language and therefore can run seamlessly on shared and distributed-memory systems. Experimental results show that metaHipMer matches or outperforms the state-of-the-art tools in terms of accuracy. Moreover, metaHipMer scales efficiently to large concurrencies and is able to assemble previously intractable grand challenge metagenomes.</blockquote></div></div></div></div><a href="includes/files/pap410s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap551"></a><div class="slot-title">Optimizing High Performance Distributed Memory Parallel Hash Tables for DNA k-mer Counting</div><div class="slot-authors">Tony C. Pan (Georgia Institute of Technology, School of Computational Science and Engineering); Sanchit Misra (Intel Corporation, Parallel Computing Lab); and Srinivas Aluru (Georgia Institute of Technology, School of Computational Science and Engineering)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_152_1539825877_66" onclick="$('#vhsjs_view_152_1539825877_66').hide();
                $('#vhsjs_hide_152_1539825877_66').show();
                $('#151_1539825877_66').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_152_1539825877_66" onclick="$('#151_1539825877_66').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_152_1539825877_66').hide();
                $('#vhsjs_view_152_1539825877_66').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="152_1539825877_66" id="151_1539825877_66" style="display: none"><div class="arrow-slidedown"><blockquote>High-throughput DNA sequencing is the mainstay of modern genomics research. A common operation used in bioinformatic analysis for many applications of high-throughput sequencing is the counting and indexing of fixed length substrings of DNA sequences called k-mers. Counting k-mers is often accomplished via hashing, and distributed memory k-mer counting algorithms for large data sets are memory access and network communication bound. In this work, we present two optimized distributed parallel hash table techniques that utilize cache friendly algorithms for local hashing, overlapped communication and computation to hide communication costs, and vectorized hash functions that are specialized for k-mer and other short key indices. On 4096 cores of the NERSC Cori supercomputer, our implementation completed index construction and query on an approximately 1 TB human genome dataset in just 11.8 seconds and 5.8 seconds, demonstrating speedups of 2.06x and 3.7x, respectively, over the previous state-of-the-art distributed memory k-mer counter.</blockquote></div></div></div></div><a href="includes/files/pap551s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap120"></a><div class="slot-title">Redesigning LAMMPS for Petascale and Hundred-Billion-Atom Simulation on Sunway TaihuLight</div><div class="slot-authors">Xiaohui Duan, Ping Gao, Tingjian Zhang, Meng Zhang, and Weiguo Liu (Shandong University); Wusheng Zhang, Wei Xue, Haohuan Fu, Lin Gan, and Dexun Chen (Tsinghua University); Xiangxu Meng (Shandong University); and Guangwen Yang (Tsinghua University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_154_1539825877_66" onclick="$('#vhsjs_view_154_1539825877_66').hide();
                $('#vhsjs_hide_154_1539825877_66').show();
                $('#153_1539825877_66').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_154_1539825877_66" onclick="$('#153_1539825877_66').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_154_1539825877_66').hide();
                $('#vhsjs_view_154_1539825877_66').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="154_1539825877_66" id="153_1539825877_66" style="display: none"><div class="arrow-slidedown"><blockquote>Large-scale molecular dynamics (MD) simulations on supercomputers play an increasingly important role in many research areas. In this paper, we present our efforts on redesigning the widely used LAMMPS MD simulator for Sunway TaihuLight supercomputer and its ShenWei many-core architecture (SW26010). The memory constraints of SW26010 bring a number of new challenges for achieving efficient MD implementation on it. In order to overcome these constraints, we employ four levels of optimization: (1) a hybrid memory update strategy; (2) a software cache strategy; (3) customized transcendental math functions; and (4) a full pipeline acceleration. Furthermore, we redesign the code to enable all possible vectorization. Experiments show that our redesigned software on a single SW26010 processor can outperform over 100 E5-2650 cores for running the latest stable release (11Aug17) of LAMMPS. We also achieve a performance of over 2.43 PFlops for a Tersoff simulation when using 16,384 nodes on Sunway TaihuLight.</blockquote></div></div></div></div><a href="includes/files/pap120s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, Data Analytics, Deep Learning, Networks, Scientific Computing, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Large-Scale Algorithms</div><div class="slot-entry"><a name="pap171"></a><div class="slot-title">Large-Scale Hierarchical K-Means for Heterogeneous Many-Core Supercomputers</div><div class="slot-authors">Liandeng Li (Tsinghua University; National Supercomputing Center, Wuxi); Teng Yu (University of St Andrews); Wenlai Zhao and Haohuan Fu (Tsinghua University; National Supercomputing Center, Wuxi); Chenyu Wang (University of St Andrews; National Supercomputing Center, Wuxi); Li Tan (Beijing Technology and Business University); Guangwen Yang (Tsinghua University; National Supercomputing Center, Wuxi); and John Thomson (University of St Andrews)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_156_1539825877_66" onclick="$('#vhsjs_view_156_1539825877_66').hide();
                $('#vhsjs_hide_156_1539825877_66').show();
                $('#155_1539825877_66').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_156_1539825877_66" onclick="$('#155_1539825877_66').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_156_1539825877_66').hide();
                $('#vhsjs_view_156_1539825877_66').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="156_1539825877_66" id="155_1539825877_66" style="display: none"><div class="arrow-slidedown"><blockquote>This paper presents a novel design and implementation of k-means clustering algorithm targeting the Sunway TaihuLight supercomputer. We introduce a multi-level parallel partition approach that not only partitions by dataflow and centroid, but also by dimension. Our multi-level (nkd) approach unlocks the potential of the hierarchical parallelism in the SW26010 heterogeneous many-core processor and the system architecture of the supercomputer. <br><br>Our design is able to process large-scale clustering problems with up to 196,608 dimensions and over 160,000 targeting centroids, while maintaining high performance and high scalability, significantly improving the capability of k-means over previous approaches. The evaluation shows our implementation achieves performance of less than 18 seconds per iteration for a large-scale clustering case with 196,608 data dimensions and 2,000 centroids by applying 4,096 nodes (1,064,496 cores) in parallel, making k-means a more feasible solution for complex scenarios.</blockquote></div></div></div></div><a href="includes/files/pap171s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap140"></a><div class="slot-title">TriCore: Parallel Triangle Counting on GPUs</div><div class="slot-authors">Yang Hu (George Washington University); Hang Liu (University of Massachusetts, Lowell); and H. Howie Huang (George Washington University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_158_1539825877_66" onclick="$('#vhsjs_view_158_1539825877_66').hide();
                $('#vhsjs_hide_158_1539825877_66').show();
                $('#157_1539825877_66').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_158_1539825877_66" onclick="$('#157_1539825877_66').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_158_1539825877_66').hide();
                $('#vhsjs_view_158_1539825877_66').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="158_1539825877_66" id="157_1539825877_66" style="display: none"><div class="arrow-slidedown"><blockquote>Triangle counting algorithm enumerates the triangles in a graph by identifying the common neighbors between two vertices of every edge. In this work, we present TriCore, a new GPU-based high-performance and scalable triangle counting system that consists of three main techniques. First, we design a binary search based counting algorithm that tremendously increases both thread parallelism and memory performance. Second, TriCore exploits a 2-D partition method to distribute the CSR representation across multiple GPUs, combined with a new streaming buffer to load the edge list from outside of GPUs. Third, we develop a dynamic workload management technique to balance the workload across multiple GPUs. Our evaluation demonstrates TriCore is 22× faster than the state-of-the-art parallel triangle counting projects. In addition, TriCore can not only process big graphs that are significant larger than the memory size of one GPU but also achieve 24× speedup when scaling to 32 GPUs.</blockquote></div></div></div></div><a href="includes/files/pap140s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap141"></a><div class="slot-title">Distributed-Memory Hierarchical Compression of Dense SPD Matrices</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Chenhan D. Yu (University of Texas), Severin Reiz (Technical University Munich), and George Biros (University of Texas)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_160_1539825877_66" onclick="$('#vhsjs_view_160_1539825877_66').hide();
                $('#vhsjs_hide_160_1539825877_66').show();
                $('#159_1539825877_66').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_160_1539825877_66" onclick="$('#159_1539825877_66').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_160_1539825877_66').hide();
                $('#vhsjs_view_160_1539825877_66').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="160_1539825877_66" id="159_1539825877_66" style="display: none"><div class="arrow-slidedown"><blockquote>We present a distributed-memory algorithm for the hierarchical compression of SPD matrices. Our method is based on GOFMM, an algorithm that appeared in doi:10.1145/3126908.3126921.<br><br>For many SPD matrices, GOFMM enables compression and approximate matrix-vector multiplication in NlogN time---as opposed to quadratic work required for a dense matrix. But GOFMM supports only shared memory parallelism. In this paper, we use the message passing interface, extending the ideas of GOFMM to the distributed memory setting. We also introduce an asynchronous algorithm for faster multiplication. We present different usage scenarios of SPD matrices that are related to graphs, neural-networks, and covariance operators. We also compare with STRUMPACK, which, to our knowledge, is the only other parallel software that can compress arbitrary SPD matrices. In our largest run, we were able to compress a 67M-by-67M matrix within three minutes and perform a multiplication with 512 vectors within 5 seconds on 6,144 Intel Skylake cores.</blockquote></div></div></div></div><a href="includes/files/pap141s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Graph Algorithms, Linear Algebra, Machine Learning, Sparse Computation, Tech Program Reg Pass</span><br /><div class="session-title">Algorithms on Sparse Data</div><div class="slot-entry"><a name="pap511"></a><div class="slot-title">HiCOO: Hierarchical Storage of Sparse Tensors</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Jiajia Li, Jimeng Sun, and Richard Vuduc (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_162_1539825877_67" onclick="$('#vhsjs_view_162_1539825877_67').hide();
                $('#vhsjs_hide_162_1539825877_67').show();
                $('#161_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_162_1539825877_67" onclick="$('#161_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_162_1539825877_67').hide();
                $('#vhsjs_view_162_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="162_1539825877_67" id="161_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>This paper proposes a new storage format for sparse tensors, called Hierarchical COOrdinate (HiCOO; pronounced: “haiku”). It derives from coordinate (COO) format, arguably the de facto standard for general sparse tensor storage. HiCOO improves upon COO by compressing the indices in units of sparse tensor blocks, with the goals of preserving the “mode-agnostic” simplicity of COO while reducing the bytes needed to represent the tensor and promoting data locality. We evaluate HiCOO by implementing a single-node, multicore-parallel version of the matricized tensor-times-Khatri-Rao product (MTTKRP) operation, which is the most expensive computational core in the widely used CANDECOMP/PARAFAC decomposition(CPD) algorithm. This MTTKRP implementation achieves up to 23.0× (6.8× on average) speedup over COO format and up to 15.6× (3.1× on average) speedup over another state-of-the-art format, compressed sparse fiber (CSF), by using less or comparable storage of them. When used within CPD, we also observe speedups against COO- and CSF-based implementations.</blockquote></div></div></div></div><a href="includes/files/pap511s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap273"></a><div class="slot-title">Distributed Memory Sparse Inverse Covariance Matrix Estimation on High-Performance Computing Architectures</div><div class="slot-authors">Aryan Eftekhari (University of Lugano), Matthias Bollhöfer (Braunschweig University of Technology), and Olaf Schenk (University of Lugano)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_164_1539825877_67" onclick="$('#vhsjs_view_164_1539825877_67').hide();
                $('#vhsjs_hide_164_1539825877_67').show();
                $('#163_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_164_1539825877_67" onclick="$('#163_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_164_1539825877_67').hide();
                $('#vhsjs_view_164_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="164_1539825877_67" id="163_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>We consider the problem of estimating sparse inverse covariance matrices for high-dimensional datasets using the l1-regularized Gaussian maximum likelihood method. This task is particularly challenging as the required computational resources increase superlinearly with the dimensionality of the dataset. We introduce a performant and scalable algorithm which builds on the current advancements of second-order, maximum likelihood methods. The routine leverages the intrinsic parallelism in the linear algebra operations and exploits the underlying sparsity of the problem. The computational bottlenecks are identified and the respective subroutines are parallelized using an MPI-OpenMP approach. Experiments conducted on a Cray XC50 system at the Swiss National Supercomputing Center show that, in comparison to the state-of-the-art algorithms, the proposed routine provides significant strong scaling speedup with ideal scalability up to 128 nodes. The developed framework is used to estimate the sparse inverse covariance matrix of both synthetic and real-world datasets with up to 10 million dimensions.</blockquote></div></div></div></div><a href="includes/files/pap273s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap466"></a><div class="slot-title">PruneJuice:  Pruning Trillion-Edge Graphs to a Precise Pattern-Matching Solution</div><div class="slot-authors">Tahsin Reza, Matei Ripeanu, and Nicolas Tripoul (University of British Columbia) and Geoffrey Sanders and Roger Pearce (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_166_1539825877_67" onclick="$('#vhsjs_view_166_1539825877_67').hide();
                $('#vhsjs_hide_166_1539825877_67').show();
                $('#165_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_166_1539825877_67" onclick="$('#165_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_166_1539825877_67').hide();
                $('#vhsjs_view_166_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="166_1539825877_67" id="165_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>Pattern matching is a powerful graph analysis tool. Unfortunately, existing solutions have limited scalability, support only a limited set of search patterns, and/or focus on only a subset of the real-world problems associated with pattern matching. This paper presents a new algorithmic pipeline that: (i) enables highly scalable pattern matching on labeled graphs, (ii) supports arbitrary patterns, (iii) enables trade-offs between precision and time-to-solution (while always selecting all vertices and edges that participate in matches, thus offering 100% recall), and (iv) supports a set of popular data analytics scenarios. We implement our approach on top of HavoqGT and demonstrate its advantages through strong and weak scaling experiments on massive-scale real-world (up to 257 billion edges) and synthetic (up to 4.4 trillion edges) graphs, respectively, and at scales (1,024 nodes / 36,864 cores) orders of magnitude larger than used in the past for similar problems.</blockquote></div></div></div></div><a href="includes/files/pap466s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, Memory, Networks, Parallel Programming Languages, Libraries, and Models, Power, Programming Systems, Scheduling, Tech Program Reg Pass</span><br /><div class="session-title">Task-Based Programming</div><div class="slot-entry"><a name="pap490"></a><div class="slot-title">Dynamic Tracing: Memoization of Task Graphs for Dynamic Task-Based Runtimes</div><div class="slot-authors">Wonchan Lee (Stanford University), Elliott Slaughter (SLAC National Accelerator Laboratory), Michael Bauer and Sean Treichler (Nvidia Corporation), Todd Warszawski (Stanford University), Michael Garland (Nvidia Corporation), and Alex Aiken (Stanford University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_168_1539825877_67" onclick="$('#vhsjs_view_168_1539825877_67').hide();
                $('#vhsjs_hide_168_1539825877_67').show();
                $('#167_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_168_1539825877_67" onclick="$('#167_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_168_1539825877_67').hide();
                $('#vhsjs_view_168_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="168_1539825877_67" id="167_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>Many recent programming systems for both supercomputing and data center workloads generate task graphs to express computations that run on parallel and distributed machines. Due to the overhead associated with constructing these graphs the dependence analysis that generates them is often statically computed and memoized, and the resulting graph executed repeatedly at runtime. However, many applications require a dynamic dependence analysis due to data dependent behavior, but there are new challenges in capturing and re-executing task graphs at runtime. In this work, we introduce dynamic tracing, a technique to capture a dynamic dependence analysis of a trace that generates a task graph, and replay it. We show that an implementation of dynamic tracing improves strong scaling by an average of 4.9X and up to 7.0X on a suite of already optimized benchmarks.</blockquote></div></div></div></div><a href="includes/files/pap490s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap338"></a><div class="slot-title">Runtime-Assisted Cache Coherence Deactivation in Task Parallel Programs</div><div class="slot-authors">Paul Caheny (Barcelona Supercomputing Center, Polytechnic University of Catalonia); Lluc Alvarez (Barcelona Supercomputing Center); Mateo Valero and Miquel Moretó (Barcelona Supercomputing Center, Polytechnic University of Catalonia); and Marc Casas (Barcelona Supercomputing Center)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_170_1539825877_67" onclick="$('#vhsjs_view_170_1539825877_67').hide();
                $('#vhsjs_hide_170_1539825877_67').show();
                $('#169_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_170_1539825877_67" onclick="$('#169_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_170_1539825877_67').hide();
                $('#vhsjs_view_170_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="170_1539825877_67" id="169_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>With increasing core counts, the scalability of directory-based cache coherence has become a challenging problem. To reduce the area and power needs of the directory, recent proposals reduce its size by classifying data as private or shared, and disable coherence for private data. However, existing classification methods suffer from inaccuracies and require complex hardware support with limited scalability.<br><br>This paper proposes a hardware/software co-designed approach: the runtime system identifies data that is guaranteed by the programming model semantics to not require coherence and notifies the microarchitecture. The microarchitecture deactivates coherence for this private data and powers off unused directory capacity. Our proposal reduces directory accesses to just 26% of the baseline system and supports a 64× smaller directory with only 2.8% performance degradation. By dynamically calibrating the directory size, our proposal saves 86% of dynamic energy consumption in the directory without harming performance.</blockquote></div></div></div></div><a href="includes/files/pap338s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap547"></a><div class="slot-title">A Divide and Conquer Algorithm for DAG Scheduling Under Power Constraints</div><div class="slot-authors">Gökalp Demirci, Ivana Marincic, and Henry Hoffmann (University of Chicago)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_172_1539825877_67" onclick="$('#vhsjs_view_172_1539825877_67').hide();
                $('#vhsjs_hide_172_1539825877_67').show();
                $('#171_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_172_1539825877_67" onclick="$('#171_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_172_1539825877_67').hide();
                $('#vhsjs_view_172_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="172_1539825877_67" id="171_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>We consider the problem of scheduling a parallel computation–represented as a directed acyclic graph (DAG)–on a distributed parallel system with a global resource constraint–specifically a global power budget–and configurable resources, allowing a range of different power/performance tradeoffs. There is a rich body of literature on the independent problems of (1) scheduling DAGs and (2) scheduling independent applications under resource constraints. Very little, however, is known about the combined problem of scheduling DAGs under resource constraints. We present a novel approximation algorithm using a divide-and-conquer method for minimizing application execution time. We prove that the length of the schedule returned by our algorithm is always within O(log n)-factor of the optimum that can be achieved with selection of configurations for the tasks. We implement and test our algorithm on simulations of real application DAGs. We find that our divide-and-conquer method improves performance by up to 75% compared to greedy scheduling algorithms.</blockquote></div></div></div></div><a href="includes/files/pap547s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Computational Physics, Scientific Computing, Tech Program Reg Pass</span><br /><div class="session-title">Physics and Tensor Applications</div><div class="slot-entry"><a name="pap502"></a><div class="slot-title">Simulating the Wenchuan Earthquake with Accurate Surface Topography on Sunway TaihuLight</div><div class="slot-authors">Bingwei Chen, Haohuan Fu, Yanwen Wei, and Conghui He (Tsinghua University; National Supercomputing Center, Wuxi); Wenqiang Zhang (University of Science and Technology of China); Yuxuan Li (Tsinghua University; National Supercomputing Center, Wuxi); Wubin Wan and Wei Zhang (National Supercomputing Center, Wuxi); Lin Gan (Tsinghua University; National Supercomputing Center, Wuxi); Wei Zhang and Zhenguo Zhang (Southern University of Science and Technology, China); Guangwen Yang (Tsinghua University; National Supercomputing Center, Wuxi); and Xiaofei Chen (Southern University of Science and Technology, China)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_174_1539825877_67" onclick="$('#vhsjs_view_174_1539825877_67').hide();
                $('#vhsjs_hide_174_1539825877_67').show();
                $('#173_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_174_1539825877_67" onclick="$('#173_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_174_1539825877_67').hide();
                $('#vhsjs_view_174_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="174_1539825877_67" id="173_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>This paper reports our efforts on performing 50-m resolution earthquake simulation of the Wenchuan Earthquake (Ms 8.0, China) on Sunway TaihuLight. To accurately capture the surface topography, we adopt a curvilinear grid finite-difference method with a traction image free surface implementation and redesign the algorithm to reduce memory access costs for heterogeneous many-core architectures. We then derive a performance model of our algorithm to guide and drive the further optimization and tuning of various parameters using a genetic algorithm. A data layout transformation is also proposed to improve the direct memory access (DMA) efficiency further. Our efforts improve the simulation efficiency from 0.05% to 7.6%, with a sustained performance of 9.07 Pflops using the entire machine of the Sunway TaihuLight (over 10 million cores), and a large-scale simulation of the Wenchuan earthquake with accurate surface topography and improved coda wave effects.</blockquote></div></div></div></div><a href="includes/files/pap502s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap506"></a><div class="slot-title">Accelerating Quantum Chemistry with Vectorized and Batched Integrals</div><div class="slot-authors">Hua Huang and Edmond Chow (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_176_1539825877_67" onclick="$('#vhsjs_view_176_1539825877_67').hide();
                $('#vhsjs_hide_176_1539825877_67').show();
                $('#175_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_176_1539825877_67" onclick="$('#175_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_176_1539825877_67').hide();
                $('#vhsjs_view_176_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="176_1539825877_67" id="175_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>This paper presents the first quantum chemistry calculations using a recently developed vectorized library for computing electron repulsion integrals. To lengthen the SIMD loop and thus improve SIMD utilization, the approach used in this paper is to batch together the computation of multiple integrals that have the same code path. The standard approach is to compute integrals one at a time, and thus a batching procedure had to be developed. This paper shows proof-of-concept and demonstrates the performance gains possible when the batched approach is used. Batching also enables certain optimizations when the integrals are used to compute the Fock matrix. We further describe several other optimizations that were needed to obtain up to a 270% speedup over the no batching version of the code, making a compelling case for adopting the presented techniques in quantum chemistry software.</blockquote></div></div></div></div><a href="includes/files/pap506s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap133"></a><div class="slot-title">High-Performance Dense Tucker Decomposition on GPU Clusters</div><div class="slot-authors">Jee Choi (IBM), Xing Liu (Intel Corporation), and Venkatesan Chakaravarthy (IBM)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_178_1539825877_67" onclick="$('#vhsjs_view_178_1539825877_67').hide();
                $('#vhsjs_hide_178_1539825877_67').show();
                $('#177_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_178_1539825877_67" onclick="$('#177_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_178_1539825877_67').hide();
                $('#vhsjs_view_178_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="178_1539825877_67" id="177_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>The Tucker decomposition method is one of the most popular algorithms for analyzing and compressing data with multi-way relationship. Its execution time is typically dominated by dense matrix multiplication, which makes it well-suited for GPU acceleration. State-of-the-art distributed dense Tucker implementations for CPU clusters adopt multi-dimensional partitioning that optimizes for storage and communication. This, however, leads to smaller matrix dimensions that result in under-utilizing the GPU. <br><br>In this paper, we present our optimized implementation and performance analysis of dense Tucker decomposition on a multi-GPU cluster. We propose three optimizations: a new partitioning strategy that improves GPU performance, a new tensor matricization layout that halves the number of communication/matricization steps, and a variation of the randomized SVD algorithm to overcome the eigenvalue bottleneck that arises from the high speedups gained from GPU acceleration.  Our GPU implementation employing all three optimizations achieves up to 11.8x speedup on 64 nodes over state-of-the-art TuckerMPI.</blockquote></div></div></div></div><a href="includes/files/pap133s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Architectures, Compiler Analysis and Optimization, Floating Point, Performance, Precision, Programming Systems, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Arithmetic and Optimization</div><div class="slot-entry"><a name="pap431"></a><div class="slot-title">Associative Instruction Reordering to Alleviate Register Pressure</div><div class="slot-authors">Prashant Singh Rawat, Aravind Sukumaran-Rajam, and Atanas Rountev (Ohio State University); Fabrice Rastello (French Institute for Research in Computer Science and Automation (INRIA)); Louis-Noel Pouchet (Colorado State University); and P. Sadayappan (Ohio State University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_180_1539825877_67" onclick="$('#vhsjs_view_180_1539825877_67').hide();
                $('#vhsjs_hide_180_1539825877_67').show();
                $('#179_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_180_1539825877_67" onclick="$('#179_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_180_1539825877_67').hide();
                $('#vhsjs_view_180_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="180_1539825877_67" id="179_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>Register allocation is generally considered a practically solved problem. For most applications, the register allocation strategies in production compilers are very effective in controlling the number of loads/stores and register spills. However, existing register allocation strategies are not effective and result in excessive register spilling for computation patterns with a high degree of many-to-many data reuse, e.g., high-order stencils and tensor contractions.  We develop a source-to-source instruction reordering strategy that exploits the flexibility of reordering associative operations to alleviate register pressure.  The developed transformation module implements an adaptable strategy that can appropriately control the degree of instruction-level parallelism, while relieving register pressure.  The effectiveness of the approach is demonstrated through experimental results using multiple production compilers (GCC, Clang/LLVM) and target platforms (Intel Xeon Phi, and Intel x86 multi-core).</blockquote></div></div></div></div><a href="includes/files/pap431s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap464"></a><div class="slot-title">Harnessing GPU's Tensor Cores Fast FP16 Arithmetic to Speedup Mixed-Precision Iterative Refinement Solvers</div><div class="slot-authors">Azzam Haidar (University of Tennessee, Innovative Computing Laboratory); Stan Tomov and Jack Dongarra (University of Tennessee); and Nicholas Higham (University of Manchester, School of Mathematics)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_182_1539825877_67" onclick="$('#vhsjs_view_182_1539825877_67').hide();
                $('#vhsjs_hide_182_1539825877_67').show();
                $('#181_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_182_1539825877_67" onclick="$('#181_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_182_1539825877_67').hide();
                $('#vhsjs_view_182_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="182_1539825877_67" id="181_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>The use of low-precision arithmetic in computing methods has been a powerful tool to accelerate numerous scientific computing applications including Artificial Intelligence. We present an investigation showing that other HPC applications can harness this power too, and in particular, the general HPC problem of solving Ax = b, where A is a large dense matrix, and the solution is needed in FP64 accuracy. Our approach is based on the mixed-precision (FP16->FP64) iterative refinement technique – we generalize and extend prior advances into a framework, for which we develop architecture-specific algorithms and highly-tuned implementations where we show how the use of FP16-TC (tensor cores) arithmetic can provide up to 4X speedup and improve the energy consumption by a factor of 5 achieving 74 Gflop/Watt. This is due to the performance boost that the FP16 (Tensor Cores) provide and to its better accuracy that outperforms the classical FP16.</blockquote></div></div></div></div><a href="includes/files/pap464s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap503"></a><div class="slot-title">ADAPT: Algorithmic Differentiation Applied to Floating-Point Precision Tuning</div><div class="slot-authors">Harshitha Menon (Lawrence Livermore National Laboratory); Michael O. Lam (James Madison University, Lawrence Livermore National Laboratory); and Daniel Osei-Kuffuor, Markus Schordan, Scott Lloyd, Kathryn Mohror, and Jeffrey Hittinger (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_184_1539825877_67" onclick="$('#vhsjs_view_184_1539825877_67').hide();
                $('#vhsjs_hide_184_1539825877_67').show();
                $('#183_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_184_1539825877_67" onclick="$('#183_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_184_1539825877_67').hide();
                $('#vhsjs_view_184_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="184_1539825877_67" id="183_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>HPC applications extensively use floating point arithmetic operations to solve computational problems in various domains. Mixed precision computing, use of lowest precision data type sufficient to achieve a desired accuracy, have been explored to improve performance, reduce power consumption and data movement. Manually optimizing the program to use mixed precision is challenging. In this work, we present ADAPT, an approach for mixed precision analysis on HPC workloads while providing guarantees about the final output error. Our approach uses algorithmic differentiation to accurately estimate the output error for mixed precision configuration. ADAPT provides floating-point precision sensitivity of programs, which highlights regions of the code that that can potentially be converted to lower precision, is used to make algorithmic choices and develop mixed precision configurations. We evaluate ADAPT on six benchmarks and a proxy application and show that we are able to achieve a speedup of 1.2x on the proxy application, LULESH.</blockquote></div></div></div></div><a href="includes/files/pap503s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, GPUs, Linear Algebra, Networks, Resiliency, Tech Program Reg Pass</span><br /><div class="session-title">Resilience III: GPUs</div><div class="slot-entry"><a name="pap247"></a><div class="slot-title">Optimizing Software-Directed Instruction Replication for GPU Error Detection</div><div class="slot-authors">Abdulrahman Mahmoud (University of Illinois) and Siva Kumar Sastry Hari, Michael B. Sullivan, Timothy Tsai, and Stephen W. Keckler (Nvidia Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_186_1539825877_67" onclick="$('#vhsjs_view_186_1539825877_67').hide();
                $('#vhsjs_hide_186_1539825877_67').show();
                $('#185_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_186_1539825877_67" onclick="$('#185_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_186_1539825877_67').hide();
                $('#vhsjs_view_186_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="186_1539825877_67" id="185_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>Application execution on safety-critical and high-performance computer systems must be resilient to transient errors. As GPUs become more pervasive in such systems, they must supplement ECC/parity for major storage structures with reliability techniques that cover more of the GPU hardware logic.  Instruction duplication has been explored for CPU resilience; however, it has never been studied in the context of GPUs, and it is unclear whether the performance and design choices it presents makes it a feasible GPU solution. This paper describes a practical methodology to employ instruction duplication for GPUs and identifies implementation challenges that can incur high overheads (69% on average). It explores GPU-specific software optimizations that trade fine-grained recoverability for performance. It also proposes simple ISA extensions with limited hardware changes and area costs to further improve performance, cutting the runtime overheads by more than half to an average of 30%.</blockquote></div></div></div></div><a href="includes/files/pap247s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap244"></a><div class="slot-title">Fault Tolerant One-Sided Matrix Decompositions on Heterogeneous Systems with GPUs</div><div class="slot-authors">Jieyang Chen, Hongbo Li, Sihuan Li, and Xin Liang (University of California, Riverside); Panruo Wu (University of Houston); Dingwen Tao (University of Alabama); Kaiming Ouyang, Yuanlai Liu, and Kai Zhao (University of California, Riverside); Qiang Guan (Kent State University); and Zizhong Chen (University of California, Riverside)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_188_1539825877_67" onclick="$('#vhsjs_view_188_1539825877_67').hide();
                $('#vhsjs_hide_188_1539825877_67').show();
                $('#187_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_188_1539825877_67" onclick="$('#187_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_188_1539825877_67').hide();
                $('#vhsjs_view_188_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="188_1539825877_67" id="187_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>Current algorithm-based fault tolerance (ABFT) approach for one-sided matrix decomposition on heterogeneous systems with GPUs have following limitations: (1) they do not provide sufficient protection as most of them only maintain checksum in one dimension; (2) their checking scheme is not efficient due to redundant checksum verifications; (3) they fail to protect PCIe communication; (4) the checksum calculation based on a special type of matrix multiplication is far from efficient. By overcoming the above limitations, we design an efficient ABFT approach providing stronger protection for one-sided matrix decomposition methods on heterogeneous systems. First, we provide full matrix protection by using checksums in two dimensions. Second, our checking scheme is more efficient by prioritizing the checksum verification according to the sensitivity of matrix operations to soft errors. Third, we protect PCIe communication by reordering checksum verifications and decomposition steps. Fourth, we accelerate the checksum calculation by 1.7x via better utilizing GPUs.</blockquote></div></div></div></div><a href="includes/files/pap244s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap430"></a><div class="slot-title">PRISM: Predicting Resilience of GPU Applications Using Statistical Methods</div><div class="slot-authors">Charu Kalra, Fritz Previlon, and Xiangyu Li (Northeastern University); Norman Rubin (Nvidia Corporation); and David Kaeli (Northeastern University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_190_1539825877_67" onclick="$('#vhsjs_view_190_1539825877_67').hide();
                $('#vhsjs_hide_190_1539825877_67').show();
                $('#189_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_190_1539825877_67" onclick="$('#189_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_190_1539825877_67').hide();
                $('#vhsjs_view_190_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="190_1539825877_67" id="189_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>As Graphics Processing Units (GPUs) become more pervasive in HPC and safety-critical domains, ensuring that GPU applications can be protected from data corruption grows in importance. Despite prior efforts to mitigate errors, we still lack a clear understanding of how resilient these applications are in the presence of transient faults.  Due to the random nature of these faults, predicting whether they will alter the program output is a challenging problem. In this paper, we build a framework named PRISM, which uses a systematic approach to predict failures in GPU programs. PRISM extracts micro-architecture agnostic features to characterize program resiliency, which serve as predictors in our statistical model. PRISM enables us to predict failures in applications without running exhaustive fault-injection campaigns on a GPU, thereby reducing the error estimation effort. PRISM can also be used to gain insight into potential architectural support required to improve the reliability of GPU applications.</blockquote></div></div></div></div><a href="includes/files/pap430s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Computational Physics, Scientific Computing, Tech Program Reg Pass</span><br /><div class="session-title">Astrophysics Applications</div><div class="slot-entry"><a name="pap239"></a><div class="slot-title">Phase Asynchronous AMR Execution for Productive and Performant Astrophysical Flows</div><div class="slot-authors">Muhammad Nufail Farooqi (Koc University); Tan Nguyen, Weiqun Zhang, Ann S. Almgren, and John Shalf (Lawrence Berkeley National Laboratory); and Didem Unat (Koc University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_192_1539825877_67" onclick="$('#vhsjs_view_192_1539825877_67').hide();
                $('#vhsjs_hide_192_1539825877_67').show();
                $('#191_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_192_1539825877_67" onclick="$('#191_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_192_1539825877_67').hide();
                $('#vhsjs_view_192_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="192_1539825877_67" id="191_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>Adaptive Mesh Refinement (AMR) is an approach to solving PDEs that reduces the computational and memory requirements at the expense of increased communication. Although adopting asynchronous execution can overcome communication issues, manually restructuring an AMR application to realize asynchrony is extremely complicated and hinders readability and long-term maintainability. To balance performance against productivity, we design a user-friendly API and adopt phase asynchronous execution model where all subgrids at an AMR level can be computed asynchronously. <br><br>We apply the phase asynchrony to transform a real-world AMR application, CASTRO, which solves multicomponent compressible hydrodynamic equations for astrophysical flows. We evaluate the performance and programming effort required to use our carefully designed API and execution model for transitioning large legacy codes from synchronous to asynchronous execution up to 278,528 Intel-KNL cores. CASTRO is about 100K lines of code but less than 0.2% code changes are required to achieve significant performance improvement.</blockquote></div></div></div></div><a href="includes/files/pap239s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap294"></a><div class="slot-title">Computing Planetary Interior Normal Modes with a Highly Parallel Polynomial Filtering Eigensolver</div><div class="slot-authors">Jia Shi (Rice University), Ruipeng Li (Lawrence Livermore National Laboratory), Yuanzhe Xi and Yousef Saad (University of Minnesota), and Maarten V. de Hoop (Rice University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_194_1539825877_67" onclick="$('#vhsjs_view_194_1539825877_67').hide();
                $('#vhsjs_hide_194_1539825877_67').show();
                $('#193_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_194_1539825877_67" onclick="$('#193_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_194_1539825877_67').hide();
                $('#vhsjs_view_194_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="194_1539825877_67" id="193_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>A highly parallel algorithm has been developed and exploited to compute the planetary normal modes of the elastic-gravitational system, which is approximated via the mixed finite element method on unstructured tetrahedral meshes. The eigenmodes of the relevant generalized eigenvalue problem were extracted by a Lanczos approach combined with polynomial filtering. In contrast with the standard shift-and-invert and the full-mode coupling algorithms, the polynomial filtering technique is ideally suited for solving large-scale 3-D interior eigenvalue problems since it significantly enhances the memory and computational efficiency without loss of accuracy.  The parallel efficiency and scalability of this approach are demonstrated on Stampede2 at the Texas Advanced Computing Center. To our knowledge, this is the first time that the direct calculation of the normal modes of 3-D strongly heterogeneous planets, in particular, Earth and Mars, is made feasible via a combination of multiple matrix-free methods and a separation of the essential spectra.</blockquote></div></div></div></div><a href="includes/files/pap294s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack105"></a><div class="section-title">Applications</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Computational Biology, Scientific Computing, Tech Program Reg Pass</span><br /><div class="session-title">Biology Applications</div><div class="slot-entry"><a name="pap410"></a><div class="slot-title">Extreme Scale De Novo Metagenome Assembly</div><div><span class="BP award">Best Paper Finalists</span></div><div class="slot-authors">Evangelos Georganas (Intel Corporation) and Rob Egan, Steven Hofmeyr, Eugene Goltsman, Bill Arndt, Andrew Tritt, Aydin Buluc, Leonid Oliker, and Katherine Yelick (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_196_1539825877_67" onclick="$('#vhsjs_view_196_1539825877_67').hide();
                $('#vhsjs_hide_196_1539825877_67').show();
                $('#195_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_196_1539825877_67" onclick="$('#195_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_196_1539825877_67').hide();
                $('#vhsjs_view_196_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="196_1539825877_67" id="195_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>Metagenome assembly is the process of transforming a set of short, overlapping, and potentially erroneous DNA segments from environmental samples into the accurate representation of the underlying microbiomes's genomes. State-of-the-art tools require large shared memory machines and cannot handle contemporary metagenome datasets that exceed terabytes in size. In this paper, we introduce the metaHipMer pipeline, a high-quality and high-performance metagenome assembler that employs an iterative de Bruijn graph approach. MetaHipMer leverages a specialized scaffolding algorithm that produces long scaffolds and accommodates the idiosyncrasies of metagenomes. MetaHipMer is end-to-end parallelized using the Unified Parallel C language and therefore can run seamlessly on shared and distributed-memory systems. Experimental results show that metaHipMer matches or outperforms the state-of-the-art tools in terms of accuracy. Moreover, metaHipMer scales efficiently to large concurrencies and is able to assemble previously intractable grand challenge metagenomes.</blockquote></div></div></div></div><a href="includes/files/pap410s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap551"></a><div class="slot-title">Optimizing High Performance Distributed Memory Parallel Hash Tables for DNA k-mer Counting</div><div class="slot-authors">Tony C. Pan (Georgia Institute of Technology, School of Computational Science and Engineering); Sanchit Misra (Intel Corporation, Parallel Computing Lab); and Srinivas Aluru (Georgia Institute of Technology, School of Computational Science and Engineering)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_198_1539825877_67" onclick="$('#vhsjs_view_198_1539825877_67').hide();
                $('#vhsjs_hide_198_1539825877_67').show();
                $('#197_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_198_1539825877_67" onclick="$('#197_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_198_1539825877_67').hide();
                $('#vhsjs_view_198_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="198_1539825877_67" id="197_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>High-throughput DNA sequencing is the mainstay of modern genomics research. A common operation used in bioinformatic analysis for many applications of high-throughput sequencing is the counting and indexing of fixed length substrings of DNA sequences called k-mers. Counting k-mers is often accomplished via hashing, and distributed memory k-mer counting algorithms for large data sets are memory access and network communication bound. In this work, we present two optimized distributed parallel hash table techniques that utilize cache friendly algorithms for local hashing, overlapped communication and computation to hide communication costs, and vectorized hash functions that are specialized for k-mer and other short key indices. On 4096 cores of the NERSC Cori supercomputer, our implementation completed index construction and query on an approximately 1 TB human genome dataset in just 11.8 seconds and 5.8 seconds, demonstrating speedups of 2.06x and 3.7x, respectively, over the previous state-of-the-art distributed memory k-mer counter.</blockquote></div></div></div></div><a href="includes/files/pap551s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap120"></a><div class="slot-title">Redesigning LAMMPS for Petascale and Hundred-Billion-Atom Simulation on Sunway TaihuLight</div><div class="slot-authors">Xiaohui Duan, Ping Gao, Tingjian Zhang, Meng Zhang, and Weiguo Liu (Shandong University); Wusheng Zhang, Wei Xue, Haohuan Fu, Lin Gan, and Dexun Chen (Tsinghua University); Xiangxu Meng (Shandong University); and Guangwen Yang (Tsinghua University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_200_1539825877_67" onclick="$('#vhsjs_view_200_1539825877_67').hide();
                $('#vhsjs_hide_200_1539825877_67').show();
                $('#199_1539825877_67').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_200_1539825877_67" onclick="$('#199_1539825877_67').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_200_1539825877_67').hide();
                $('#vhsjs_view_200_1539825877_67').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="200_1539825877_67" id="199_1539825877_67" style="display: none"><div class="arrow-slidedown"><blockquote>Large-scale molecular dynamics (MD) simulations on supercomputers play an increasingly important role in many research areas. In this paper, we present our efforts on redesigning the widely used LAMMPS MD simulator for Sunway TaihuLight supercomputer and its ShenWei many-core architecture (SW26010). The memory constraints of SW26010 bring a number of new challenges for achieving efficient MD implementation on it. In order to overcome these constraints, we employ four levels of optimization: (1) a hybrid memory update strategy; (2) a software cache strategy; (3) customized transcendental math functions; and (4) a full pipeline acceleration. Furthermore, we redesign the code to enable all possible vectorization. Experiments show that our redesigned software on a single SW26010 processor can outperform over 100 E5-2650 cores for running the latest stable release (11Aug17) of LAMMPS. We also achieve a performance of over 2.43 PFlops for a Tersoff simulation when using 16,384 nodes on Sunway TaihuLight.</blockquote></div></div></div></div><a href="includes/files/pap120s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Computational Physics, Scientific Computing, Tech Program Reg Pass</span><br /><div class="session-title">Physics and Tensor Applications</div><div class="slot-entry"><a name="pap502"></a><div class="slot-title">Simulating the Wenchuan Earthquake with Accurate Surface Topography on Sunway TaihuLight</div><div class="slot-authors">Bingwei Chen, Haohuan Fu, Yanwen Wei, and Conghui He (Tsinghua University; National Supercomputing Center, Wuxi); Wenqiang Zhang (University of Science and Technology of China); Yuxuan Li (Tsinghua University; National Supercomputing Center, Wuxi); Wubin Wan and Wei Zhang (National Supercomputing Center, Wuxi); Lin Gan (Tsinghua University; National Supercomputing Center, Wuxi); Wei Zhang and Zhenguo Zhang (Southern University of Science and Technology, China); Guangwen Yang (Tsinghua University; National Supercomputing Center, Wuxi); and Xiaofei Chen (Southern University of Science and Technology, China)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_202_1539825877_68" onclick="$('#vhsjs_view_202_1539825877_68').hide();
                $('#vhsjs_hide_202_1539825877_68').show();
                $('#201_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_202_1539825877_68" onclick="$('#201_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_202_1539825877_68').hide();
                $('#vhsjs_view_202_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="202_1539825877_68" id="201_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>This paper reports our efforts on performing 50-m resolution earthquake simulation of the Wenchuan Earthquake (Ms 8.0, China) on Sunway TaihuLight. To accurately capture the surface topography, we adopt a curvilinear grid finite-difference method with a traction image free surface implementation and redesign the algorithm to reduce memory access costs for heterogeneous many-core architectures. We then derive a performance model of our algorithm to guide and drive the further optimization and tuning of various parameters using a genetic algorithm. A data layout transformation is also proposed to improve the direct memory access (DMA) efficiency further. Our efforts improve the simulation efficiency from 0.05% to 7.6%, with a sustained performance of 9.07 Pflops using the entire machine of the Sunway TaihuLight (over 10 million cores), and a large-scale simulation of the Wenchuan earthquake with accurate surface topography and improved coda wave effects.</blockquote></div></div></div></div><a href="includes/files/pap502s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap506"></a><div class="slot-title">Accelerating Quantum Chemistry with Vectorized and Batched Integrals</div><div class="slot-authors">Hua Huang and Edmond Chow (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_204_1539825877_68" onclick="$('#vhsjs_view_204_1539825877_68').hide();
                $('#vhsjs_hide_204_1539825877_68').show();
                $('#203_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_204_1539825877_68" onclick="$('#203_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_204_1539825877_68').hide();
                $('#vhsjs_view_204_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="204_1539825877_68" id="203_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>This paper presents the first quantum chemistry calculations using a recently developed vectorized library for computing electron repulsion integrals. To lengthen the SIMD loop and thus improve SIMD utilization, the approach used in this paper is to batch together the computation of multiple integrals that have the same code path. The standard approach is to compute integrals one at a time, and thus a batching procedure had to be developed. This paper shows proof-of-concept and demonstrates the performance gains possible when the batched approach is used. Batching also enables certain optimizations when the integrals are used to compute the Fock matrix. We further describe several other optimizations that were needed to obtain up to a 270% speedup over the no batching version of the code, making a compelling case for adopting the presented techniques in quantum chemistry software.</blockquote></div></div></div></div><a href="includes/files/pap506s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap133"></a><div class="slot-title">High-Performance Dense Tucker Decomposition on GPU Clusters</div><div class="slot-authors">Jee Choi (IBM), Xing Liu (Intel Corporation), and Venkatesan Chakaravarthy (IBM)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_206_1539825877_68" onclick="$('#vhsjs_view_206_1539825877_68').hide();
                $('#vhsjs_hide_206_1539825877_68').show();
                $('#205_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_206_1539825877_68" onclick="$('#205_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_206_1539825877_68').hide();
                $('#vhsjs_view_206_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="206_1539825877_68" id="205_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>The Tucker decomposition method is one of the most popular algorithms for analyzing and compressing data with multi-way relationship. Its execution time is typically dominated by dense matrix multiplication, which makes it well-suited for GPU acceleration. State-of-the-art distributed dense Tucker implementations for CPU clusters adopt multi-dimensional partitioning that optimizes for storage and communication. This, however, leads to smaller matrix dimensions that result in under-utilizing the GPU. <br><br>In this paper, we present our optimized implementation and performance analysis of dense Tucker decomposition on a multi-GPU cluster. We propose three optimizations: a new partitioning strategy that improves GPU performance, a new tensor matricization layout that halves the number of communication/matricization steps, and a variation of the randomized SVD algorithm to overcome the eigenvalue bottleneck that arises from the high speedups gained from GPU acceleration.  Our GPU implementation employing all three optimizations achieves up to 11.8x speedup on 64 nodes over state-of-the-art TuckerMPI.</blockquote></div></div></div></div><a href="includes/files/pap133s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Architectures, Compiler Analysis and Optimization, Floating Point, Performance, Precision, Programming Systems, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Arithmetic and Optimization</div><div class="slot-entry"><a name="pap431"></a><div class="slot-title">Associative Instruction Reordering to Alleviate Register Pressure</div><div class="slot-authors">Prashant Singh Rawat, Aravind Sukumaran-Rajam, and Atanas Rountev (Ohio State University); Fabrice Rastello (French Institute for Research in Computer Science and Automation (INRIA)); Louis-Noel Pouchet (Colorado State University); and P. Sadayappan (Ohio State University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_208_1539825877_68" onclick="$('#vhsjs_view_208_1539825877_68').hide();
                $('#vhsjs_hide_208_1539825877_68').show();
                $('#207_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_208_1539825877_68" onclick="$('#207_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_208_1539825877_68').hide();
                $('#vhsjs_view_208_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="208_1539825877_68" id="207_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>Register allocation is generally considered a practically solved problem. For most applications, the register allocation strategies in production compilers are very effective in controlling the number of loads/stores and register spills. However, existing register allocation strategies are not effective and result in excessive register spilling for computation patterns with a high degree of many-to-many data reuse, e.g., high-order stencils and tensor contractions.  We develop a source-to-source instruction reordering strategy that exploits the flexibility of reordering associative operations to alleviate register pressure.  The developed transformation module implements an adaptable strategy that can appropriately control the degree of instruction-level parallelism, while relieving register pressure.  The effectiveness of the approach is demonstrated through experimental results using multiple production compilers (GCC, Clang/LLVM) and target platforms (Intel Xeon Phi, and Intel x86 multi-core).</blockquote></div></div></div></div><a href="includes/files/pap431s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap464"></a><div class="slot-title">Harnessing GPU's Tensor Cores Fast FP16 Arithmetic to Speedup Mixed-Precision Iterative Refinement Solvers</div><div class="slot-authors">Azzam Haidar (University of Tennessee, Innovative Computing Laboratory); Stan Tomov and Jack Dongarra (University of Tennessee); and Nicholas Higham (University of Manchester, School of Mathematics)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_210_1539825877_68" onclick="$('#vhsjs_view_210_1539825877_68').hide();
                $('#vhsjs_hide_210_1539825877_68').show();
                $('#209_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_210_1539825877_68" onclick="$('#209_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_210_1539825877_68').hide();
                $('#vhsjs_view_210_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="210_1539825877_68" id="209_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>The use of low-precision arithmetic in computing methods has been a powerful tool to accelerate numerous scientific computing applications including Artificial Intelligence. We present an investigation showing that other HPC applications can harness this power too, and in particular, the general HPC problem of solving Ax = b, where A is a large dense matrix, and the solution is needed in FP64 accuracy. Our approach is based on the mixed-precision (FP16->FP64) iterative refinement technique – we generalize and extend prior advances into a framework, for which we develop architecture-specific algorithms and highly-tuned implementations where we show how the use of FP16-TC (tensor cores) arithmetic can provide up to 4X speedup and improve the energy consumption by a factor of 5 achieving 74 Gflop/Watt. This is due to the performance boost that the FP16 (Tensor Cores) provide and to its better accuracy that outperforms the classical FP16.</blockquote></div></div></div></div><a href="includes/files/pap464s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap503"></a><div class="slot-title">ADAPT: Algorithmic Differentiation Applied to Floating-Point Precision Tuning</div><div class="slot-authors">Harshitha Menon (Lawrence Livermore National Laboratory); Michael O. Lam (James Madison University, Lawrence Livermore National Laboratory); and Daniel Osei-Kuffuor, Markus Schordan, Scott Lloyd, Kathryn Mohror, and Jeffrey Hittinger (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_212_1539825877_68" onclick="$('#vhsjs_view_212_1539825877_68').hide();
                $('#vhsjs_hide_212_1539825877_68').show();
                $('#211_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_212_1539825877_68" onclick="$('#211_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_212_1539825877_68').hide();
                $('#vhsjs_view_212_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="212_1539825877_68" id="211_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>HPC applications extensively use floating point arithmetic operations to solve computational problems in various domains. Mixed precision computing, use of lowest precision data type sufficient to achieve a desired accuracy, have been explored to improve performance, reduce power consumption and data movement. Manually optimizing the program to use mixed precision is challenging. In this work, we present ADAPT, an approach for mixed precision analysis on HPC workloads while providing guarantees about the final output error. Our approach uses algorithmic differentiation to accurately estimate the output error for mixed precision configuration. ADAPT provides floating-point precision sensitivity of programs, which highlights regions of the code that that can potentially be converted to lower precision, is used to make algorithmic choices and develop mixed precision configurations. We evaluate ADAPT on six benchmarks and a proxy application and show that we are able to achieve a speedup of 1.2x on the proxy application, LULESH.</blockquote></div></div></div></div><a href="includes/files/pap503s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Applications, Graph Algorithms, Security, Tech Program Reg Pass</span><br /><div class="session-title">Graph Algorithms and Systems</div><div class="slot-entry"><a name="pap115"></a><div class="slot-title">iSpan: Parallel Identification of Strongly Connected Components with Spanning Trees</div><div class="slot-authors">Yuede Ji (George Washington University); Hang Liu (University of Massachusetts, Lowell); and H. Howie Huang (George Washington University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_214_1539825877_68" onclick="$('#vhsjs_view_214_1539825877_68').hide();
                $('#vhsjs_hide_214_1539825877_68').show();
                $('#213_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_214_1539825877_68" onclick="$('#213_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_214_1539825877_68').hide();
                $('#vhsjs_view_214_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="214_1539825877_68" id="213_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>Detecting strongly connected components (SCCs) in a directed graph is crucial for understanding the structure of graphs. Most real-world graphs have one large SCC that contains the majority of the vertices, and many small SCCs whose sizes are reversely proportional to the frequency of their occurrence. For both types of SCCs, current approaches that rely on depth or breadth first search (DFS or BFS) face the challenges of strict synchronization requirement and high computation cost. In this paper, we advocate a new paradigm of identifying SCCs with simple spanning trees, since SCC detection requires only the knowledge of connectivity among the vertices. We have developed a prototype called iSpan which consists of parallel, relaxed synchronization construction of spanning trees for detecting the large and small SCCs. The evaluations show that iSpan is able to significantly outperform current state-of-the-art DFS and BFS- based methods by average 18× and 4×, respectively.</blockquote></div></div></div></div><a href="includes/files/pap115s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap468"></a><div class="slot-title">Adaptive Anonymization of Data with b-Edge Covers</div><div class="slot-authors">Arif Khan (Pacific Northwest National Laboratory), Krzysztof Choromanski (Google LLC), Alex Pothen and S M Ferdous (Purdue University), and Mahantesh Halappanavar and Antonino Tumeo (Pacific Northwest National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_216_1539825877_68" onclick="$('#vhsjs_view_216_1539825877_68').hide();
                $('#vhsjs_hide_216_1539825877_68').show();
                $('#215_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_216_1539825877_68" onclick="$('#215_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_216_1539825877_68').hide();
                $('#vhsjs_view_216_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="216_1539825877_68" id="215_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>We explore the problem of sharing data that pertains to individuals with anonymity guarantees, where each user requires a desired level of privacy.  We propose the first shared-memory as well as distributed memory parallel algorithms for the adaptive anonymity problem that achieves this goal, and produces high quality anonymized datasets.  <br><br>The new algorithm is based on an optimization procedure that iteratively computes weights on the edges of a dissimilarity matrix, and at each iteration computes a minimum weighted b-Edge cover in the graph. We are able to solve adaptive anonymity problems with hundreds of thousands of instances and hundreds of features on a leadership-class supercomputer in under five minutes. Our algorithm scales up to 4K cores on a distributed memory supercomputer, while also providing good speedups on shared memory multiprocessors. On smaller problems, where an algorithm based on Belief Propagation is feasible, our algorithm is two orders of magnitude faster.</blockquote></div></div></div></div><a href="includes/files/pap468s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap167"></a><div class="slot-title">faimGraph: High Performance Management of Fully-Dynamic Graphs Under Tight Memory Constraints on the GPU</div><div class="slot-authors">Martin Winter and Daniel Mlakar (Graz University of Technology); Rhaleb Zayer and Hans-Peter Seidel (Max Planck Institute for Informatics); and Markus Steinberger (Graz University of Technology, Max Planck Institute for Informatics)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_218_1539825877_68" onclick="$('#vhsjs_view_218_1539825877_68').hide();
                $('#vhsjs_hide_218_1539825877_68').show();
                $('#217_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_218_1539825877_68" onclick="$('#217_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_218_1539825877_68').hide();
                $('#vhsjs_view_218_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="218_1539825877_68" id="217_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>In this paper, we present a fully-dynamic graph data structure for the Graphics Processing Unit (GPU). It delivers high update rates while keeping a low memory footprint using autonomous memory management directly on the GPU. The data structure is fully-dynamic, allowing not only for edge but also vertex updates. Performing the memory management on the GPU allows for fast initialization times and efficient update procedures without additional intervention or reallocation procedures from the host.  faimGraph is the first GPU graph framework that fully reclaims unused memory, permitting long time application with highly changing graph structures. Performance evaluations show that our approach outperforms that previous state-of-the-art in for all types of graph updates. Furthermore, evaluate algorithmic performance using a PageRank and a Static Triangle Counting (STC) implementation, demonstrating the suitability of the framework even for memory access intensive algorithms.</blockquote></div></div></div></div><a href="includes/files/pap167s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Applications, Cosmology, Data Analytics, Deep Learning, Machine Learning, Programming Systems, Storage, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Deep Learning</div><div class="slot-entry"><a name="pap425"></a><div class="slot-title">Exploring Flexible Communications for Streamlining DNN Ensemble Training Pipelines</div><div class="slot-authors">Randall Pittman, Hui Guan, and Xipeng Shen (North Carolina State University) and Seung-Hwan Lim and Robert M. Patton (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_220_1539825877_68" onclick="$('#vhsjs_view_220_1539825877_68').hide();
                $('#vhsjs_hide_220_1539825877_68').show();
                $('#219_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_220_1539825877_68" onclick="$('#219_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_220_1539825877_68').hide();
                $('#vhsjs_view_220_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="220_1539825877_68" id="219_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>Parallel training of a Deep Neural Network (DNN) ensemble on a cluster of nodes is a common practice to train multiple models in order to construct a model with a higher prediction accuracy. Existing ensemble training pipelines can perform a great deal of redundant operations, resulting in unnecessary CPU usage, or even poor pipeline performance.  In order to remove these redundancies, we need pipelines with more communication flexibility than existing DNN frameworks provide.<br><br>This project investigates a series of designs to improve pipeline flexibility and adaptivity, while also increasing performance. We implement our designs using Tensorflow with Horovod, and test it using several large DNNs. Our results show that the CPU time spent during training is reduced by 2-11X. Furthermore, our implementation can achieve up to 10X speedups when CPU core limits are imposed. Our best pipeline also reduces the average power draw of the ensemble training process by 5-16%.</blockquote></div></div></div></div><a href="includes/files/pap425s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap429"></a><div class="slot-title">CosmoFlow: Using Deep Learning to Learn the Universe at Scale</div><div class="slot-authors">Amrita Mathuriya (Intel Corporation); Deborah Bard (National Energy Research Scientific Computing Center (NERSC), Lawrence Berkeley National Laboratory); Pete Mendygral (Cray Inc); Lawrence Meadows (Intel Corporation); James Arnemann (University of California, Berkeley); Lei Shao (Intel Corporation); Siyu He (Carnegie Mellon University); Tuomas Karna (Intel Corporation); Diana Moise (Cray Inc); Simon J. Pennycook (Intel Corporation); Kristyn Maschhoff (Cray Inc); Jason Sewall and Nalini Kumar (Intel Corporation); Shirley Ho (Lawrence Berkeley National Laboratory, Carnegie Mellon University); Michael F. Ringenburg (Cray Inc); Mr Prabhat (Lawrence Berkeley National Laboratory, National Energy Research Scientific Computing Center (NERSC)); and Victor Lee (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_222_1539825877_68" onclick="$('#vhsjs_view_222_1539825877_68').hide();
                $('#vhsjs_hide_222_1539825877_68').show();
                $('#221_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_222_1539825877_68" onclick="$('#221_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_222_1539825877_68').hide();
                $('#vhsjs_view_222_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="222_1539825877_68" id="221_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>Deep learning is a promising tool to determine the physical model that describes our universe.   To handle the considerable computational cost of this problem, we present CosmoFlow: a highly scalable deep learning application built on top of the TensorFlow framework.<br><br>CosmoFlow uses efficient implementations of 3D convolution and pooling primitives, together with improvements in threading for many element-wise operations, to improve training performance on Intel Xeon Phi processors.  We also utilize the Cray PE Machine Learning Plugin for efficient scaling to multiple nodes. We demonstrate fully synchronous data-parallel training on 8192 nodes of Cori with 77% parallel efficiency, achieving 3.5 Pflop/s sustained performance. <br><br>To our knowledge, this is the first large-scale science application of the TensorFlow framework at supercomputer scale with fully-synchronous training. These enhancements enable us to process large 3D dark matter distribution and predict the cosmological parameters Omega_M, sigma_8 and N_s with unprecedented accuracy.</blockquote></div></div></div></div><a href="includes/files/pap429s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap322"></a><div class="slot-title">Anatomy of High-Performance Deep Learning Convolutions on SIMD Architectures</div><div class="slot-authors">Evangelos Georganas, Sasikanth Avancha, Kunal Banerjee, Dhiraj Kalamkar, Greg Henry, Hans Pabst, and Alexander Heinecke (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_224_1539825877_68" onclick="$('#vhsjs_view_224_1539825877_68').hide();
                $('#vhsjs_hide_224_1539825877_68').show();
                $('#223_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_224_1539825877_68" onclick="$('#223_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_224_1539825877_68').hide();
                $('#vhsjs_view_224_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="224_1539825877_68" id="223_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>Convolution layers are prevalent in many classes of deep neural networks, including Convolutional Neural Networks (CNNs) which provide state-of-the-art results for tasks like image recognition, neural machine translation, and speech recognition. The computationally expensive nature of a convolution operation has led to the proliferation of implementations including matrix-matrix multiplication formulation, and direct convolution primarily targeting GPUs. In this paper, we introduce direct convolution kernels for x86 architectures, in particular for Xeon and Xeon Phi systems, which are implemented via a dynamic compilation approach. Our JIT-based implementation shows close to theoretical peak performance, depending on the setting and the CPU architecture at hand. We additionally demonstrate how these JIT-optimized kernels can be integrated into a light-weight multi-node graph execution model. This illustrates that single- and multi-node runs yield high efficiencies and high image-throughputs  when executing state of the art image recognition tasks on CPUs.</blockquote></div></div></div></div><a href="includes/files/pap322s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Computational Physics, Scientific Computing, Tech Program Reg Pass</span><br /><div class="session-title">Astrophysics Applications</div><div class="slot-entry"><a name="pap239"></a><div class="slot-title">Phase Asynchronous AMR Execution for Productive and Performant Astrophysical Flows</div><div class="slot-authors">Muhammad Nufail Farooqi (Koc University); Tan Nguyen, Weiqun Zhang, Ann S. Almgren, and John Shalf (Lawrence Berkeley National Laboratory); and Didem Unat (Koc University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_226_1539825877_68" onclick="$('#vhsjs_view_226_1539825877_68').hide();
                $('#vhsjs_hide_226_1539825877_68').show();
                $('#225_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_226_1539825877_68" onclick="$('#225_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_226_1539825877_68').hide();
                $('#vhsjs_view_226_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="226_1539825877_68" id="225_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>Adaptive Mesh Refinement (AMR) is an approach to solving PDEs that reduces the computational and memory requirements at the expense of increased communication. Although adopting asynchronous execution can overcome communication issues, manually restructuring an AMR application to realize asynchrony is extremely complicated and hinders readability and long-term maintainability. To balance performance against productivity, we design a user-friendly API and adopt phase asynchronous execution model where all subgrids at an AMR level can be computed asynchronously. <br><br>We apply the phase asynchrony to transform a real-world AMR application, CASTRO, which solves multicomponent compressible hydrodynamic equations for astrophysical flows. We evaluate the performance and programming effort required to use our carefully designed API and execution model for transitioning large legacy codes from synchronous to asynchronous execution up to 278,528 Intel-KNL cores. CASTRO is about 100K lines of code but less than 0.2% code changes are required to achieve significant performance improvement.</blockquote></div></div></div></div><a href="includes/files/pap239s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap294"></a><div class="slot-title">Computing Planetary Interior Normal Modes with a Highly Parallel Polynomial Filtering Eigensolver</div><div class="slot-authors">Jia Shi (Rice University), Ruipeng Li (Lawrence Livermore National Laboratory), Yuanzhe Xi and Yousef Saad (University of Minnesota), and Maarten V. de Hoop (Rice University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_228_1539825877_68" onclick="$('#vhsjs_view_228_1539825877_68').hide();
                $('#vhsjs_hide_228_1539825877_68').show();
                $('#227_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_228_1539825877_68" onclick="$('#227_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_228_1539825877_68').hide();
                $('#vhsjs_view_228_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="228_1539825877_68" id="227_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>A highly parallel algorithm has been developed and exploited to compute the planetary normal modes of the elastic-gravitational system, which is approximated via the mixed finite element method on unstructured tetrahedral meshes. The eigenmodes of the relevant generalized eigenvalue problem were extracted by a Lanczos approach combined with polynomial filtering. In contrast with the standard shift-and-invert and the full-mode coupling algorithms, the polynomial filtering technique is ideally suited for solving large-scale 3-D interior eigenvalue problems since it significantly enhances the memory and computational efficiency without loss of accuracy.  The parallel efficiency and scalability of this approach are demonstrated on Stampede2 at the Texas Advanced Computing Center. To our knowledge, this is the first time that the direct calculation of the normal modes of 3-D strongly heterogeneous planets, in particular, Earth and Mars, is made feasible via a combination of multiple matrix-free methods and a separation of the essential spectra.</blockquote></div></div></div></div><a href="includes/files/pap294s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack106"></a><div class="section-title">Architectures</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Data Analytics, Networks, Tech Program Reg Pass</span><br /><div class="session-title">Next-Generation Networking</div><div class="slot-entry"><a name="pap147"></a><div class="slot-title">Exploiting Idle Resources in a High-Radix Switch for Supplemental Storage</div><div><span class="BP award">Best Paper Finalists</span></div><div class="slot-authors">Matthias A. Blumrich, Nan Jiang, and Larry R. Dennison (Nvidia Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_230_1539825877_68" onclick="$('#vhsjs_view_230_1539825877_68').hide();
                $('#vhsjs_hide_230_1539825877_68').show();
                $('#229_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_230_1539825877_68" onclick="$('#229_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_230_1539825877_68').hide();
                $('#vhsjs_view_230_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="230_1539825877_68" id="229_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>A general-purpose switch for a high-performance network is usually designed with symmetric ports providing credit-based flow control and error recovery via link-level retransmission. Because port buffers must be sized for the longest links and modern asymmetric network topologies have a wide range of link lengths, we observe that there can be a significant amount of unused buffer memory, particularly in edge switches. We also observe that the tiled architecture used in many high-radix switches contains an abundance of internal bandwidth. We combine these observations to create a new switch architecture that allows ports to stash packets in unused buffers on other ports, accessible via excess internal bandwidth in the tiled switch. We explore this architecture through two use cases: end-to-end resilience and congestion mitigation. We find that stashing is highly effective and does not negatively impact network performance.</blockquote></div></div></div></div><a href="includes/files/pap147s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap163"></a><div class="slot-title">Fine-Grained, Multi-Domain Network Resource Abstraction as a Fundamental Primitive to Enable High-Performance, Collaborative Data Sciences</div><div class="slot-authors">Qiao Xiang (Yale University); J. Jensen Zhang, X. Tony Wang, and Y. Jace Liu (Tongji University); Chin Guok (Lawrence Berkeley National Laboratory); Franck Le (IBM); John MacAuley (Lawrence Berkeley National Laboratory); Harvey Newman (California Institute of Technology); and Y. Richard Yang (Yale University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_232_1539825877_68" onclick="$('#vhsjs_view_232_1539825877_68').hide();
                $('#vhsjs_hide_232_1539825877_68').show();
                $('#231_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_232_1539825877_68" onclick="$('#231_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_232_1539825877_68').hide();
                $('#vhsjs_view_232_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="232_1539825877_68" id="231_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>Multi-domain network resource reservation systems are being deployed, driven by the demand and substantial benefits of providing predictable network resources. However, a major lack of existing systems is their coarse granularity, due to the participating networks’ concern of revealing sensitive information, which can result in substantial inefficiencies. This paper presents Mercator, a novel multi-domain network resource discovery system to provide fine-grained, global network resource information, for collaborative sciences. The foundation of Mercator is a resource abstraction through algebraic-expression enumeration (i.e., linear inequalities/equations), as a compact representation of the available bandwidth in multi-domain networks. In addition, we develop an obfuscating protocol, to address the privacy concerns by ensuring that no participant can associate the algebraic expressions with the corresponding member networks. We also introduce a superset projection technique to increase Mercator’s scalability. Finally, we implement Mercator and demonstrate both its efficiency and efficacy through extensive experiments using real topologies and traces.</blockquote></div></div></div></div><a href="includes/files/pap163s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap157"></a><div class="slot-title">Light-Weight Protocols for Wire-Speed Ordering</div><div class="slot-authors">Hans Eberle and Larry Dennison (Nvidia Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_234_1539825877_68" onclick="$('#vhsjs_view_234_1539825877_68').hide();
                $('#vhsjs_hide_234_1539825877_68').show();
                $('#233_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_234_1539825877_68" onclick="$('#233_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_234_1539825877_68').hide();
                $('#vhsjs_view_234_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="234_1539825877_68" id="233_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>We describe light-weight protocols for selective packet ordering in out-of-order networks that carry memory traffic. The protocols are designed for heterogeneous high-performance systems, in particular, accelerated systems with endpoints that have few resources available for interfacing the network.<br><br>The protocols preserve the semantics of a relaxed memory ordering model as adopted by highly-threaded many-core processors and accelerators.<br><br>The protocols achieve link-rate performance through the following techniques: (1) speculative connection setup avoids round-trip delays found in protocols with little knowledge about endpoint resources, (2) target-side ordering avoids round-trip delays found in source-side ordering mechanisms, (3) fine-grained ordering removes dependencies unwarranted by program code avoiding cumulative ordering dependencies caused by coarse-grained ordering, (4) ordering relaxations and optimizations for producer/consumer communication patterns.<br><br>We describe two ordering protocols that provide (1) strict sequential ordering and (2) relaxed ordering for multi-packet transfers. The protocols impose no restrictions on routing, including multipath routing.</blockquote></div></div></div></div><a href="includes/files/pap157s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, Data Analytics, Deep Learning, Networks, Scientific Computing, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Large-Scale Algorithms</div><div class="slot-entry"><a name="pap171"></a><div class="slot-title">Large-Scale Hierarchical K-Means for Heterogeneous Many-Core Supercomputers</div><div class="slot-authors">Liandeng Li (Tsinghua University; National Supercomputing Center, Wuxi); Teng Yu (University of St Andrews); Wenlai Zhao and Haohuan Fu (Tsinghua University; National Supercomputing Center, Wuxi); Chenyu Wang (University of St Andrews; National Supercomputing Center, Wuxi); Li Tan (Beijing Technology and Business University); Guangwen Yang (Tsinghua University; National Supercomputing Center, Wuxi); and John Thomson (University of St Andrews)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_236_1539825877_68" onclick="$('#vhsjs_view_236_1539825877_68').hide();
                $('#vhsjs_hide_236_1539825877_68').show();
                $('#235_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_236_1539825877_68" onclick="$('#235_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_236_1539825877_68').hide();
                $('#vhsjs_view_236_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="236_1539825877_68" id="235_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>This paper presents a novel design and implementation of k-means clustering algorithm targeting the Sunway TaihuLight supercomputer. We introduce a multi-level parallel partition approach that not only partitions by dataflow and centroid, but also by dimension. Our multi-level (nkd) approach unlocks the potential of the hierarchical parallelism in the SW26010 heterogeneous many-core processor and the system architecture of the supercomputer. <br><br>Our design is able to process large-scale clustering problems with up to 196,608 dimensions and over 160,000 targeting centroids, while maintaining high performance and high scalability, significantly improving the capability of k-means over previous approaches. The evaluation shows our implementation achieves performance of less than 18 seconds per iteration for a large-scale clustering case with 196,608 data dimensions and 2,000 centroids by applying 4,096 nodes (1,064,496 cores) in parallel, making k-means a more feasible solution for complex scenarios.</blockquote></div></div></div></div><a href="includes/files/pap171s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap140"></a><div class="slot-title">TriCore: Parallel Triangle Counting on GPUs</div><div class="slot-authors">Yang Hu (George Washington University); Hang Liu (University of Massachusetts, Lowell); and H. Howie Huang (George Washington University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_238_1539825877_68" onclick="$('#vhsjs_view_238_1539825877_68').hide();
                $('#vhsjs_hide_238_1539825877_68').show();
                $('#237_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_238_1539825877_68" onclick="$('#237_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_238_1539825877_68').hide();
                $('#vhsjs_view_238_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="238_1539825877_68" id="237_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>Triangle counting algorithm enumerates the triangles in a graph by identifying the common neighbors between two vertices of every edge. In this work, we present TriCore, a new GPU-based high-performance and scalable triangle counting system that consists of three main techniques. First, we design a binary search based counting algorithm that tremendously increases both thread parallelism and memory performance. Second, TriCore exploits a 2-D partition method to distribute the CSR representation across multiple GPUs, combined with a new streaming buffer to load the edge list from outside of GPUs. Third, we develop a dynamic workload management technique to balance the workload across multiple GPUs. Our evaluation demonstrates TriCore is 22× faster than the state-of-the-art parallel triangle counting projects. In addition, TriCore can not only process big graphs that are significant larger than the memory size of one GPU but also achieve 24× speedup when scaling to 32 GPUs.</blockquote></div></div></div></div><a href="includes/files/pap140s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap141"></a><div class="slot-title">Distributed-Memory Hierarchical Compression of Dense SPD Matrices</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Chenhan D. Yu (University of Texas), Severin Reiz (Technical University Munich), and George Biros (University of Texas)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_240_1539825877_68" onclick="$('#vhsjs_view_240_1539825877_68').hide();
                $('#vhsjs_hide_240_1539825877_68').show();
                $('#239_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_240_1539825877_68" onclick="$('#239_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_240_1539825877_68').hide();
                $('#vhsjs_view_240_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="240_1539825877_68" id="239_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>We present a distributed-memory algorithm for the hierarchical compression of SPD matrices. Our method is based on GOFMM, an algorithm that appeared in doi:10.1145/3126908.3126921.<br><br>For many SPD matrices, GOFMM enables compression and approximate matrix-vector multiplication in NlogN time---as opposed to quadratic work required for a dense matrix. But GOFMM supports only shared memory parallelism. In this paper, we use the message passing interface, extending the ideas of GOFMM to the distributed memory setting. We also introduce an asynchronous algorithm for faster multiplication. We present different usage scenarios of SPD matrices that are related to graphs, neural-networks, and covariance operators. We also compare with STRUMPACK, which, to our knowledge, is the only other parallel software that can compress arbitrary SPD matrices. In our largest run, we were able to compress a 67M-by-67M matrix within three minutes and perform a multiplication with 512 vectors within 5 seconds on 6,144 Intel Skylake cores.</blockquote></div></div></div></div><a href="includes/files/pap141s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, MPI, Networks, Performance, Programming Systems, State of the Practice, Tech Program Reg Pass</span><br /><div class="session-title">MPI Optimization and Characterization</div><div class="slot-entry"><a name="pap504"></a><div class="slot-title">Cooperative Rendezvous Protocols for Improved Performance and Overlap</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">S. Chakraborty, M. Bayatpour, J. Hashmi, H. Subramoni, and D. K. Panda (Ohio State University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_242_1539825877_68" onclick="$('#vhsjs_view_242_1539825877_68').hide();
                $('#vhsjs_hide_242_1539825877_68').show();
                $('#241_1539825877_68').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_242_1539825877_68" onclick="$('#241_1539825877_68').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_242_1539825877_68').hide();
                $('#vhsjs_view_242_1539825877_68').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="242_1539825877_68" id="241_1539825877_68" style="display: none"><div class="arrow-slidedown"><blockquote>With the emergence of larger multi-/many-core clusters, performance of large message communication is becoming more important. MPI libraries use different Rendezvous protocols to perform large message communication. However, existing Rendezvous protocols do not consider the overall communication pattern and make optimal use of the Sender and the Receiver CPUs. In this work, we propose a cooperative Rendezvous protocol that can provide up to 2x improvement in intra-node bandwidth and latency for large messages. We also propose a scheme to dynamically choose the best Rendezvous protocol for each message based on the communication pattern.  Finally, we show how these improvements can increase the overlap of computation with intra-node and inter-node communication, and lead to application level benefits. We evaluate proposed designs on three different architectures including Intel Xeon, Knights Landing, and OpenPOWER with different HPC applications and obtain benefits up to 19% with Graph500, 16% with CoMD, and 10% with MiniGhost.</blockquote></div></div></div></div><a href="includes/files/pap504s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap463"></a><div class="slot-title">Framework for Scalable Intra-Node Collective Operations Using Shared Memory</div><div class="slot-authors">Surabhi Jain, Rashid Kaleem, Marc Gamell Balmana, Akhil Langer, Dmitry Durnov, Alexander Sannikov, and Maria Garzaran (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_244_1539825877_69" onclick="$('#vhsjs_view_244_1539825877_69').hide();
                $('#vhsjs_hide_244_1539825877_69').show();
                $('#243_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_244_1539825877_69" onclick="$('#243_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_244_1539825877_69').hide();
                $('#vhsjs_view_244_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="244_1539825877_69" id="243_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>Collective operations are used in MPI programs to express common communication patterns, collective computations, or synchronizations. In many collectives, such as barrier or allreduce, the intra-node component of the collective is in the critical path, as the inter-node communication cannot start until the intra-node component has been executed. Thus, with increasing number of core counts in each node, intra-node optimizations that leverage the intra-node shared memory become increasingly important.<br><br>In this paper, we focus on the performance benefit of optimizing intra-node collectives using shared memory. We optimize several collectives using the primitives in broadcast and reduce as building blocks for other collectives. A comparison of our implementation on top of MPICH shows significant performance speedups with respect to the original MPICH implementation, MVAPICH, and OpenMPI, among others.</blockquote></div></div></div></div><a href="includes/files/pap463s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap537"></a><div class="slot-title">Characterization of MPI Usage on a Production Supercomputer</div><div class="slot-authors">Sudheer Chunduri, Scott Parker, Pavan Balaji, Kevin Harms, and Kalyan Kumaran (Argonne National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_246_1539825877_69" onclick="$('#vhsjs_view_246_1539825877_69').hide();
                $('#vhsjs_hide_246_1539825877_69').show();
                $('#245_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_246_1539825877_69" onclick="$('#245_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_246_1539825877_69').hide();
                $('#vhsjs_view_246_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="246_1539825877_69" id="245_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>MPI is the most prominent programming model used in scientific computing today.  Despite it's importance, however, how scientific applications use it in production is not very well understood due to the lack of low overhead profiling tools.   We used a lightweight profiling tool, called autoperf, to log the MPI usage characteristics of production applications on a large supercomputing system (Mira) and its corresponding development system (Cetus).  Autoperf limits the amount of information that it records in order to keep the overhead to a minimum while still storing enough data to derive useful insights.  MPI usage statistics have been collected for over 100K jobs that were run within a 2-year period and are analyzed.  The analysis of this data is intended as a mechanism to provide useful insights for MPI developers and network hardware developers for their next generation of improvements, and for supercomputing center operators for their next system procurements.</blockquote></div></div></div></div><a href="includes/files/pap537s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, Memory, Networks, Parallel Programming Languages, Libraries, and Models, Power, Programming Systems, Scheduling, Tech Program Reg Pass</span><br /><div class="session-title">Task-Based Programming</div><div class="slot-entry"><a name="pap490"></a><div class="slot-title">Dynamic Tracing: Memoization of Task Graphs for Dynamic Task-Based Runtimes</div><div class="slot-authors">Wonchan Lee (Stanford University), Elliott Slaughter (SLAC National Accelerator Laboratory), Michael Bauer and Sean Treichler (Nvidia Corporation), Todd Warszawski (Stanford University), Michael Garland (Nvidia Corporation), and Alex Aiken (Stanford University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_248_1539825877_69" onclick="$('#vhsjs_view_248_1539825877_69').hide();
                $('#vhsjs_hide_248_1539825877_69').show();
                $('#247_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_248_1539825877_69" onclick="$('#247_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_248_1539825877_69').hide();
                $('#vhsjs_view_248_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="248_1539825877_69" id="247_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>Many recent programming systems for both supercomputing and data center workloads generate task graphs to express computations that run on parallel and distributed machines. Due to the overhead associated with constructing these graphs the dependence analysis that generates them is often statically computed and memoized, and the resulting graph executed repeatedly at runtime. However, many applications require a dynamic dependence analysis due to data dependent behavior, but there are new challenges in capturing and re-executing task graphs at runtime. In this work, we introduce dynamic tracing, a technique to capture a dynamic dependence analysis of a trace that generates a task graph, and replay it. We show that an implementation of dynamic tracing improves strong scaling by an average of 4.9X and up to 7.0X on a suite of already optimized benchmarks.</blockquote></div></div></div></div><a href="includes/files/pap490s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap338"></a><div class="slot-title">Runtime-Assisted Cache Coherence Deactivation in Task Parallel Programs</div><div class="slot-authors">Paul Caheny (Barcelona Supercomputing Center, Polytechnic University of Catalonia); Lluc Alvarez (Barcelona Supercomputing Center); Mateo Valero and Miquel Moretó (Barcelona Supercomputing Center, Polytechnic University of Catalonia); and Marc Casas (Barcelona Supercomputing Center)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_250_1539825877_69" onclick="$('#vhsjs_view_250_1539825877_69').hide();
                $('#vhsjs_hide_250_1539825877_69').show();
                $('#249_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_250_1539825877_69" onclick="$('#249_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_250_1539825877_69').hide();
                $('#vhsjs_view_250_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="250_1539825877_69" id="249_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>With increasing core counts, the scalability of directory-based cache coherence has become a challenging problem. To reduce the area and power needs of the directory, recent proposals reduce its size by classifying data as private or shared, and disable coherence for private data. However, existing classification methods suffer from inaccuracies and require complex hardware support with limited scalability.<br><br>This paper proposes a hardware/software co-designed approach: the runtime system identifies data that is guaranteed by the programming model semantics to not require coherence and notifies the microarchitecture. The microarchitecture deactivates coherence for this private data and powers off unused directory capacity. Our proposal reduces directory accesses to just 26% of the baseline system and supports a 64× smaller directory with only 2.8% performance degradation. By dynamically calibrating the directory size, our proposal saves 86% of dynamic energy consumption in the directory without harming performance.</blockquote></div></div></div></div><a href="includes/files/pap338s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap547"></a><div class="slot-title">A Divide and Conquer Algorithm for DAG Scheduling Under Power Constraints</div><div class="slot-authors">Gökalp Demirci, Ivana Marincic, and Henry Hoffmann (University of Chicago)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_252_1539825877_69" onclick="$('#vhsjs_view_252_1539825877_69').hide();
                $('#vhsjs_hide_252_1539825877_69').show();
                $('#251_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_252_1539825877_69" onclick="$('#251_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_252_1539825877_69').hide();
                $('#vhsjs_view_252_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="252_1539825877_69" id="251_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>We consider the problem of scheduling a parallel computation–represented as a directed acyclic graph (DAG)–on a distributed parallel system with a global resource constraint–specifically a global power budget–and configurable resources, allowing a range of different power/performance tradeoffs. There is a rich body of literature on the independent problems of (1) scheduling DAGs and (2) scheduling independent applications under resource constraints. Very little, however, is known about the combined problem of scheduling DAGs under resource constraints. We present a novel approximation algorithm using a divide-and-conquer method for minimizing application execution time. We prove that the length of the schedule returned by our algorithm is always within O(log n)-factor of the optimum that can be achieved with selection of configurations for the tasks. We implement and test our algorithm on simulations of real application DAGs. We find that our divide-and-conquer method improves performance by up to 75% compared to greedy scheduling algorithms.</blockquote></div></div></div></div><a href="includes/files/pap547s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Networks, Performance, Scientific Computing, State of the Practice, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Large Scale System Deployments</div><div class="slot-entry"><a name="pap277"></a><div class="slot-title">The Design, Deployment, and Evaluation of the CORAL Pre-Exascale Systems</div><div class="slot-authors">Sudharshan S. Vazhkudai (Oak Ridge National Laboratory); Bronis R. de Supinski (Lawrence Livermore National Laboratory); Arthur S. Bland and Al Geist (Oak Ridge National Laboratory); James Sexton and Jim Kahle (IBM); Christopher J. Zimmer, Scott Atchley, Sarp H. Oral, Don E. Maxwell, and Veronica G. Vergara Larrea (Oak Ridge National Laboratory); Adam Bertsch and Robin Goldstone (Lawrence Livermore National Laboratory); Wayne Joubert (Oak Ridge National Laboratory); Chris Chambreau (Lawrence Livermore National Laboratory); David Appelhans and Robert Blackmore (IBM); Ben Casses (Lawrence Livermore National Laboratory); George Chochia and Gene Davison (IBM); Matthew A. Ezell (Oak Ridge National Laboratory); Tom Gooding (IBM); Elsa Gonsiorowski (Lawrence Livermore National Laboratory); Leopold Grinberg, Bill Hanson, and Bill Hartner (IBM); Ian Karlin and Matthew L. Leininger (Lawrence Livermore National Laboratory); Dustin Leverman (Oak Ridge National Laboratory); Chris Marroquin (IBM); Adam Moody (Lawrence Livermore National Laboratory); Martin Ohmacht (IBM); Ramesh Pankajakshan (Lawrence Livermore National Laboratory); Fernando Pizzano (IBM); James H. Rogers (Oak Ridge National Laboratory); Bryan Rosenburg (IBM); Drew Schmidt, Mallikarjun Shankar, and Feiyi Wang (Oak Ridge National Laboratory); Py Watson (Lawrence Livermore National Laboratory); Bob Walkup (IBM); Lance D. Weems (Lawrence Livermore National Laboratory); and Junqi Yin (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_254_1539825877_69" onclick="$('#vhsjs_view_254_1539825877_69').hide();
                $('#vhsjs_hide_254_1539825877_69').show();
                $('#253_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_254_1539825877_69" onclick="$('#253_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_254_1539825877_69').hide();
                $('#vhsjs_view_254_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="254_1539825877_69" id="253_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>CORAL, the Collaboration of Oak Ridge, Argonne and Livermore, is fielding two similar IBM systems, Summit and Sierra, with NVIDIA GPUs that will replace the existing Titan and Sequoia systems. Summit and Sierra are currently ranked No. 1 and No. 3, respectively, on the Top500 list. We discuss the design and key differences of the systems. Our evaluation of the systems highlights the following. Applications that fit in HBM see the most benefit and may prefer more GPUs; however, for some applications, the CPU-GPU bandwidth is more important than the number of GPUs. The node-local burst buffer scales linearly, and can achieve a 4X improvement over the parallel file system for large jobs; smaller jobs, however, may benefit from writing directly to the PFS. Finally, several CPU, network and memory bound analytics and GPU-bound deep learning codes achieve up to a 11X and 79X speedup/node, respectively over Titan.</blockquote></div></div></div></div><a href="includes/files/pap277s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap449"></a><div class="slot-title">Best Practices and Lessons from Deploying and Operating a Sustained-Petascale System: The Blue Waters Experience</div><div class="slot-authors">Gregory H. Bauer, Brett Bode, Jeremy Enos, William T. Kramer, Scott Lathrop, Celso L. Mendes, and Roberto R. Sisneros (University of Illinois, National Center for Supercomputing Applications)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_256_1539825877_69" onclick="$('#vhsjs_view_256_1539825877_69').hide();
                $('#vhsjs_hide_256_1539825877_69').show();
                $('#255_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_256_1539825877_69" onclick="$('#255_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_256_1539825877_69').hide();
                $('#vhsjs_view_256_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="256_1539825877_69" id="255_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>Building and operating versatile extreme-scale computing systems that work productively for a range of frontier research domains present many challenges and opportunities. Solutions created, experiences acquired, and lessons learned, while rarely published, could drive the development of new methods and practices and raise the bar for all organizations supporting research, scholarship, and education. This paper describes the methods and procedures developed for deploying, supporting, and continuously improving the Blue Waters system and its services during the last five years. Being the first US sustained-petascale computing platform available to the open-science community, the Blue Waters project pioneered various unique practices that we are sharing to be adopted and further improved by the community. We present our support and service methodologies, and the leadership practices employed for ensuring that the system stays highly efficient and productive. We also provide the return on investment summaries related to deploying and operating the system.</blockquote></div></div></div></div><a href="includes/files/pap449s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap346"></a><div class="slot-title">Performance Evaluation of a Vector Supercomputer SX-Aurora TSUBASA</div><div class="slot-authors">Kazuhiko Komatsu (Tohoku University); Shintaro Momose, Yoko Isobe, Osamu Watanabe, and Akihiro Musa (Tohoku University, NEC Corporation); Mitsuo Yokokawa (Kobe University, NEC Corporation); Toshikazu Aoyama (NEC Corporation); and Masayuki Sato and Hiroaki Kobayashi (Tohoku University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_258_1539825877_69" onclick="$('#vhsjs_view_258_1539825877_69').hide();
                $('#vhsjs_hide_258_1539825877_69').show();
                $('#257_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_258_1539825877_69" onclick="$('#257_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_258_1539825877_69').hide();
                $('#vhsjs_view_258_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="258_1539825877_69" id="257_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>A new SX-Aurora TSUBASA vector supercomputer has been released with a new system architecture and a new execution model to achieve high sustained performance, especially for memory-intensive applications. In SX-Aurora TSUBASA, the vector host (VH) of a standard x86 Linux node is attached to the vector engine (VE) of a newly developed vector processor.  An application is executed on the VE, and only system calls are offloaded to the VH. This new execution model can avoid redundant data transfers between a VH and a VE that can easily become a bottleneck in the conventional execution model. This paper examines the potential of SX-Aurora TSUBASA. First, the basic performance of SX-Aurora TSUBASA is clarified by evaluating benchmark programs. Then, the effectiveness of the new execution model is examined by using a microbenchmark.  Finally, the high potential of SX-Aurora TSUBASA is clarified through evaluations of practical applications.</blockquote></div></div></div></div><a href="includes/files/pap346s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Architectures, Compiler Analysis and Optimization, Floating Point, Performance, Precision, Programming Systems, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Arithmetic and Optimization</div><div class="slot-entry"><a name="pap431"></a><div class="slot-title">Associative Instruction Reordering to Alleviate Register Pressure</div><div class="slot-authors">Prashant Singh Rawat, Aravind Sukumaran-Rajam, and Atanas Rountev (Ohio State University); Fabrice Rastello (French Institute for Research in Computer Science and Automation (INRIA)); Louis-Noel Pouchet (Colorado State University); and P. Sadayappan (Ohio State University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_260_1539825877_69" onclick="$('#vhsjs_view_260_1539825877_69').hide();
                $('#vhsjs_hide_260_1539825877_69').show();
                $('#259_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_260_1539825877_69" onclick="$('#259_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_260_1539825877_69').hide();
                $('#vhsjs_view_260_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="260_1539825877_69" id="259_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>Register allocation is generally considered a practically solved problem. For most applications, the register allocation strategies in production compilers are very effective in controlling the number of loads/stores and register spills. However, existing register allocation strategies are not effective and result in excessive register spilling for computation patterns with a high degree of many-to-many data reuse, e.g., high-order stencils and tensor contractions.  We develop a source-to-source instruction reordering strategy that exploits the flexibility of reordering associative operations to alleviate register pressure.  The developed transformation module implements an adaptable strategy that can appropriately control the degree of instruction-level parallelism, while relieving register pressure.  The effectiveness of the approach is demonstrated through experimental results using multiple production compilers (GCC, Clang/LLVM) and target platforms (Intel Xeon Phi, and Intel x86 multi-core).</blockquote></div></div></div></div><a href="includes/files/pap431s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap464"></a><div class="slot-title">Harnessing GPU's Tensor Cores Fast FP16 Arithmetic to Speedup Mixed-Precision Iterative Refinement Solvers</div><div class="slot-authors">Azzam Haidar (University of Tennessee, Innovative Computing Laboratory); Stan Tomov and Jack Dongarra (University of Tennessee); and Nicholas Higham (University of Manchester, School of Mathematics)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_262_1539825877_69" onclick="$('#vhsjs_view_262_1539825877_69').hide();
                $('#vhsjs_hide_262_1539825877_69').show();
                $('#261_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_262_1539825877_69" onclick="$('#261_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_262_1539825877_69').hide();
                $('#vhsjs_view_262_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="262_1539825877_69" id="261_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>The use of low-precision arithmetic in computing methods has been a powerful tool to accelerate numerous scientific computing applications including Artificial Intelligence. We present an investigation showing that other HPC applications can harness this power too, and in particular, the general HPC problem of solving Ax = b, where A is a large dense matrix, and the solution is needed in FP64 accuracy. Our approach is based on the mixed-precision (FP16->FP64) iterative refinement technique – we generalize and extend prior advances into a framework, for which we develop architecture-specific algorithms and highly-tuned implementations where we show how the use of FP16-TC (tensor cores) arithmetic can provide up to 4X speedup and improve the energy consumption by a factor of 5 achieving 74 Gflop/Watt. This is due to the performance boost that the FP16 (Tensor Cores) provide and to its better accuracy that outperforms the classical FP16.</blockquote></div></div></div></div><a href="includes/files/pap464s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap503"></a><div class="slot-title">ADAPT: Algorithmic Differentiation Applied to Floating-Point Precision Tuning</div><div class="slot-authors">Harshitha Menon (Lawrence Livermore National Laboratory); Michael O. Lam (James Madison University, Lawrence Livermore National Laboratory); and Daniel Osei-Kuffuor, Markus Schordan, Scott Lloyd, Kathryn Mohror, and Jeffrey Hittinger (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_264_1539825877_69" onclick="$('#vhsjs_view_264_1539825877_69').hide();
                $('#vhsjs_hide_264_1539825877_69').show();
                $('#263_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_264_1539825877_69" onclick="$('#263_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_264_1539825877_69').hide();
                $('#vhsjs_view_264_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="264_1539825877_69" id="263_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>HPC applications extensively use floating point arithmetic operations to solve computational problems in various domains. Mixed precision computing, use of lowest precision data type sufficient to achieve a desired accuracy, have been explored to improve performance, reduce power consumption and data movement. Manually optimizing the program to use mixed precision is challenging. In this work, we present ADAPT, an approach for mixed precision analysis on HPC workloads while providing guarantees about the final output error. Our approach uses algorithmic differentiation to accurately estimate the output error for mixed precision configuration. ADAPT provides floating-point precision sensitivity of programs, which highlights regions of the code that that can potentially be converted to lower precision, is used to make algorithmic choices and develop mixed precision configurations. We evaluate ADAPT on six benchmarks and a proxy application and show that we are able to achieve a speedup of 1.2x on the proxy application, LULESH.</blockquote></div></div></div></div><a href="includes/files/pap503s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, GPUs, Linear Algebra, Networks, Resiliency, Tech Program Reg Pass</span><br /><div class="session-title">Resilience III: GPUs</div><div class="slot-entry"><a name="pap247"></a><div class="slot-title">Optimizing Software-Directed Instruction Replication for GPU Error Detection</div><div class="slot-authors">Abdulrahman Mahmoud (University of Illinois) and Siva Kumar Sastry Hari, Michael B. Sullivan, Timothy Tsai, and Stephen W. Keckler (Nvidia Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_266_1539825877_69" onclick="$('#vhsjs_view_266_1539825877_69').hide();
                $('#vhsjs_hide_266_1539825877_69').show();
                $('#265_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_266_1539825877_69" onclick="$('#265_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_266_1539825877_69').hide();
                $('#vhsjs_view_266_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="266_1539825877_69" id="265_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>Application execution on safety-critical and high-performance computer systems must be resilient to transient errors. As GPUs become more pervasive in such systems, they must supplement ECC/parity for major storage structures with reliability techniques that cover more of the GPU hardware logic.  Instruction duplication has been explored for CPU resilience; however, it has never been studied in the context of GPUs, and it is unclear whether the performance and design choices it presents makes it a feasible GPU solution. This paper describes a practical methodology to employ instruction duplication for GPUs and identifies implementation challenges that can incur high overheads (69% on average). It explores GPU-specific software optimizations that trade fine-grained recoverability for performance. It also proposes simple ISA extensions with limited hardware changes and area costs to further improve performance, cutting the runtime overheads by more than half to an average of 30%.</blockquote></div></div></div></div><a href="includes/files/pap247s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap244"></a><div class="slot-title">Fault Tolerant One-Sided Matrix Decompositions on Heterogeneous Systems with GPUs</div><div class="slot-authors">Jieyang Chen, Hongbo Li, Sihuan Li, and Xin Liang (University of California, Riverside); Panruo Wu (University of Houston); Dingwen Tao (University of Alabama); Kaiming Ouyang, Yuanlai Liu, and Kai Zhao (University of California, Riverside); Qiang Guan (Kent State University); and Zizhong Chen (University of California, Riverside)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_268_1539825877_69" onclick="$('#vhsjs_view_268_1539825877_69').hide();
                $('#vhsjs_hide_268_1539825877_69').show();
                $('#267_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_268_1539825877_69" onclick="$('#267_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_268_1539825877_69').hide();
                $('#vhsjs_view_268_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="268_1539825877_69" id="267_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>Current algorithm-based fault tolerance (ABFT) approach for one-sided matrix decomposition on heterogeneous systems with GPUs have following limitations: (1) they do not provide sufficient protection as most of them only maintain checksum in one dimension; (2) their checking scheme is not efficient due to redundant checksum verifications; (3) they fail to protect PCIe communication; (4) the checksum calculation based on a special type of matrix multiplication is far from efficient. By overcoming the above limitations, we design an efficient ABFT approach providing stronger protection for one-sided matrix decomposition methods on heterogeneous systems. First, we provide full matrix protection by using checksums in two dimensions. Second, our checking scheme is more efficient by prioritizing the checksum verification according to the sensitivity of matrix operations to soft errors. Third, we protect PCIe communication by reordering checksum verifications and decomposition steps. Fourth, we accelerate the checksum calculation by 1.7x via better utilizing GPUs.</blockquote></div></div></div></div><a href="includes/files/pap244s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap430"></a><div class="slot-title">PRISM: Predicting Resilience of GPU Applications Using Statistical Methods</div><div class="slot-authors">Charu Kalra, Fritz Previlon, and Xiangyu Li (Northeastern University); Norman Rubin (Nvidia Corporation); and David Kaeli (Northeastern University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_270_1539825877_69" onclick="$('#vhsjs_view_270_1539825877_69').hide();
                $('#vhsjs_hide_270_1539825877_69').show();
                $('#269_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_270_1539825877_69" onclick="$('#269_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_270_1539825877_69').hide();
                $('#vhsjs_view_270_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="270_1539825877_69" id="269_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>As Graphics Processing Units (GPUs) become more pervasive in HPC and safety-critical domains, ensuring that GPU applications can be protected from data corruption grows in importance. Despite prior efforts to mitigate errors, we still lack a clear understanding of how resilient these applications are in the presence of transient faults.  Due to the random nature of these faults, predicting whether they will alter the program output is a challenging problem. In this paper, we build a framework named PRISM, which uses a systematic approach to predict failures in GPU programs. PRISM extracts micro-architecture agnostic features to characterize program resiliency, which serve as predictors in our statistical model. PRISM enables us to predict failures in applications without running exhaustive fault-injection campaigns on a GPU, thereby reducing the error estimation effort. PRISM can also be used to gain insight into potential architectural support required to improve the reliability of GPU applications.</blockquote></div></div></div></div><a href="includes/files/pap430s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Data Management, File Systems, Networks, State of the Practice, System Software, Workflows, Tech Program Reg Pass</span><br /><div class="session-title">File Systems: Data Movement and Provenance</div><div class="slot-entry"><a name="pap407"></a><div class="slot-title">Dac-Man: Data Change Management for Scientific Datasets on HPC Systems</div><div class="slot-authors">Devarshi Ghoshal, Lavanya Ramakrishnan, and Deborah Agarwal (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_272_1539825877_69" onclick="$('#vhsjs_view_272_1539825877_69').hide();
                $('#vhsjs_hide_272_1539825877_69').show();
                $('#271_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_272_1539825877_69" onclick="$('#271_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_272_1539825877_69').hide();
                $('#vhsjs_view_272_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="272_1539825877_69" id="271_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>Scientific data is growing rapidly and often changes due to instrument configurations, software updates, or quality assessments. These changes in datasets can result in significant waste of compute and storage resources on HPC systems as downstream pipelines are reprocessed. Data changes need to be detected, tracked, and analyzed for understanding the impact of data change, managing data provenance, and making efficient and effective decisions about reprocessing and use of HPC resources. Existing methods for identifying and capturing change are often manual, domain-specific, and error-prone and do not scale to large scientific datasets. In this paper, we describe the design and implementation of Dac-Man framework, which identifies, captures, and manages change in large scientific datasets, and enables plug-in of domain-specific change analysis with minimal user effort. Our evaluations show that it can retrieve file changes from directories containing millions of files and terabytes of data in less than a minute.</blockquote></div></div></div></div><a href="includes/files/pap407s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap521"></a><div class="slot-title">Stacker: An Autonomic Data Movement Engine for Extreme-Scale Data Staging-Based In Situ Workflows</div><div class="slot-authors">Pradeep Subedi, Philip Davis, and Shaohua Duan (Rutgers University); Scott Klasky (Oak Ridge National Laboratory); Hemanth Kolla (Sandia National Laboratories); and Manish Parashar (Rutgers University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_274_1539825877_69" onclick="$('#vhsjs_view_274_1539825877_69').hide();
                $('#vhsjs_hide_274_1539825877_69').show();
                $('#273_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_274_1539825877_69" onclick="$('#273_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_274_1539825877_69').hide();
                $('#vhsjs_view_274_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="274_1539825877_69" id="273_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>Data staging and in situ workflows are being explored extensively as an approach to address data-related costs at very large scales. However, the impact of emerging storage architectures (e.g., deep memory hierarchies and burst buffers) upon data staging solutions remains a challenge. In this paper, we investigate how burst buffers can be effectively used by data staging solutions, for example, as a persistence storage tier of the memory hierarchy. Furthermore, we use machine learning based prefetching techniques to move data between the storage levels in an autonomous manner. We also present Stacker, a prototype of the proposed solutions implemented within the Data\-Spaces data staging service, and experimentally evaluate its performance and scalability using the S3D combustion workflow on current leadership class platforms. Our experiments demonstrate that Stacker achieves low latency, high volume data-staging with low overhead as compared to in-memory staging services for production scientific workflows.</blockquote></div></div></div></div><a href="includes/files/pap521s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap206"></a><div class="slot-title">A Year in the Life of a Parallel File System</div><div class="slot-authors">Glenn K. Lockwood (Lawrence Berkeley National Laboratory), Shane Snyder (Argonne National Laboratory), Teng Wang and Suren Byna (Lawrence Berkeley National Laboratory), Philip Carns (Argonne National Laboratory), and Nicholas J. Wright (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_276_1539825877_69" onclick="$('#vhsjs_view_276_1539825877_69').hide();
                $('#vhsjs_hide_276_1539825877_69').show();
                $('#275_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_276_1539825877_69" onclick="$('#275_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_276_1539825877_69').hide();
                $('#vhsjs_view_276_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="276_1539825877_69" id="275_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>I/O performance is a critical aspect of data-intensive scientific computing.  We seek to advance the state of the practice in understanding and diagnosing I/O performance issues through investigation of a comprehensive I/O performance data set that captures a full year of production storage activity at two leadership-scale computing facilities.  We demonstrate techniques to identify regions of interest, perform focused investigations of both long-term trends and transient anomalies, and uncover the contributing factors that lead to performance fluctuation.<br><br>We find that a year in the life of a parallel file system is comprised of distinct regions of long-term performance variation in addition to short-term performance transients.  We demonstrate how systematic identification of these performance regions, combined with comprehensive analysis, allows us to isolate the factors contributing to different performance maladies at different time scales.  From this, we present specific lessons learned and important considerations for HPC storage practitioners.</blockquote></div></div></div></div><a href="includes/files/pap206s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack108"></a><div class="section-title">Clouds and Distributed Computing</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Clouds and Distributed Computing, File Systems, I/O, Storage, Tech Program Reg Pass</span><br /><div class="session-title">Data and Storage</div><div class="slot-entry"><a name="pap165"></a><div class="slot-title">SP-Cache: Load-Balanced, Redundancy-Free Cluster Caching with Selective Partition</div><div class="slot-authors">Yinghao Yu, Renfei Huang, Wei Wang, Jun Zhang, and Khaled Ben Letaief (Hong Kong University of Science and Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_278_1539825877_69" onclick="$('#vhsjs_view_278_1539825877_69').hide();
                $('#vhsjs_hide_278_1539825877_69').show();
                $('#277_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_278_1539825877_69" onclick="$('#277_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_278_1539825877_69').hide();
                $('#vhsjs_view_278_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="278_1539825877_69" id="277_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>Data-intensive clusters increasingly employ in-memory solutions to improve I/O performance. However, the routinely observed file popularity skew and load imbalance create hotspots, which significantly degrades the benefits of in-memory solutions. Common approaches to tame load imbalance include copying multiple replicas of hot files and creating parity chunks using storage codes. Yet, these techniques either suffer from high memory redundancy or incur non-trivial encoding/decoding overhead. In this paper, we propose a different approach to achieve load balancing without memory redundancy or encoding/decoding overhead. Our solution, termed SP-Cache, selectively partitions files based on their popularity and evenly caches those partitions across the cluster. We develop an efficient algorithm to determine the optimal number of partitions for hot files—too few partitions are incapable of mitigating hotspots, while too many are susceptible to stragglers. EC2 deployment and trace-driven simulations show that, compared with existing solutions, SP-Cache reduces the read latencies by up to 40%.</blockquote></div></div></div></div><a href="includes/files/pap165s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap585"></a><div class="slot-title">BESPOKV: Application Tailored Scale-Out Key-Value Stores</div><div class="slot-authors">Ali Anwar (IBM), Yue Cheng (George Mason University), Hai Huang (IBM), Jingoo Han (Virginia Tech), Hyogi Sim (Oak Ridge National Laboratory), Dongyoon Lee (Virginia Tech), Fred Douglis (Perspecta Labs), and Ali R. Butt (Virginia Tech)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_280_1539825877_69" onclick="$('#vhsjs_view_280_1539825877_69').hide();
                $('#vhsjs_hide_280_1539825877_69').show();
                $('#279_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_280_1539825877_69" onclick="$('#279_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_280_1539825877_69').hide();
                $('#vhsjs_view_280_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="280_1539825877_69" id="279_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>Enterprise KV stores are not well suited for HPC applications, and entail customization and cumbersome end-to-end KV design to extract the HPC application needs. In this paper we present BESPOKV, an adaptive, extensible, and scale-out KV store framework. BESPOKV decouples the KV store design into the control plane for distributed management and the data plane for local data store. BESPOKV takes as input a single-server KV store, called a datalet, and transparently enables a scalable and fault-tolerant distributed KV store service. The resulting distributed stores are also adaptive to consistency or topology requirement changes and can be easily extended for new types of services. Experiments show that BESPOKV-enabled distributed KV stores scale horizontally to a large number of nodes, and performs comparably and sometimes better than the state-of-the-art systems.</blockquote></div></div></div></div><a href="includes/files/pap585s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap450"></a><div class="slot-title">Scaling Embedded In Situ Indexing with DeltaFS</div><div class="slot-authors">Qing Zheng, Charles D. Cranor, Danhao Guo, Gregory R. Ganger, George Amvrosiadis, and Garth A. Gibson (Carnegie Mellon University) and Bradley W. Settlemyer, Gary Grider, and Fan Guo (Los Alamos National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_282_1539825877_69" onclick="$('#vhsjs_view_282_1539825877_69').hide();
                $('#vhsjs_hide_282_1539825877_69').show();
                $('#281_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_282_1539825877_69" onclick="$('#281_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_282_1539825877_69').hide();
                $('#vhsjs_view_282_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="282_1539825877_69" id="281_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>Analysis of large-scale simulation output is a core element of scientific inquiry, but analysis queries may experience significant I/O overhead when the data is not structured for efficient retrieval. While in-situ processing allows for improved time-to-insight for many applications, scaling in-situ frameworks to hundreds of thousands of cores can be difficult in practice. The DeltaFS in-situ indexing is a new approach for in-situ processing of massive amounts of data to achieve efficient point and small-range queries. This paper describes the challenges and lessons learned when scaling this in-situ processing function to hundreds of thousands of cores. We propose techniques for scalable all-to-all communication that is memory and bandwidth efficient, concurrent indexing, and specialized LSM-Tree formats. Combining these techniques allows DeltaFS to control the cost of in-situ processing while maintaining 3 orders of magnitude query speedup when scaling alongside the popular VPIC particle-in-cell code to 131,072 cores.</blockquote></div></div></div></div><a href="includes/files/pap450s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Clouds and Distributed Computing, Resource Management, Scheduling, Tech Program Reg Pass</span><br /><div class="session-title">Clouds and Distributed Computing</div><div class="slot-entry"><a name="pap229"></a><div class="slot-title">A Reference Architecture for Datacenter Scheduling: Design, Validation, and Experiments</div><div class="slot-authors">Georgios Andreadis (Delft University of Technology, Vrije University Amsterdam); Laurens Versluis (Vrije University Amsterdam); Fabian Mastenbroek (Delft University of Technology); and Alexandru Iosup (Vrije University Amsterdam, Delft University of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_284_1539825877_69" onclick="$('#vhsjs_view_284_1539825877_69').hide();
                $('#vhsjs_hide_284_1539825877_69').show();
                $('#283_1539825877_69').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_284_1539825877_69" onclick="$('#283_1539825877_69').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_284_1539825877_69').hide();
                $('#vhsjs_view_284_1539825877_69').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="284_1539825877_69" id="283_1539825877_69" style="display: none"><div class="arrow-slidedown"><blockquote>Datacenters act as cloud-infrastructure to stakeholders across industry, government, and academia. To meet growing demand yet operate efficiently, datacenter operators employ increasingly more sophisticated scheduling systems, mechanisms, and policies. Although many scheduling techniques already exist, relatively little research has gone into the abstraction of the scheduling process itself, hampering design, tuning, and comparison of existing techniques. In this work, we propose a reference architecture for datacenter schedulers. The architecture follows five design principles: components with clearly distinct responsibilities, grouping of related components where possible, separation of mechanism from policy, scheduling as complex workflow, and hierarchical multi-scheduler structure. To demonstrate the validity of the reference architecture, we map to it state-of-the-art datacenter schedulers. We find scheduler-stages are commonly underspecified in peer-reviewed publications. Through trace-based simulation and real-world experiments, we show underspecification of scheduler-stages can lead to significant variations in performance.</blockquote></div></div></div></div><a href="includes/files/pap229s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap356"></a><div class="slot-title">Dynamically Negotiating Capacity Between On-Demand and Batch Clusters</div><div class="slot-authors">Feng Liu (University of Minnesota), Kate Keahey (Argonne National Laboratory), Pierre Riteau (University of Chicago), and Jon Weissman (University of Minnesota)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_286_1539825877_7" onclick="$('#vhsjs_view_286_1539825877_7').hide();
                $('#vhsjs_hide_286_1539825877_7').show();
                $('#285_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_286_1539825877_7" onclick="$('#285_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_286_1539825877_7').hide();
                $('#vhsjs_view_286_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="286_1539825877_7" id="285_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>In the era of rapid experimental expansion data analysis needs are rapidly outpacing the capabilities of small institutional clusters and looking to integrate HPC resources into their workflow. We propose one way of reconciling on-demand needs of experimental analytics with the batch managed HPC resources within a system that dynamically moves nodes between an on-demand cluster configured with cloud technology (OpenStack) and a traditional HPC cluster managed by a batch scheduler (Torque). We evaluate this system experimentally both in the context of real-life traces representing two years of a specific institutional need, and via experiments in the context of synthetic traces that capture generalized characteristics of potential batch and on-demand workloads. Our results for the real-life scenario show that our approach could reduce the current investment in on-demand infrastructure by 82% while at the same time improving the mean batch wait time almost by an order of magnitude (8x).</blockquote></div></div></div></div><a href="includes/files/pap356s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap365"></a><div class="slot-title">A Lightweight Model for Right-Sizing Master-Worker Applications</div><div class="slot-authors">Nathaniel Kremer-Herman, Benjamin Tovar, and Douglas Thain (University of Notre Dame)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_288_1539825877_7" onclick="$('#vhsjs_view_288_1539825877_7').hide();
                $('#vhsjs_hide_288_1539825877_7').show();
                $('#287_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_288_1539825877_7" onclick="$('#287_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_288_1539825877_7').hide();
                $('#vhsjs_view_288_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="288_1539825877_7" id="287_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>When running a parallel application at scale, a resource provisioning policy should minimize over-commitment (idle resources) and under-commitment (resource contention). However, users seldom know the quantity of resources to appropriately execute their application. Even with such knowledge, over- and under-commitment of resources may still occur because the application does not run in isolation. It shares resources  such as network and filesystems.<br><br>We formally define the capacity of a parallel application as the quantity of resources that may effectively be provisioned for the best  execution time in an environment.  We present a model to compute an estimate of the capacity of master-worker applications as they run based on execution and data-transfer times. We demonstrate this model with two bioinformatics workflows, a machine learning application, and one synthetic application.  Our results show the model correctly tracks the known value of capacity in scaling,  dynamic task behavior, and with improvements in task throughput.</blockquote></div></div></div></div><a href="includes/files/pap365s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack112"></a><div class="section-title">Compiler Analysis and Optimization</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Architectures, Compiler Analysis and Optimization, Floating Point, Performance, Precision, Programming Systems, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Arithmetic and Optimization</div><div class="slot-entry"><a name="pap431"></a><div class="slot-title">Associative Instruction Reordering to Alleviate Register Pressure</div><div class="slot-authors">Prashant Singh Rawat, Aravind Sukumaran-Rajam, and Atanas Rountev (Ohio State University); Fabrice Rastello (French Institute for Research in Computer Science and Automation (INRIA)); Louis-Noel Pouchet (Colorado State University); and P. Sadayappan (Ohio State University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_290_1539825877_7" onclick="$('#vhsjs_view_290_1539825877_7').hide();
                $('#vhsjs_hide_290_1539825877_7').show();
                $('#289_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_290_1539825877_7" onclick="$('#289_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_290_1539825877_7').hide();
                $('#vhsjs_view_290_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="290_1539825877_7" id="289_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>Register allocation is generally considered a practically solved problem. For most applications, the register allocation strategies in production compilers are very effective in controlling the number of loads/stores and register spills. However, existing register allocation strategies are not effective and result in excessive register spilling for computation patterns with a high degree of many-to-many data reuse, e.g., high-order stencils and tensor contractions.  We develop a source-to-source instruction reordering strategy that exploits the flexibility of reordering associative operations to alleviate register pressure.  The developed transformation module implements an adaptable strategy that can appropriately control the degree of instruction-level parallelism, while relieving register pressure.  The effectiveness of the approach is demonstrated through experimental results using multiple production compilers (GCC, Clang/LLVM) and target platforms (Intel Xeon Phi, and Intel x86 multi-core).</blockquote></div></div></div></div><a href="includes/files/pap431s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap464"></a><div class="slot-title">Harnessing GPU's Tensor Cores Fast FP16 Arithmetic to Speedup Mixed-Precision Iterative Refinement Solvers</div><div class="slot-authors">Azzam Haidar (University of Tennessee, Innovative Computing Laboratory); Stan Tomov and Jack Dongarra (University of Tennessee); and Nicholas Higham (University of Manchester, School of Mathematics)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_292_1539825877_7" onclick="$('#vhsjs_view_292_1539825877_7').hide();
                $('#vhsjs_hide_292_1539825877_7').show();
                $('#291_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_292_1539825877_7" onclick="$('#291_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_292_1539825877_7').hide();
                $('#vhsjs_view_292_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="292_1539825877_7" id="291_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>The use of low-precision arithmetic in computing methods has been a powerful tool to accelerate numerous scientific computing applications including Artificial Intelligence. We present an investigation showing that other HPC applications can harness this power too, and in particular, the general HPC problem of solving Ax = b, where A is a large dense matrix, and the solution is needed in FP64 accuracy. Our approach is based on the mixed-precision (FP16->FP64) iterative refinement technique – we generalize and extend prior advances into a framework, for which we develop architecture-specific algorithms and highly-tuned implementations where we show how the use of FP16-TC (tensor cores) arithmetic can provide up to 4X speedup and improve the energy consumption by a factor of 5 achieving 74 Gflop/Watt. This is due to the performance boost that the FP16 (Tensor Cores) provide and to its better accuracy that outperforms the classical FP16.</blockquote></div></div></div></div><a href="includes/files/pap464s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap503"></a><div class="slot-title">ADAPT: Algorithmic Differentiation Applied to Floating-Point Precision Tuning</div><div class="slot-authors">Harshitha Menon (Lawrence Livermore National Laboratory); Michael O. Lam (James Madison University, Lawrence Livermore National Laboratory); and Daniel Osei-Kuffuor, Markus Schordan, Scott Lloyd, Kathryn Mohror, and Jeffrey Hittinger (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_294_1539825877_7" onclick="$('#vhsjs_view_294_1539825877_7').hide();
                $('#vhsjs_hide_294_1539825877_7').show();
                $('#293_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_294_1539825877_7" onclick="$('#293_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_294_1539825877_7').hide();
                $('#vhsjs_view_294_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="294_1539825877_7" id="293_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>HPC applications extensively use floating point arithmetic operations to solve computational problems in various domains. Mixed precision computing, use of lowest precision data type sufficient to achieve a desired accuracy, have been explored to improve performance, reduce power consumption and data movement. Manually optimizing the program to use mixed precision is challenging. In this work, we present ADAPT, an approach for mixed precision analysis on HPC workloads while providing guarantees about the final output error. Our approach uses algorithmic differentiation to accurately estimate the output error for mixed precision configuration. ADAPT provides floating-point precision sensitivity of programs, which highlights regions of the code that that can potentially be converted to lower precision, is used to make algorithmic choices and develop mixed precision configurations. We evaluate ADAPT on six benchmarks and a proxy application and show that we are able to achieve a speedup of 1.2x on the proxy application, LULESH.</blockquote></div></div></div></div><a href="includes/files/pap503s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack171"></a><div class="section-title">Computational Biology</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Computational Biology, Scientific Computing, Tech Program Reg Pass</span><br /><div class="session-title">Biology Applications</div><div class="slot-entry"><a name="pap410"></a><div class="slot-title">Extreme Scale De Novo Metagenome Assembly</div><div><span class="BP award">Best Paper Finalists</span></div><div class="slot-authors">Evangelos Georganas (Intel Corporation) and Rob Egan, Steven Hofmeyr, Eugene Goltsman, Bill Arndt, Andrew Tritt, Aydin Buluc, Leonid Oliker, and Katherine Yelick (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_296_1539825877_7" onclick="$('#vhsjs_view_296_1539825877_7').hide();
                $('#vhsjs_hide_296_1539825877_7').show();
                $('#295_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_296_1539825877_7" onclick="$('#295_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_296_1539825877_7').hide();
                $('#vhsjs_view_296_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="296_1539825877_7" id="295_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>Metagenome assembly is the process of transforming a set of short, overlapping, and potentially erroneous DNA segments from environmental samples into the accurate representation of the underlying microbiomes's genomes. State-of-the-art tools require large shared memory machines and cannot handle contemporary metagenome datasets that exceed terabytes in size. In this paper, we introduce the metaHipMer pipeline, a high-quality and high-performance metagenome assembler that employs an iterative de Bruijn graph approach. MetaHipMer leverages a specialized scaffolding algorithm that produces long scaffolds and accommodates the idiosyncrasies of metagenomes. MetaHipMer is end-to-end parallelized using the Unified Parallel C language and therefore can run seamlessly on shared and distributed-memory systems. Experimental results show that metaHipMer matches or outperforms the state-of-the-art tools in terms of accuracy. Moreover, metaHipMer scales efficiently to large concurrencies and is able to assemble previously intractable grand challenge metagenomes.</blockquote></div></div></div></div><a href="includes/files/pap410s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap551"></a><div class="slot-title">Optimizing High Performance Distributed Memory Parallel Hash Tables for DNA k-mer Counting</div><div class="slot-authors">Tony C. Pan (Georgia Institute of Technology, School of Computational Science and Engineering); Sanchit Misra (Intel Corporation, Parallel Computing Lab); and Srinivas Aluru (Georgia Institute of Technology, School of Computational Science and Engineering)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_298_1539825877_7" onclick="$('#vhsjs_view_298_1539825877_7').hide();
                $('#vhsjs_hide_298_1539825877_7').show();
                $('#297_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_298_1539825877_7" onclick="$('#297_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_298_1539825877_7').hide();
                $('#vhsjs_view_298_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="298_1539825877_7" id="297_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>High-throughput DNA sequencing is the mainstay of modern genomics research. A common operation used in bioinformatic analysis for many applications of high-throughput sequencing is the counting and indexing of fixed length substrings of DNA sequences called k-mers. Counting k-mers is often accomplished via hashing, and distributed memory k-mer counting algorithms for large data sets are memory access and network communication bound. In this work, we present two optimized distributed parallel hash table techniques that utilize cache friendly algorithms for local hashing, overlapped communication and computation to hide communication costs, and vectorized hash functions that are specialized for k-mer and other short key indices. On 4096 cores of the NERSC Cori supercomputer, our implementation completed index construction and query on an approximately 1 TB human genome dataset in just 11.8 seconds and 5.8 seconds, demonstrating speedups of 2.06x and 3.7x, respectively, over the previous state-of-the-art distributed memory k-mer counter.</blockquote></div></div></div></div><a href="includes/files/pap551s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap120"></a><div class="slot-title">Redesigning LAMMPS for Petascale and Hundred-Billion-Atom Simulation on Sunway TaihuLight</div><div class="slot-authors">Xiaohui Duan, Ping Gao, Tingjian Zhang, Meng Zhang, and Weiguo Liu (Shandong University); Wusheng Zhang, Wei Xue, Haohuan Fu, Lin Gan, and Dexun Chen (Tsinghua University); Xiangxu Meng (Shandong University); and Guangwen Yang (Tsinghua University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_300_1539825877_7" onclick="$('#vhsjs_view_300_1539825877_7').hide();
                $('#vhsjs_hide_300_1539825877_7').show();
                $('#299_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_300_1539825877_7" onclick="$('#299_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_300_1539825877_7').hide();
                $('#vhsjs_view_300_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="300_1539825877_7" id="299_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>Large-scale molecular dynamics (MD) simulations on supercomputers play an increasingly important role in many research areas. In this paper, we present our efforts on redesigning the widely used LAMMPS MD simulator for Sunway TaihuLight supercomputer and its ShenWei many-core architecture (SW26010). The memory constraints of SW26010 bring a number of new challenges for achieving efficient MD implementation on it. In order to overcome these constraints, we employ four levels of optimization: (1) a hybrid memory update strategy; (2) a software cache strategy; (3) customized transcendental math functions; and (4) a full pipeline acceleration. Furthermore, we redesign the code to enable all possible vectorization. Experiments show that our redesigned software on a single SW26010 processor can outperform over 100 E5-2650 cores for running the latest stable release (11Aug17) of LAMMPS. We also achieve a performance of over 2.43 PFlops for a Tersoff simulation when using 16,384 nodes on Sunway TaihuLight.</blockquote></div></div></div></div><a href="includes/files/pap120s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack173"></a><div class="section-title">Computational Physics</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Computational Physics, Scientific Computing, Tech Program Reg Pass</span><br /><div class="session-title">Physics and Tensor Applications</div><div class="slot-entry"><a name="pap502"></a><div class="slot-title">Simulating the Wenchuan Earthquake with Accurate Surface Topography on Sunway TaihuLight</div><div class="slot-authors">Bingwei Chen, Haohuan Fu, Yanwen Wei, and Conghui He (Tsinghua University; National Supercomputing Center, Wuxi); Wenqiang Zhang (University of Science and Technology of China); Yuxuan Li (Tsinghua University; National Supercomputing Center, Wuxi); Wubin Wan and Wei Zhang (National Supercomputing Center, Wuxi); Lin Gan (Tsinghua University; National Supercomputing Center, Wuxi); Wei Zhang and Zhenguo Zhang (Southern University of Science and Technology, China); Guangwen Yang (Tsinghua University; National Supercomputing Center, Wuxi); and Xiaofei Chen (Southern University of Science and Technology, China)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_302_1539825877_7" onclick="$('#vhsjs_view_302_1539825877_7').hide();
                $('#vhsjs_hide_302_1539825877_7').show();
                $('#301_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_302_1539825877_7" onclick="$('#301_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_302_1539825877_7').hide();
                $('#vhsjs_view_302_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="302_1539825877_7" id="301_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>This paper reports our efforts on performing 50-m resolution earthquake simulation of the Wenchuan Earthquake (Ms 8.0, China) on Sunway TaihuLight. To accurately capture the surface topography, we adopt a curvilinear grid finite-difference method with a traction image free surface implementation and redesign the algorithm to reduce memory access costs for heterogeneous many-core architectures. We then derive a performance model of our algorithm to guide and drive the further optimization and tuning of various parameters using a genetic algorithm. A data layout transformation is also proposed to improve the direct memory access (DMA) efficiency further. Our efforts improve the simulation efficiency from 0.05% to 7.6%, with a sustained performance of 9.07 Pflops using the entire machine of the Sunway TaihuLight (over 10 million cores), and a large-scale simulation of the Wenchuan earthquake with accurate surface topography and improved coda wave effects.</blockquote></div></div></div></div><a href="includes/files/pap502s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap506"></a><div class="slot-title">Accelerating Quantum Chemistry with Vectorized and Batched Integrals</div><div class="slot-authors">Hua Huang and Edmond Chow (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_304_1539825877_7" onclick="$('#vhsjs_view_304_1539825877_7').hide();
                $('#vhsjs_hide_304_1539825877_7').show();
                $('#303_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_304_1539825877_7" onclick="$('#303_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_304_1539825877_7').hide();
                $('#vhsjs_view_304_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="304_1539825877_7" id="303_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>This paper presents the first quantum chemistry calculations using a recently developed vectorized library for computing electron repulsion integrals. To lengthen the SIMD loop and thus improve SIMD utilization, the approach used in this paper is to batch together the computation of multiple integrals that have the same code path. The standard approach is to compute integrals one at a time, and thus a batching procedure had to be developed. This paper shows proof-of-concept and demonstrates the performance gains possible when the batched approach is used. Batching also enables certain optimizations when the integrals are used to compute the Fock matrix. We further describe several other optimizations that were needed to obtain up to a 270% speedup over the no batching version of the code, making a compelling case for adopting the presented techniques in quantum chemistry software.</blockquote></div></div></div></div><a href="includes/files/pap506s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap133"></a><div class="slot-title">High-Performance Dense Tucker Decomposition on GPU Clusters</div><div class="slot-authors">Jee Choi (IBM), Xing Liu (Intel Corporation), and Venkatesan Chakaravarthy (IBM)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_306_1539825877_7" onclick="$('#vhsjs_view_306_1539825877_7').hide();
                $('#vhsjs_hide_306_1539825877_7').show();
                $('#305_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_306_1539825877_7" onclick="$('#305_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_306_1539825877_7').hide();
                $('#vhsjs_view_306_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="306_1539825877_7" id="305_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>The Tucker decomposition method is one of the most popular algorithms for analyzing and compressing data with multi-way relationship. Its execution time is typically dominated by dense matrix multiplication, which makes it well-suited for GPU acceleration. State-of-the-art distributed dense Tucker implementations for CPU clusters adopt multi-dimensional partitioning that optimizes for storage and communication. This, however, leads to smaller matrix dimensions that result in under-utilizing the GPU. <br><br>In this paper, we present our optimized implementation and performance analysis of dense Tucker decomposition on a multi-GPU cluster. We propose three optimizations: a new partitioning strategy that improves GPU performance, a new tensor matricization layout that halves the number of communication/matricization steps, and a variation of the randomized SVD algorithm to overcome the eigenvalue bottleneck that arises from the high speedups gained from GPU acceleration.  Our GPU implementation employing all three optimizations achieves up to 11.8x speedup on 64 nodes over state-of-the-art TuckerMPI.</blockquote></div></div></div></div><a href="includes/files/pap133s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Computational Physics, Scientific Computing, Tech Program Reg Pass</span><br /><div class="session-title">Astrophysics Applications</div><div class="slot-entry"><a name="pap239"></a><div class="slot-title">Phase Asynchronous AMR Execution for Productive and Performant Astrophysical Flows</div><div class="slot-authors">Muhammad Nufail Farooqi (Koc University); Tan Nguyen, Weiqun Zhang, Ann S. Almgren, and John Shalf (Lawrence Berkeley National Laboratory); and Didem Unat (Koc University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_308_1539825877_7" onclick="$('#vhsjs_view_308_1539825877_7').hide();
                $('#vhsjs_hide_308_1539825877_7').show();
                $('#307_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_308_1539825877_7" onclick="$('#307_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_308_1539825877_7').hide();
                $('#vhsjs_view_308_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="308_1539825877_7" id="307_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>Adaptive Mesh Refinement (AMR) is an approach to solving PDEs that reduces the computational and memory requirements at the expense of increased communication. Although adopting asynchronous execution can overcome communication issues, manually restructuring an AMR application to realize asynchrony is extremely complicated and hinders readability and long-term maintainability. To balance performance against productivity, we design a user-friendly API and adopt phase asynchronous execution model where all subgrids at an AMR level can be computed asynchronously. <br><br>We apply the phase asynchrony to transform a real-world AMR application, CASTRO, which solves multicomponent compressible hydrodynamic equations for astrophysical flows. We evaluate the performance and programming effort required to use our carefully designed API and execution model for transitioning large legacy codes from synchronous to asynchronous execution up to 278,528 Intel-KNL cores. CASTRO is about 100K lines of code but less than 0.2% code changes are required to achieve significant performance improvement.</blockquote></div></div></div></div><a href="includes/files/pap239s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap294"></a><div class="slot-title">Computing Planetary Interior Normal Modes with a Highly Parallel Polynomial Filtering Eigensolver</div><div class="slot-authors">Jia Shi (Rice University), Ruipeng Li (Lawrence Livermore National Laboratory), Yuanzhe Xi and Yousef Saad (University of Minnesota), and Maarten V. de Hoop (Rice University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_310_1539825877_7" onclick="$('#vhsjs_view_310_1539825877_7').hide();
                $('#vhsjs_hide_310_1539825877_7').show();
                $('#309_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_310_1539825877_7" onclick="$('#309_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_310_1539825877_7').hide();
                $('#vhsjs_view_310_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="310_1539825877_7" id="309_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>A highly parallel algorithm has been developed and exploited to compute the planetary normal modes of the elastic-gravitational system, which is approximated via the mixed finite element method on unstructured tetrahedral meshes. The eigenmodes of the relevant generalized eigenvalue problem were extracted by a Lanczos approach combined with polynomial filtering. In contrast with the standard shift-and-invert and the full-mode coupling algorithms, the polynomial filtering technique is ideally suited for solving large-scale 3-D interior eigenvalue problems since it significantly enhances the memory and computational efficiency without loss of accuracy.  The parallel efficiency and scalability of this approach are demonstrated on Stampede2 at the Texas Advanced Computing Center. To our knowledge, this is the first time that the direct calculation of the normal modes of 3-D strongly heterogeneous planets, in particular, Earth and Mars, is made feasible via a combination of multiple matrix-free methods and a separation of the essential spectra.</blockquote></div></div></div></div><a href="includes/files/pap294s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack191"></a><div class="section-title">Cosmology</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Applications, Cosmology, Data Analytics, Deep Learning, Machine Learning, Programming Systems, Storage, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Deep Learning</div><div class="slot-entry"><a name="pap425"></a><div class="slot-title">Exploring Flexible Communications for Streamlining DNN Ensemble Training Pipelines</div><div class="slot-authors">Randall Pittman, Hui Guan, and Xipeng Shen (North Carolina State University) and Seung-Hwan Lim and Robert M. Patton (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_312_1539825877_7" onclick="$('#vhsjs_view_312_1539825877_7').hide();
                $('#vhsjs_hide_312_1539825877_7').show();
                $('#311_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_312_1539825877_7" onclick="$('#311_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_312_1539825877_7').hide();
                $('#vhsjs_view_312_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="312_1539825877_7" id="311_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>Parallel training of a Deep Neural Network (DNN) ensemble on a cluster of nodes is a common practice to train multiple models in order to construct a model with a higher prediction accuracy. Existing ensemble training pipelines can perform a great deal of redundant operations, resulting in unnecessary CPU usage, or even poor pipeline performance.  In order to remove these redundancies, we need pipelines with more communication flexibility than existing DNN frameworks provide.<br><br>This project investigates a series of designs to improve pipeline flexibility and adaptivity, while also increasing performance. We implement our designs using Tensorflow with Horovod, and test it using several large DNNs. Our results show that the CPU time spent during training is reduced by 2-11X. Furthermore, our implementation can achieve up to 10X speedups when CPU core limits are imposed. Our best pipeline also reduces the average power draw of the ensemble training process by 5-16%.</blockquote></div></div></div></div><a href="includes/files/pap425s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap429"></a><div class="slot-title">CosmoFlow: Using Deep Learning to Learn the Universe at Scale</div><div class="slot-authors">Amrita Mathuriya (Intel Corporation); Deborah Bard (National Energy Research Scientific Computing Center (NERSC), Lawrence Berkeley National Laboratory); Pete Mendygral (Cray Inc); Lawrence Meadows (Intel Corporation); James Arnemann (University of California, Berkeley); Lei Shao (Intel Corporation); Siyu He (Carnegie Mellon University); Tuomas Karna (Intel Corporation); Diana Moise (Cray Inc); Simon J. Pennycook (Intel Corporation); Kristyn Maschhoff (Cray Inc); Jason Sewall and Nalini Kumar (Intel Corporation); Shirley Ho (Lawrence Berkeley National Laboratory, Carnegie Mellon University); Michael F. Ringenburg (Cray Inc); Mr Prabhat (Lawrence Berkeley National Laboratory, National Energy Research Scientific Computing Center (NERSC)); and Victor Lee (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_314_1539825877_7" onclick="$('#vhsjs_view_314_1539825877_7').hide();
                $('#vhsjs_hide_314_1539825877_7').show();
                $('#313_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_314_1539825877_7" onclick="$('#313_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_314_1539825877_7').hide();
                $('#vhsjs_view_314_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="314_1539825877_7" id="313_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>Deep learning is a promising tool to determine the physical model that describes our universe.   To handle the considerable computational cost of this problem, we present CosmoFlow: a highly scalable deep learning application built on top of the TensorFlow framework.<br><br>CosmoFlow uses efficient implementations of 3D convolution and pooling primitives, together with improvements in threading for many element-wise operations, to improve training performance on Intel Xeon Phi processors.  We also utilize the Cray PE Machine Learning Plugin for efficient scaling to multiple nodes. We demonstrate fully synchronous data-parallel training on 8192 nodes of Cori with 77% parallel efficiency, achieving 3.5 Pflop/s sustained performance. <br><br>To our knowledge, this is the first large-scale science application of the TensorFlow framework at supercomputer scale with fully-synchronous training. These enhancements enable us to process large 3D dark matter distribution and predict the cosmological parameters Omega_M, sigma_8 and N_s with unprecedented accuracy.</blockquote></div></div></div></div><a href="includes/files/pap429s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap322"></a><div class="slot-title">Anatomy of High-Performance Deep Learning Convolutions on SIMD Architectures</div><div class="slot-authors">Evangelos Georganas, Sasikanth Avancha, Kunal Banerjee, Dhiraj Kalamkar, Greg Henry, Hans Pabst, and Alexander Heinecke (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_316_1539825877_7" onclick="$('#vhsjs_view_316_1539825877_7').hide();
                $('#vhsjs_hide_316_1539825877_7').show();
                $('#315_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_316_1539825877_7" onclick="$('#315_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_316_1539825877_7').hide();
                $('#vhsjs_view_316_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="316_1539825877_7" id="315_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>Convolution layers are prevalent in many classes of deep neural networks, including Convolutional Neural Networks (CNNs) which provide state-of-the-art results for tasks like image recognition, neural machine translation, and speech recognition. The computationally expensive nature of a convolution operation has led to the proliferation of implementations including matrix-matrix multiplication formulation, and direct convolution primarily targeting GPUs. In this paper, we introduce direct convolution kernels for x86 architectures, in particular for Xeon and Xeon Phi systems, which are implemented via a dynamic compilation approach. Our JIT-based implementation shows close to theoretical peak performance, depending on the setting and the CPU architecture at hand. We additionally demonstrate how these JIT-optimized kernels can be integrated into a light-weight multi-node graph execution model. This illustrates that single- and multi-node runs yield high efficiencies and high image-throughputs  when executing state of the art image recognition tasks on CPUs.</blockquote></div></div></div></div><a href="includes/files/pap322s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack115"></a><div class="section-title">Data Analytics</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Data Analytics, Networks, Tech Program Reg Pass</span><br /><div class="session-title">Next-Generation Networking</div><div class="slot-entry"><a name="pap147"></a><div class="slot-title">Exploiting Idle Resources in a High-Radix Switch for Supplemental Storage</div><div><span class="BP award">Best Paper Finalists</span></div><div class="slot-authors">Matthias A. Blumrich, Nan Jiang, and Larry R. Dennison (Nvidia Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_318_1539825877_7" onclick="$('#vhsjs_view_318_1539825877_7').hide();
                $('#vhsjs_hide_318_1539825877_7').show();
                $('#317_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_318_1539825877_7" onclick="$('#317_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_318_1539825877_7').hide();
                $('#vhsjs_view_318_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="318_1539825877_7" id="317_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>A general-purpose switch for a high-performance network is usually designed with symmetric ports providing credit-based flow control and error recovery via link-level retransmission. Because port buffers must be sized for the longest links and modern asymmetric network topologies have a wide range of link lengths, we observe that there can be a significant amount of unused buffer memory, particularly in edge switches. We also observe that the tiled architecture used in many high-radix switches contains an abundance of internal bandwidth. We combine these observations to create a new switch architecture that allows ports to stash packets in unused buffers on other ports, accessible via excess internal bandwidth in the tiled switch. We explore this architecture through two use cases: end-to-end resilience and congestion mitigation. We find that stashing is highly effective and does not negatively impact network performance.</blockquote></div></div></div></div><a href="includes/files/pap147s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap163"></a><div class="slot-title">Fine-Grained, Multi-Domain Network Resource Abstraction as a Fundamental Primitive to Enable High-Performance, Collaborative Data Sciences</div><div class="slot-authors">Qiao Xiang (Yale University); J. Jensen Zhang, X. Tony Wang, and Y. Jace Liu (Tongji University); Chin Guok (Lawrence Berkeley National Laboratory); Franck Le (IBM); John MacAuley (Lawrence Berkeley National Laboratory); Harvey Newman (California Institute of Technology); and Y. Richard Yang (Yale University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_320_1539825877_7" onclick="$('#vhsjs_view_320_1539825877_7').hide();
                $('#vhsjs_hide_320_1539825877_7').show();
                $('#319_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_320_1539825877_7" onclick="$('#319_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_320_1539825877_7').hide();
                $('#vhsjs_view_320_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="320_1539825877_7" id="319_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>Multi-domain network resource reservation systems are being deployed, driven by the demand and substantial benefits of providing predictable network resources. However, a major lack of existing systems is their coarse granularity, due to the participating networks’ concern of revealing sensitive information, which can result in substantial inefficiencies. This paper presents Mercator, a novel multi-domain network resource discovery system to provide fine-grained, global network resource information, for collaborative sciences. The foundation of Mercator is a resource abstraction through algebraic-expression enumeration (i.e., linear inequalities/equations), as a compact representation of the available bandwidth in multi-domain networks. In addition, we develop an obfuscating protocol, to address the privacy concerns by ensuring that no participant can associate the algebraic expressions with the corresponding member networks. We also introduce a superset projection technique to increase Mercator’s scalability. Finally, we implement Mercator and demonstrate both its efficiency and efficacy through extensive experiments using real topologies and traces.</blockquote></div></div></div></div><a href="includes/files/pap163s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap157"></a><div class="slot-title">Light-Weight Protocols for Wire-Speed Ordering</div><div class="slot-authors">Hans Eberle and Larry Dennison (Nvidia Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_322_1539825877_7" onclick="$('#vhsjs_view_322_1539825877_7').hide();
                $('#vhsjs_hide_322_1539825877_7').show();
                $('#321_1539825877_7').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_322_1539825877_7" onclick="$('#321_1539825877_7').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_322_1539825877_7').hide();
                $('#vhsjs_view_322_1539825877_7').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="322_1539825877_7" id="321_1539825877_7" style="display: none"><div class="arrow-slidedown"><blockquote>We describe light-weight protocols for selective packet ordering in out-of-order networks that carry memory traffic. The protocols are designed for heterogeneous high-performance systems, in particular, accelerated systems with endpoints that have few resources available for interfacing the network.<br><br>The protocols preserve the semantics of a relaxed memory ordering model as adopted by highly-threaded many-core processors and accelerators.<br><br>The protocols achieve link-rate performance through the following techniques: (1) speculative connection setup avoids round-trip delays found in protocols with little knowledge about endpoint resources, (2) target-side ordering avoids round-trip delays found in source-side ordering mechanisms, (3) fine-grained ordering removes dependencies unwarranted by program code avoiding cumulative ordering dependencies caused by coarse-grained ordering, (4) ordering relaxations and optimizations for producer/consumer communication patterns.<br><br>We describe two ordering protocols that provide (1) strict sequential ordering and (2) relaxed ordering for multi-packet transfers. The protocols impose no restrictions on routing, including multipath routing.</blockquote></div></div></div></div><a href="includes/files/pap157s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, Data Analytics, Deep Learning, Networks, Scientific Computing, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Large-Scale Algorithms</div><div class="slot-entry"><a name="pap171"></a><div class="slot-title">Large-Scale Hierarchical K-Means for Heterogeneous Many-Core Supercomputers</div><div class="slot-authors">Liandeng Li (Tsinghua University; National Supercomputing Center, Wuxi); Teng Yu (University of St Andrews); Wenlai Zhao and Haohuan Fu (Tsinghua University; National Supercomputing Center, Wuxi); Chenyu Wang (University of St Andrews; National Supercomputing Center, Wuxi); Li Tan (Beijing Technology and Business University); Guangwen Yang (Tsinghua University; National Supercomputing Center, Wuxi); and John Thomson (University of St Andrews)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_324_1539825877_71" onclick="$('#vhsjs_view_324_1539825877_71').hide();
                $('#vhsjs_hide_324_1539825877_71').show();
                $('#323_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_324_1539825877_71" onclick="$('#323_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_324_1539825877_71').hide();
                $('#vhsjs_view_324_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="324_1539825877_71" id="323_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>This paper presents a novel design and implementation of k-means clustering algorithm targeting the Sunway TaihuLight supercomputer. We introduce a multi-level parallel partition approach that not only partitions by dataflow and centroid, but also by dimension. Our multi-level (nkd) approach unlocks the potential of the hierarchical parallelism in the SW26010 heterogeneous many-core processor and the system architecture of the supercomputer. <br><br>Our design is able to process large-scale clustering problems with up to 196,608 dimensions and over 160,000 targeting centroids, while maintaining high performance and high scalability, significantly improving the capability of k-means over previous approaches. The evaluation shows our implementation achieves performance of less than 18 seconds per iteration for a large-scale clustering case with 196,608 data dimensions and 2,000 centroids by applying 4,096 nodes (1,064,496 cores) in parallel, making k-means a more feasible solution for complex scenarios.</blockquote></div></div></div></div><a href="includes/files/pap171s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap140"></a><div class="slot-title">TriCore: Parallel Triangle Counting on GPUs</div><div class="slot-authors">Yang Hu (George Washington University); Hang Liu (University of Massachusetts, Lowell); and H. Howie Huang (George Washington University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_326_1539825877_71" onclick="$('#vhsjs_view_326_1539825877_71').hide();
                $('#vhsjs_hide_326_1539825877_71').show();
                $('#325_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_326_1539825877_71" onclick="$('#325_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_326_1539825877_71').hide();
                $('#vhsjs_view_326_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="326_1539825877_71" id="325_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>Triangle counting algorithm enumerates the triangles in a graph by identifying the common neighbors between two vertices of every edge. In this work, we present TriCore, a new GPU-based high-performance and scalable triangle counting system that consists of three main techniques. First, we design a binary search based counting algorithm that tremendously increases both thread parallelism and memory performance. Second, TriCore exploits a 2-D partition method to distribute the CSR representation across multiple GPUs, combined with a new streaming buffer to load the edge list from outside of GPUs. Third, we develop a dynamic workload management technique to balance the workload across multiple GPUs. Our evaluation demonstrates TriCore is 22× faster than the state-of-the-art parallel triangle counting projects. In addition, TriCore can not only process big graphs that are significant larger than the memory size of one GPU but also achieve 24× speedup when scaling to 32 GPUs.</blockquote></div></div></div></div><a href="includes/files/pap140s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap141"></a><div class="slot-title">Distributed-Memory Hierarchical Compression of Dense SPD Matrices</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Chenhan D. Yu (University of Texas), Severin Reiz (Technical University Munich), and George Biros (University of Texas)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_328_1539825877_71" onclick="$('#vhsjs_view_328_1539825877_71').hide();
                $('#vhsjs_hide_328_1539825877_71').show();
                $('#327_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_328_1539825877_71" onclick="$('#327_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_328_1539825877_71').hide();
                $('#vhsjs_view_328_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="328_1539825877_71" id="327_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>We present a distributed-memory algorithm for the hierarchical compression of SPD matrices. Our method is based on GOFMM, an algorithm that appeared in doi:10.1145/3126908.3126921.<br><br>For many SPD matrices, GOFMM enables compression and approximate matrix-vector multiplication in NlogN time---as opposed to quadratic work required for a dense matrix. But GOFMM supports only shared memory parallelism. In this paper, we use the message passing interface, extending the ideas of GOFMM to the distributed memory setting. We also introduce an asynchronous algorithm for faster multiplication. We present different usage scenarios of SPD matrices that are related to graphs, neural-networks, and covariance operators. We also compare with STRUMPACK, which, to our knowledge, is the only other parallel software that can compress arbitrary SPD matrices. In our largest run, we were able to compress a 67M-by-67M matrix within three minutes and perform a multiplication with 512 vectors within 5 seconds on 6,144 Intel Skylake cores.</blockquote></div></div></div></div><a href="includes/files/pap141s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Data Analytics, Performance, Programming Systems, Storage, Tools, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Performance Optimization Studies</div><div class="slot-entry"><a name="pap335"></a><div class="slot-title">Many-Core Graph Workload Analysis</div><div class="slot-authors">Stijn Eyerman, Wim Heirman, Kristof Du Bois, Joshua B. Fryman, and Ibrahim Hur (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_330_1539825877_71" onclick="$('#vhsjs_view_330_1539825877_71').hide();
                $('#vhsjs_hide_330_1539825877_71').show();
                $('#329_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_330_1539825877_71" onclick="$('#329_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_330_1539825877_71').hide();
                $('#vhsjs_view_330_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="330_1539825877_71" id="329_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>Graph applications have specific characteristics that are not common in other application domains. In this paper, we analyze multiple graph applications on current multi- and many-core processors and provide conclusions and recommendations for future designs. We provide new insights on executing graph applications on many-core processors.<br><br>Our main novel observations are (i) some memory streams do show locality, while others show no locality, (ii) thread imbalance becomes a major problem with many threads, and (iii) many threads are required to saturate high-bandwidth memories. We recommend a selective memory access policy, where accesses with locality are cached and prefetched, while accesses without locality can remain uncached to save cache capacity. Additionally, more threads are needed, but they are not used efficiently due to thread imbalance. Our recommendation is to revise the graph analysis algorithms to provide more parallelism, and to provide a few high-performance cores that speedup sections with low parallelism.</blockquote></div></div></div></div><a href="includes/files/pap335s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap488"></a><div class="slot-title">Lessons Learned from Analyzing Dynamic Promotion for User-Level Threading</div><div class="slot-authors">Shintaro Iwasaki (University of Tokyo), Abdelhalim Amer (Argonne National Laboratory), Kenjiro Taura (University of Tokyo), and Pavan Balaji (Argonne National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_332_1539825877_71" onclick="$('#vhsjs_view_332_1539825877_71').hide();
                $('#vhsjs_hide_332_1539825877_71').show();
                $('#331_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_332_1539825877_71" onclick="$('#331_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_332_1539825877_71').hide();
                $('#vhsjs_view_332_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="332_1539825877_71" id="331_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>A performance vs. practicality trade-off exists between user-level threading techniques. The community has settled mostly on a black-and-white perspective; fully fledged threads assume that suspension is imminent and incur overheads when suspension does not take place, and run-to-completion threads are more lightweight but less practical since they cannot suspend. Gray areas exist, however, whereby threads can start with minimal capabilities and then can be dynamically promoted to acquire additional capabilities when needed. This paper investigates the full spectrum of threading techniques from a performance vs. practicality trade-off perspective on modern multicore and many-core systems. Our results indicate that achieving the best trade-off highly depends on the suspension likelihood; dynamic promotion is more appropriate when suspension is unlikely and represents a solid replacement for run to completion, thanks to its lower programming constraints, while fully fledged threads remain the technique of choice when suspension likelihood is high.</blockquote></div></div></div></div><a href="includes/files/pap488s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap484"></a><div class="slot-title">Topology-Aware Space-Shared Co-Analysis of Large-Scale Molecular Dynamics Simulations</div><div class="slot-authors">Preeti Malakar (Indian Institute of Technology Kanpur); Todd Munson, Christopher Knight, and Venkatram Vishwanath (Argonne National Laboratory); and Michael E. Papka (Argonne National Laboratory, Northern Illinois University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_334_1539825877_71" onclick="$('#vhsjs_view_334_1539825877_71').hide();
                $('#vhsjs_hide_334_1539825877_71').show();
                $('#333_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_334_1539825877_71" onclick="$('#333_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_334_1539825877_71').hide();
                $('#vhsjs_view_334_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="334_1539825877_71" id="333_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>Analysis of scientific simulation data can be concurrently executed with simulation either in time- or space-shared mode. This mitigates the I/O bottleneck.  However it results in either stalling the simulation for performing the analysis or transferring data for analysis. In this paper, we improve the throughput of space-shared in situ analysis of large-scale simulations by topology-aware mapping and optimal process decomposition. We propose node interconnect topology-aware process placement for simulation and analysis to reduce the data movement time. We also present an integer linear program for optimal 3D decompositions of simulation and analysis processes. We demonstrate our approach using molecular dynamics simulation on Mira, Cori and Theta supercomputers. Our mapping schemes, combined with optimal 3D process decomposition and code optimizations resulted in up to 30% lower execution times for space-shared in situ analysis than the default approach. Our mappings also reduce MPI collective I/O times by 10-40%.</blockquote></div></div></div></div><a href="includes/files/pap484s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Applications, Cosmology, Data Analytics, Deep Learning, Machine Learning, Programming Systems, Storage, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Deep Learning</div><div class="slot-entry"><a name="pap425"></a><div class="slot-title">Exploring Flexible Communications for Streamlining DNN Ensemble Training Pipelines</div><div class="slot-authors">Randall Pittman, Hui Guan, and Xipeng Shen (North Carolina State University) and Seung-Hwan Lim and Robert M. Patton (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_336_1539825877_71" onclick="$('#vhsjs_view_336_1539825877_71').hide();
                $('#vhsjs_hide_336_1539825877_71').show();
                $('#335_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_336_1539825877_71" onclick="$('#335_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_336_1539825877_71').hide();
                $('#vhsjs_view_336_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="336_1539825877_71" id="335_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>Parallel training of a Deep Neural Network (DNN) ensemble on a cluster of nodes is a common practice to train multiple models in order to construct a model with a higher prediction accuracy. Existing ensemble training pipelines can perform a great deal of redundant operations, resulting in unnecessary CPU usage, or even poor pipeline performance.  In order to remove these redundancies, we need pipelines with more communication flexibility than existing DNN frameworks provide.<br><br>This project investigates a series of designs to improve pipeline flexibility and adaptivity, while also increasing performance. We implement our designs using Tensorflow with Horovod, and test it using several large DNNs. Our results show that the CPU time spent during training is reduced by 2-11X. Furthermore, our implementation can achieve up to 10X speedups when CPU core limits are imposed. Our best pipeline also reduces the average power draw of the ensemble training process by 5-16%.</blockquote></div></div></div></div><a href="includes/files/pap425s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap429"></a><div class="slot-title">CosmoFlow: Using Deep Learning to Learn the Universe at Scale</div><div class="slot-authors">Amrita Mathuriya (Intel Corporation); Deborah Bard (National Energy Research Scientific Computing Center (NERSC), Lawrence Berkeley National Laboratory); Pete Mendygral (Cray Inc); Lawrence Meadows (Intel Corporation); James Arnemann (University of California, Berkeley); Lei Shao (Intel Corporation); Siyu He (Carnegie Mellon University); Tuomas Karna (Intel Corporation); Diana Moise (Cray Inc); Simon J. Pennycook (Intel Corporation); Kristyn Maschhoff (Cray Inc); Jason Sewall and Nalini Kumar (Intel Corporation); Shirley Ho (Lawrence Berkeley National Laboratory, Carnegie Mellon University); Michael F. Ringenburg (Cray Inc); Mr Prabhat (Lawrence Berkeley National Laboratory, National Energy Research Scientific Computing Center (NERSC)); and Victor Lee (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_338_1539825877_71" onclick="$('#vhsjs_view_338_1539825877_71').hide();
                $('#vhsjs_hide_338_1539825877_71').show();
                $('#337_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_338_1539825877_71" onclick="$('#337_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_338_1539825877_71').hide();
                $('#vhsjs_view_338_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="338_1539825877_71" id="337_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>Deep learning is a promising tool to determine the physical model that describes our universe.   To handle the considerable computational cost of this problem, we present CosmoFlow: a highly scalable deep learning application built on top of the TensorFlow framework.<br><br>CosmoFlow uses efficient implementations of 3D convolution and pooling primitives, together with improvements in threading for many element-wise operations, to improve training performance on Intel Xeon Phi processors.  We also utilize the Cray PE Machine Learning Plugin for efficient scaling to multiple nodes. We demonstrate fully synchronous data-parallel training on 8192 nodes of Cori with 77% parallel efficiency, achieving 3.5 Pflop/s sustained performance. <br><br>To our knowledge, this is the first large-scale science application of the TensorFlow framework at supercomputer scale with fully-synchronous training. These enhancements enable us to process large 3D dark matter distribution and predict the cosmological parameters Omega_M, sigma_8 and N_s with unprecedented accuracy.</blockquote></div></div></div></div><a href="includes/files/pap429s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap322"></a><div class="slot-title">Anatomy of High-Performance Deep Learning Convolutions on SIMD Architectures</div><div class="slot-authors">Evangelos Georganas, Sasikanth Avancha, Kunal Banerjee, Dhiraj Kalamkar, Greg Henry, Hans Pabst, and Alexander Heinecke (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_340_1539825877_71" onclick="$('#vhsjs_view_340_1539825877_71').hide();
                $('#vhsjs_hide_340_1539825877_71').show();
                $('#339_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_340_1539825877_71" onclick="$('#339_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_340_1539825877_71').hide();
                $('#vhsjs_view_340_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="340_1539825877_71" id="339_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>Convolution layers are prevalent in many classes of deep neural networks, including Convolutional Neural Networks (CNNs) which provide state-of-the-art results for tasks like image recognition, neural machine translation, and speech recognition. The computationally expensive nature of a convolution operation has led to the proliferation of implementations including matrix-matrix multiplication formulation, and direct convolution primarily targeting GPUs. In this paper, we introduce direct convolution kernels for x86 architectures, in particular for Xeon and Xeon Phi systems, which are implemented via a dynamic compilation approach. Our JIT-based implementation shows close to theoretical peak performance, depending on the setting and the CPU architecture at hand. We additionally demonstrate how these JIT-optimized kernels can be integrated into a light-weight multi-node graph execution model. This illustrates that single- and multi-node runs yield high efficiencies and high image-throughputs  when executing state of the art image recognition tasks on CPUs.</blockquote></div></div></div></div><a href="includes/files/pap322s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack116"></a><div class="section-title">Data Management</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Data Management, File Systems, Networks, State of the Practice, System Software, Workflows, Tech Program Reg Pass</span><br /><div class="session-title">File Systems: Data Movement and Provenance</div><div class="slot-entry"><a name="pap407"></a><div class="slot-title">Dac-Man: Data Change Management for Scientific Datasets on HPC Systems</div><div class="slot-authors">Devarshi Ghoshal, Lavanya Ramakrishnan, and Deborah Agarwal (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_342_1539825877_71" onclick="$('#vhsjs_view_342_1539825877_71').hide();
                $('#vhsjs_hide_342_1539825877_71').show();
                $('#341_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_342_1539825877_71" onclick="$('#341_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_342_1539825877_71').hide();
                $('#vhsjs_view_342_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="342_1539825877_71" id="341_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>Scientific data is growing rapidly and often changes due to instrument configurations, software updates, or quality assessments. These changes in datasets can result in significant waste of compute and storage resources on HPC systems as downstream pipelines are reprocessed. Data changes need to be detected, tracked, and analyzed for understanding the impact of data change, managing data provenance, and making efficient and effective decisions about reprocessing and use of HPC resources. Existing methods for identifying and capturing change are often manual, domain-specific, and error-prone and do not scale to large scientific datasets. In this paper, we describe the design and implementation of Dac-Man framework, which identifies, captures, and manages change in large scientific datasets, and enables plug-in of domain-specific change analysis with minimal user effort. Our evaluations show that it can retrieve file changes from directories containing millions of files and terabytes of data in less than a minute.</blockquote></div></div></div></div><a href="includes/files/pap407s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap521"></a><div class="slot-title">Stacker: An Autonomic Data Movement Engine for Extreme-Scale Data Staging-Based In Situ Workflows</div><div class="slot-authors">Pradeep Subedi, Philip Davis, and Shaohua Duan (Rutgers University); Scott Klasky (Oak Ridge National Laboratory); Hemanth Kolla (Sandia National Laboratories); and Manish Parashar (Rutgers University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_344_1539825877_71" onclick="$('#vhsjs_view_344_1539825877_71').hide();
                $('#vhsjs_hide_344_1539825877_71').show();
                $('#343_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_344_1539825877_71" onclick="$('#343_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_344_1539825877_71').hide();
                $('#vhsjs_view_344_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="344_1539825877_71" id="343_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>Data staging and in situ workflows are being explored extensively as an approach to address data-related costs at very large scales. However, the impact of emerging storage architectures (e.g., deep memory hierarchies and burst buffers) upon data staging solutions remains a challenge. In this paper, we investigate how burst buffers can be effectively used by data staging solutions, for example, as a persistence storage tier of the memory hierarchy. Furthermore, we use machine learning based prefetching techniques to move data between the storage levels in an autonomous manner. We also present Stacker, a prototype of the proposed solutions implemented within the Data\-Spaces data staging service, and experimentally evaluate its performance and scalability using the S3D combustion workflow on current leadership class platforms. Our experiments demonstrate that Stacker achieves low latency, high volume data-staging with low overhead as compared to in-memory staging services for production scientific workflows.</blockquote></div></div></div></div><a href="includes/files/pap521s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap206"></a><div class="slot-title">A Year in the Life of a Parallel File System</div><div class="slot-authors">Glenn K. Lockwood (Lawrence Berkeley National Laboratory), Shane Snyder (Argonne National Laboratory), Teng Wang and Suren Byna (Lawrence Berkeley National Laboratory), Philip Carns (Argonne National Laboratory), and Nicholas J. Wright (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_346_1539825877_71" onclick="$('#vhsjs_view_346_1539825877_71').hide();
                $('#vhsjs_hide_346_1539825877_71').show();
                $('#345_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_346_1539825877_71" onclick="$('#345_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_346_1539825877_71').hide();
                $('#vhsjs_view_346_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="346_1539825877_71" id="345_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>I/O performance is a critical aspect of data-intensive scientific computing.  We seek to advance the state of the practice in understanding and diagnosing I/O performance issues through investigation of a comprehensive I/O performance data set that captures a full year of production storage activity at two leadership-scale computing facilities.  We demonstrate techniques to identify regions of interest, perform focused investigations of both long-term trends and transient anomalies, and uncover the contributing factors that lead to performance fluctuation.<br><br>We find that a year in the life of a parallel file system is comprised of distinct regions of long-term performance variation in addition to short-term performance transients.  We demonstrate how systematic identification of these performance regions, combined with comprehensive analysis, allows us to isolate the factors contributing to different performance maladies at different time scales.  From this, we present specific lessons learned and important considerations for HPC storage practitioners.</blockquote></div></div></div></div><a href="includes/files/pap206s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack118"></a><div class="section-title">Deep Learning</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, Data Analytics, Deep Learning, Networks, Scientific Computing, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Large-Scale Algorithms</div><div class="slot-entry"><a name="pap171"></a><div class="slot-title">Large-Scale Hierarchical K-Means for Heterogeneous Many-Core Supercomputers</div><div class="slot-authors">Liandeng Li (Tsinghua University; National Supercomputing Center, Wuxi); Teng Yu (University of St Andrews); Wenlai Zhao and Haohuan Fu (Tsinghua University; National Supercomputing Center, Wuxi); Chenyu Wang (University of St Andrews; National Supercomputing Center, Wuxi); Li Tan (Beijing Technology and Business University); Guangwen Yang (Tsinghua University; National Supercomputing Center, Wuxi); and John Thomson (University of St Andrews)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_348_1539825877_71" onclick="$('#vhsjs_view_348_1539825877_71').hide();
                $('#vhsjs_hide_348_1539825877_71').show();
                $('#347_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_348_1539825877_71" onclick="$('#347_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_348_1539825877_71').hide();
                $('#vhsjs_view_348_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="348_1539825877_71" id="347_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>This paper presents a novel design and implementation of k-means clustering algorithm targeting the Sunway TaihuLight supercomputer. We introduce a multi-level parallel partition approach that not only partitions by dataflow and centroid, but also by dimension. Our multi-level (nkd) approach unlocks the potential of the hierarchical parallelism in the SW26010 heterogeneous many-core processor and the system architecture of the supercomputer. <br><br>Our design is able to process large-scale clustering problems with up to 196,608 dimensions and over 160,000 targeting centroids, while maintaining high performance and high scalability, significantly improving the capability of k-means over previous approaches. The evaluation shows our implementation achieves performance of less than 18 seconds per iteration for a large-scale clustering case with 196,608 data dimensions and 2,000 centroids by applying 4,096 nodes (1,064,496 cores) in parallel, making k-means a more feasible solution for complex scenarios.</blockquote></div></div></div></div><a href="includes/files/pap171s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap140"></a><div class="slot-title">TriCore: Parallel Triangle Counting on GPUs</div><div class="slot-authors">Yang Hu (George Washington University); Hang Liu (University of Massachusetts, Lowell); and H. Howie Huang (George Washington University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_350_1539825877_71" onclick="$('#vhsjs_view_350_1539825877_71').hide();
                $('#vhsjs_hide_350_1539825877_71').show();
                $('#349_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_350_1539825877_71" onclick="$('#349_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_350_1539825877_71').hide();
                $('#vhsjs_view_350_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="350_1539825877_71" id="349_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>Triangle counting algorithm enumerates the triangles in a graph by identifying the common neighbors between two vertices of every edge. In this work, we present TriCore, a new GPU-based high-performance and scalable triangle counting system that consists of three main techniques. First, we design a binary search based counting algorithm that tremendously increases both thread parallelism and memory performance. Second, TriCore exploits a 2-D partition method to distribute the CSR representation across multiple GPUs, combined with a new streaming buffer to load the edge list from outside of GPUs. Third, we develop a dynamic workload management technique to balance the workload across multiple GPUs. Our evaluation demonstrates TriCore is 22× faster than the state-of-the-art parallel triangle counting projects. In addition, TriCore can not only process big graphs that are significant larger than the memory size of one GPU but also achieve 24× speedup when scaling to 32 GPUs.</blockquote></div></div></div></div><a href="includes/files/pap140s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap141"></a><div class="slot-title">Distributed-Memory Hierarchical Compression of Dense SPD Matrices</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Chenhan D. Yu (University of Texas), Severin Reiz (Technical University Munich), and George Biros (University of Texas)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_352_1539825877_71" onclick="$('#vhsjs_view_352_1539825877_71').hide();
                $('#vhsjs_hide_352_1539825877_71').show();
                $('#351_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_352_1539825877_71" onclick="$('#351_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_352_1539825877_71').hide();
                $('#vhsjs_view_352_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="352_1539825877_71" id="351_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>We present a distributed-memory algorithm for the hierarchical compression of SPD matrices. Our method is based on GOFMM, an algorithm that appeared in doi:10.1145/3126908.3126921.<br><br>For many SPD matrices, GOFMM enables compression and approximate matrix-vector multiplication in NlogN time---as opposed to quadratic work required for a dense matrix. But GOFMM supports only shared memory parallelism. In this paper, we use the message passing interface, extending the ideas of GOFMM to the distributed memory setting. We also introduce an asynchronous algorithm for faster multiplication. We present different usage scenarios of SPD matrices that are related to graphs, neural-networks, and covariance operators. We also compare with STRUMPACK, which, to our knowledge, is the only other parallel software that can compress arbitrary SPD matrices. In our largest run, we were able to compress a 67M-by-67M matrix within three minutes and perform a multiplication with 512 vectors within 5 seconds on 6,144 Intel Skylake cores.</blockquote></div></div></div></div><a href="includes/files/pap141s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Applications, Cosmology, Data Analytics, Deep Learning, Machine Learning, Programming Systems, Storage, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Deep Learning</div><div class="slot-entry"><a name="pap425"></a><div class="slot-title">Exploring Flexible Communications for Streamlining DNN Ensemble Training Pipelines</div><div class="slot-authors">Randall Pittman, Hui Guan, and Xipeng Shen (North Carolina State University) and Seung-Hwan Lim and Robert M. Patton (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_354_1539825877_71" onclick="$('#vhsjs_view_354_1539825877_71').hide();
                $('#vhsjs_hide_354_1539825877_71').show();
                $('#353_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_354_1539825877_71" onclick="$('#353_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_354_1539825877_71').hide();
                $('#vhsjs_view_354_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="354_1539825877_71" id="353_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>Parallel training of a Deep Neural Network (DNN) ensemble on a cluster of nodes is a common practice to train multiple models in order to construct a model with a higher prediction accuracy. Existing ensemble training pipelines can perform a great deal of redundant operations, resulting in unnecessary CPU usage, or even poor pipeline performance.  In order to remove these redundancies, we need pipelines with more communication flexibility than existing DNN frameworks provide.<br><br>This project investigates a series of designs to improve pipeline flexibility and adaptivity, while also increasing performance. We implement our designs using Tensorflow with Horovod, and test it using several large DNNs. Our results show that the CPU time spent during training is reduced by 2-11X. Furthermore, our implementation can achieve up to 10X speedups when CPU core limits are imposed. Our best pipeline also reduces the average power draw of the ensemble training process by 5-16%.</blockquote></div></div></div></div><a href="includes/files/pap425s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap429"></a><div class="slot-title">CosmoFlow: Using Deep Learning to Learn the Universe at Scale</div><div class="slot-authors">Amrita Mathuriya (Intel Corporation); Deborah Bard (National Energy Research Scientific Computing Center (NERSC), Lawrence Berkeley National Laboratory); Pete Mendygral (Cray Inc); Lawrence Meadows (Intel Corporation); James Arnemann (University of California, Berkeley); Lei Shao (Intel Corporation); Siyu He (Carnegie Mellon University); Tuomas Karna (Intel Corporation); Diana Moise (Cray Inc); Simon J. Pennycook (Intel Corporation); Kristyn Maschhoff (Cray Inc); Jason Sewall and Nalini Kumar (Intel Corporation); Shirley Ho (Lawrence Berkeley National Laboratory, Carnegie Mellon University); Michael F. Ringenburg (Cray Inc); Mr Prabhat (Lawrence Berkeley National Laboratory, National Energy Research Scientific Computing Center (NERSC)); and Victor Lee (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_356_1539825877_71" onclick="$('#vhsjs_view_356_1539825877_71').hide();
                $('#vhsjs_hide_356_1539825877_71').show();
                $('#355_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_356_1539825877_71" onclick="$('#355_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_356_1539825877_71').hide();
                $('#vhsjs_view_356_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="356_1539825877_71" id="355_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>Deep learning is a promising tool to determine the physical model that describes our universe.   To handle the considerable computational cost of this problem, we present CosmoFlow: a highly scalable deep learning application built on top of the TensorFlow framework.<br><br>CosmoFlow uses efficient implementations of 3D convolution and pooling primitives, together with improvements in threading for many element-wise operations, to improve training performance on Intel Xeon Phi processors.  We also utilize the Cray PE Machine Learning Plugin for efficient scaling to multiple nodes. We demonstrate fully synchronous data-parallel training on 8192 nodes of Cori with 77% parallel efficiency, achieving 3.5 Pflop/s sustained performance. <br><br>To our knowledge, this is the first large-scale science application of the TensorFlow framework at supercomputer scale with fully-synchronous training. These enhancements enable us to process large 3D dark matter distribution and predict the cosmological parameters Omega_M, sigma_8 and N_s with unprecedented accuracy.</blockquote></div></div></div></div><a href="includes/files/pap429s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap322"></a><div class="slot-title">Anatomy of High-Performance Deep Learning Convolutions on SIMD Architectures</div><div class="slot-authors">Evangelos Georganas, Sasikanth Avancha, Kunal Banerjee, Dhiraj Kalamkar, Greg Henry, Hans Pabst, and Alexander Heinecke (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_358_1539825877_71" onclick="$('#vhsjs_view_358_1539825877_71').hide();
                $('#vhsjs_hide_358_1539825877_71').show();
                $('#357_1539825877_71').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_358_1539825877_71" onclick="$('#357_1539825877_71').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_358_1539825877_71').hide();
                $('#vhsjs_view_358_1539825877_71').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="358_1539825877_71" id="357_1539825877_71" style="display: none"><div class="arrow-slidedown"><blockquote>Convolution layers are prevalent in many classes of deep neural networks, including Convolutional Neural Networks (CNNs) which provide state-of-the-art results for tasks like image recognition, neural machine translation, and speech recognition. The computationally expensive nature of a convolution operation has led to the proliferation of implementations including matrix-matrix multiplication formulation, and direct convolution primarily targeting GPUs. In this paper, we introduce direct convolution kernels for x86 architectures, in particular for Xeon and Xeon Phi systems, which are implemented via a dynamic compilation approach. Our JIT-based implementation shows close to theoretical peak performance, depending on the setting and the CPU architecture at hand. We additionally demonstrate how these JIT-optimized kernels can be integrated into a light-weight multi-node graph execution model. This illustrates that single- and multi-node runs yield high efficiencies and high image-throughputs  when executing state of the art image recognition tasks on CPUs.</blockquote></div></div></div></div><a href="includes/files/pap322s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack123"></a><div class="section-title">File Systems</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Clouds and Distributed Computing, File Systems, I/O, Storage, Tech Program Reg Pass</span><br /><div class="session-title">Data and Storage</div><div class="slot-entry"><a name="pap165"></a><div class="slot-title">SP-Cache: Load-Balanced, Redundancy-Free Cluster Caching with Selective Partition</div><div class="slot-authors">Yinghao Yu, Renfei Huang, Wei Wang, Jun Zhang, and Khaled Ben Letaief (Hong Kong University of Science and Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_360_1539825877_72" onclick="$('#vhsjs_view_360_1539825877_72').hide();
                $('#vhsjs_hide_360_1539825877_72').show();
                $('#359_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_360_1539825877_72" onclick="$('#359_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_360_1539825877_72').hide();
                $('#vhsjs_view_360_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="360_1539825877_72" id="359_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>Data-intensive clusters increasingly employ in-memory solutions to improve I/O performance. However, the routinely observed file popularity skew and load imbalance create hotspots, which significantly degrades the benefits of in-memory solutions. Common approaches to tame load imbalance include copying multiple replicas of hot files and creating parity chunks using storage codes. Yet, these techniques either suffer from high memory redundancy or incur non-trivial encoding/decoding overhead. In this paper, we propose a different approach to achieve load balancing without memory redundancy or encoding/decoding overhead. Our solution, termed SP-Cache, selectively partitions files based on their popularity and evenly caches those partitions across the cluster. We develop an efficient algorithm to determine the optimal number of partitions for hot files—too few partitions are incapable of mitigating hotspots, while too many are susceptible to stragglers. EC2 deployment and trace-driven simulations show that, compared with existing solutions, SP-Cache reduces the read latencies by up to 40%.</blockquote></div></div></div></div><a href="includes/files/pap165s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap585"></a><div class="slot-title">BESPOKV: Application Tailored Scale-Out Key-Value Stores</div><div class="slot-authors">Ali Anwar (IBM), Yue Cheng (George Mason University), Hai Huang (IBM), Jingoo Han (Virginia Tech), Hyogi Sim (Oak Ridge National Laboratory), Dongyoon Lee (Virginia Tech), Fred Douglis (Perspecta Labs), and Ali R. Butt (Virginia Tech)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_362_1539825877_72" onclick="$('#vhsjs_view_362_1539825877_72').hide();
                $('#vhsjs_hide_362_1539825877_72').show();
                $('#361_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_362_1539825877_72" onclick="$('#361_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_362_1539825877_72').hide();
                $('#vhsjs_view_362_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="362_1539825877_72" id="361_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>Enterprise KV stores are not well suited for HPC applications, and entail customization and cumbersome end-to-end KV design to extract the HPC application needs. In this paper we present BESPOKV, an adaptive, extensible, and scale-out KV store framework. BESPOKV decouples the KV store design into the control plane for distributed management and the data plane for local data store. BESPOKV takes as input a single-server KV store, called a datalet, and transparently enables a scalable and fault-tolerant distributed KV store service. The resulting distributed stores are also adaptive to consistency or topology requirement changes and can be easily extended for new types of services. Experiments show that BESPOKV-enabled distributed KV stores scale horizontally to a large number of nodes, and performs comparably and sometimes better than the state-of-the-art systems.</blockquote></div></div></div></div><a href="includes/files/pap585s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap450"></a><div class="slot-title">Scaling Embedded In Situ Indexing with DeltaFS</div><div class="slot-authors">Qing Zheng, Charles D. Cranor, Danhao Guo, Gregory R. Ganger, George Amvrosiadis, and Garth A. Gibson (Carnegie Mellon University) and Bradley W. Settlemyer, Gary Grider, and Fan Guo (Los Alamos National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_364_1539825877_72" onclick="$('#vhsjs_view_364_1539825877_72').hide();
                $('#vhsjs_hide_364_1539825877_72').show();
                $('#363_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_364_1539825877_72" onclick="$('#363_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_364_1539825877_72').hide();
                $('#vhsjs_view_364_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="364_1539825877_72" id="363_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>Analysis of large-scale simulation output is a core element of scientific inquiry, but analysis queries may experience significant I/O overhead when the data is not structured for efficient retrieval. While in-situ processing allows for improved time-to-insight for many applications, scaling in-situ frameworks to hundreds of thousands of cores can be difficult in practice. The DeltaFS in-situ indexing is a new approach for in-situ processing of massive amounts of data to achieve efficient point and small-range queries. This paper describes the challenges and lessons learned when scaling this in-situ processing function to hundreds of thousands of cores. We propose techniques for scalable all-to-all communication that is memory and bandwidth efficient, concurrent indexing, and specialized LSM-Tree formats. Combining these techniques allows DeltaFS to control the cost of in-situ processing while maintaining 3 orders of magnitude query speedup when scaling alongside the popular VPIC particle-in-cell code to 131,072 cores.</blockquote></div></div></div></div><a href="includes/files/pap450s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Data Management, File Systems, Networks, State of the Practice, System Software, Workflows, Tech Program Reg Pass</span><br /><div class="session-title">File Systems: Data Movement and Provenance</div><div class="slot-entry"><a name="pap407"></a><div class="slot-title">Dac-Man: Data Change Management for Scientific Datasets on HPC Systems</div><div class="slot-authors">Devarshi Ghoshal, Lavanya Ramakrishnan, and Deborah Agarwal (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_366_1539825877_72" onclick="$('#vhsjs_view_366_1539825877_72').hide();
                $('#vhsjs_hide_366_1539825877_72').show();
                $('#365_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_366_1539825877_72" onclick="$('#365_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_366_1539825877_72').hide();
                $('#vhsjs_view_366_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="366_1539825877_72" id="365_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>Scientific data is growing rapidly and often changes due to instrument configurations, software updates, or quality assessments. These changes in datasets can result in significant waste of compute and storage resources on HPC systems as downstream pipelines are reprocessed. Data changes need to be detected, tracked, and analyzed for understanding the impact of data change, managing data provenance, and making efficient and effective decisions about reprocessing and use of HPC resources. Existing methods for identifying and capturing change are often manual, domain-specific, and error-prone and do not scale to large scientific datasets. In this paper, we describe the design and implementation of Dac-Man framework, which identifies, captures, and manages change in large scientific datasets, and enables plug-in of domain-specific change analysis with minimal user effort. Our evaluations show that it can retrieve file changes from directories containing millions of files and terabytes of data in less than a minute.</blockquote></div></div></div></div><a href="includes/files/pap407s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap521"></a><div class="slot-title">Stacker: An Autonomic Data Movement Engine for Extreme-Scale Data Staging-Based In Situ Workflows</div><div class="slot-authors">Pradeep Subedi, Philip Davis, and Shaohua Duan (Rutgers University); Scott Klasky (Oak Ridge National Laboratory); Hemanth Kolla (Sandia National Laboratories); and Manish Parashar (Rutgers University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_368_1539825877_72" onclick="$('#vhsjs_view_368_1539825877_72').hide();
                $('#vhsjs_hide_368_1539825877_72').show();
                $('#367_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_368_1539825877_72" onclick="$('#367_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_368_1539825877_72').hide();
                $('#vhsjs_view_368_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="368_1539825877_72" id="367_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>Data staging and in situ workflows are being explored extensively as an approach to address data-related costs at very large scales. However, the impact of emerging storage architectures (e.g., deep memory hierarchies and burst buffers) upon data staging solutions remains a challenge. In this paper, we investigate how burst buffers can be effectively used by data staging solutions, for example, as a persistence storage tier of the memory hierarchy. Furthermore, we use machine learning based prefetching techniques to move data between the storage levels in an autonomous manner. We also present Stacker, a prototype of the proposed solutions implemented within the Data\-Spaces data staging service, and experimentally evaluate its performance and scalability using the S3D combustion workflow on current leadership class platforms. Our experiments demonstrate that Stacker achieves low latency, high volume data-staging with low overhead as compared to in-memory staging services for production scientific workflows.</blockquote></div></div></div></div><a href="includes/files/pap521s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap206"></a><div class="slot-title">A Year in the Life of a Parallel File System</div><div class="slot-authors">Glenn K. Lockwood (Lawrence Berkeley National Laboratory), Shane Snyder (Argonne National Laboratory), Teng Wang and Suren Byna (Lawrence Berkeley National Laboratory), Philip Carns (Argonne National Laboratory), and Nicholas J. Wright (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_370_1539825877_72" onclick="$('#vhsjs_view_370_1539825877_72').hide();
                $('#vhsjs_hide_370_1539825877_72').show();
                $('#369_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_370_1539825877_72" onclick="$('#369_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_370_1539825877_72').hide();
                $('#vhsjs_view_370_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="370_1539825877_72" id="369_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>I/O performance is a critical aspect of data-intensive scientific computing.  We seek to advance the state of the practice in understanding and diagnosing I/O performance issues through investigation of a comprehensive I/O performance data set that captures a full year of production storage activity at two leadership-scale computing facilities.  We demonstrate techniques to identify regions of interest, perform focused investigations of both long-term trends and transient anomalies, and uncover the contributing factors that lead to performance fluctuation.<br><br>We find that a year in the life of a parallel file system is comprised of distinct regions of long-term performance variation in addition to short-term performance transients.  We demonstrate how systematic identification of these performance regions, combined with comprehensive analysis, allows us to isolate the factors contributing to different performance maladies at different time scales.  From this, we present specific lessons learned and important considerations for HPC storage practitioners.</blockquote></div></div></div></div><a href="includes/files/pap206s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack188"></a><div class="section-title">Floating Point</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Architectures, Compiler Analysis and Optimization, Floating Point, Performance, Precision, Programming Systems, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Arithmetic and Optimization</div><div class="slot-entry"><a name="pap431"></a><div class="slot-title">Associative Instruction Reordering to Alleviate Register Pressure</div><div class="slot-authors">Prashant Singh Rawat, Aravind Sukumaran-Rajam, and Atanas Rountev (Ohio State University); Fabrice Rastello (French Institute for Research in Computer Science and Automation (INRIA)); Louis-Noel Pouchet (Colorado State University); and P. Sadayappan (Ohio State University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_372_1539825877_72" onclick="$('#vhsjs_view_372_1539825877_72').hide();
                $('#vhsjs_hide_372_1539825877_72').show();
                $('#371_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_372_1539825877_72" onclick="$('#371_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_372_1539825877_72').hide();
                $('#vhsjs_view_372_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="372_1539825877_72" id="371_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>Register allocation is generally considered a practically solved problem. For most applications, the register allocation strategies in production compilers are very effective in controlling the number of loads/stores and register spills. However, existing register allocation strategies are not effective and result in excessive register spilling for computation patterns with a high degree of many-to-many data reuse, e.g., high-order stencils and tensor contractions.  We develop a source-to-source instruction reordering strategy that exploits the flexibility of reordering associative operations to alleviate register pressure.  The developed transformation module implements an adaptable strategy that can appropriately control the degree of instruction-level parallelism, while relieving register pressure.  The effectiveness of the approach is demonstrated through experimental results using multiple production compilers (GCC, Clang/LLVM) and target platforms (Intel Xeon Phi, and Intel x86 multi-core).</blockquote></div></div></div></div><a href="includes/files/pap431s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap464"></a><div class="slot-title">Harnessing GPU's Tensor Cores Fast FP16 Arithmetic to Speedup Mixed-Precision Iterative Refinement Solvers</div><div class="slot-authors">Azzam Haidar (University of Tennessee, Innovative Computing Laboratory); Stan Tomov and Jack Dongarra (University of Tennessee); and Nicholas Higham (University of Manchester, School of Mathematics)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_374_1539825877_72" onclick="$('#vhsjs_view_374_1539825877_72').hide();
                $('#vhsjs_hide_374_1539825877_72').show();
                $('#373_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_374_1539825877_72" onclick="$('#373_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_374_1539825877_72').hide();
                $('#vhsjs_view_374_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="374_1539825877_72" id="373_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>The use of low-precision arithmetic in computing methods has been a powerful tool to accelerate numerous scientific computing applications including Artificial Intelligence. We present an investigation showing that other HPC applications can harness this power too, and in particular, the general HPC problem of solving Ax = b, where A is a large dense matrix, and the solution is needed in FP64 accuracy. Our approach is based on the mixed-precision (FP16->FP64) iterative refinement technique – we generalize and extend prior advances into a framework, for which we develop architecture-specific algorithms and highly-tuned implementations where we show how the use of FP16-TC (tensor cores) arithmetic can provide up to 4X speedup and improve the energy consumption by a factor of 5 achieving 74 Gflop/Watt. This is due to the performance boost that the FP16 (Tensor Cores) provide and to its better accuracy that outperforms the classical FP16.</blockquote></div></div></div></div><a href="includes/files/pap464s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap503"></a><div class="slot-title">ADAPT: Algorithmic Differentiation Applied to Floating-Point Precision Tuning</div><div class="slot-authors">Harshitha Menon (Lawrence Livermore National Laboratory); Michael O. Lam (James Madison University, Lawrence Livermore National Laboratory); and Daniel Osei-Kuffuor, Markus Schordan, Scott Lloyd, Kathryn Mohror, and Jeffrey Hittinger (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_376_1539825877_72" onclick="$('#vhsjs_view_376_1539825877_72').hide();
                $('#vhsjs_hide_376_1539825877_72').show();
                $('#375_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_376_1539825877_72" onclick="$('#375_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_376_1539825877_72').hide();
                $('#vhsjs_view_376_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="376_1539825877_72" id="375_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>HPC applications extensively use floating point arithmetic operations to solve computational problems in various domains. Mixed precision computing, use of lowest precision data type sufficient to achieve a desired accuracy, have been explored to improve performance, reduce power consumption and data movement. Manually optimizing the program to use mixed precision is challenging. In this work, we present ADAPT, an approach for mixed precision analysis on HPC workloads while providing guarantees about the final output error. Our approach uses algorithmic differentiation to accurately estimate the output error for mixed precision configuration. ADAPT provides floating-point precision sensitivity of programs, which highlights regions of the code that that can potentially be converted to lower precision, is used to make algorithmic choices and develop mixed precision configurations. We evaluate ADAPT on six benchmarks and a proxy application and show that we are able to achieve a speedup of 1.2x on the proxy application, LULESH.</blockquote></div></div></div></div><a href="includes/files/pap503s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack183"></a><div class="section-title">GPUs</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">GPUs, Resiliency, State of the Practice, System Software, Tech Program Reg Pass</span><br /><div class="session-title">Resilience</div><div class="slot-entry"><a name="pap262"></a><div class="slot-title">GPU Age-Aware Scheduling to Improve the Reliability of Leadership Jobs on Titan</div><div class="slot-authors">Christopher Zimmer, Don Maxwell, Stephen McNally, Scott Atchley, and Sudharshan S. Vazhkudai (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_378_1539825877_72" onclick="$('#vhsjs_view_378_1539825877_72').hide();
                $('#vhsjs_hide_378_1539825877_72').show();
                $('#377_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_378_1539825877_72" onclick="$('#377_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_378_1539825877_72').hide();
                $('#vhsjs_view_378_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="378_1539825877_72" id="377_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>The increasing rate of failures on the Oak Ridge Leadership Computing Facility's (OLCF) Titan supercomputer, resulted in the replacement of 50% of its GPUs between 2015 and 2017. The largest jobs, also known as "leadership jobs'', continued to experience increased application failures. These jobs contained significant amounts of low-failure rate and high-failure rate GPUs. The impacts of these failures were felt more by leadership jobs due to longer wait times, runtimes, and higher charge rates. In this work, we have designed techniques to increase the use of low-failure GPUs in leadership jobs through targeted resource allocation. This employed two complementary techniques, updating both the system ordering and the allocation mechanisms. In simulation, the application of these techniques resulted in a 33% increase in low-failure GPU hours being assigned to leadership jobs. Our GPU Age-Aware Scheduling has been used in production on Titan since July of 2017.</blockquote></div></div></div></div><a href="includes/files/pap262s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap109"></a><div class="slot-title">FlipTracker: Understanding Natural Error Resilience in HPC Applications</div><div class="slot-authors">Luanzheng Guo and Dong Li (University of California, Merced); Ignacio Laguna (Lawrence Livermore National Laboratory); and Martin Schulz (Technical University Munich)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_380_1539825877_72" onclick="$('#vhsjs_view_380_1539825877_72').hide();
                $('#vhsjs_hide_380_1539825877_72').show();
                $('#379_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_380_1539825877_72" onclick="$('#379_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_380_1539825877_72').hide();
                $('#vhsjs_view_380_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="380_1539825877_72" id="379_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>As high-performance computing systems scale in size and computational power, the danger of silent errors, i.e., errors that can bypass hardware detection mechanisms and impact application state, grows dramatically. Consequently, applications running on HPC systems need to exhibit resilience to such errors. Previous work has found that, for certain codes, this resilience can come for free, i.e., some applications are naturally resilient, but few works have shown the code patterns—combinations or sequences of computations—that make an application naturally resilient. In this paper, we present FlipTracker, a framework designed to extract these patterns using fine-grained tracking of error propagation and resilience properties, and we use it to present a set of computation patterns that are responsible for making representative HPC applications naturally resilient to errors. This not only enables a deeper understanding of resilience properties of these codes, but also can guide future application designs toward patterns with natural resilience.</blockquote></div></div></div></div><a href="includes/files/pap109s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap111"></a><div class="slot-title">Doomsday: Predicting Which Node Will Fail When on Supercomputers</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Anwesha Das and Frank Mueller (North Carolina State University) and Paul Hargrove, Eric Roman, and Scott Baden (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_382_1539825877_72" onclick="$('#vhsjs_view_382_1539825877_72').hide();
                $('#vhsjs_hide_382_1539825877_72').show();
                $('#381_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_382_1539825877_72" onclick="$('#381_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_382_1539825877_72').hide();
                $('#vhsjs_view_382_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="382_1539825877_72" id="381_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>Predicting which node will fail and how soon remains a challenge for HPC resilience, yet may pave the way to exploiting proactive remedies before jobs fail. Not only for increasing scalability up to exascale systems, but even for contemporary supercomputer architectures does it require substantial efforts to distill anomalous events from noisy raw logs. To this end, we propose a novel phrase extraction mechanism called TBP (time-based phrases) to pin-point node failures, which is unprecedented.  Our study, based on real system data and statistical machine learning, demonstrates the feasibility to predict which specific node will fail in Cray systems. TBP achieves no less than 83% recall rates with lead times as high as 2 minutes. This opens up the door for enhancing prediction lead times for supercomputing systems in general, thereby facilitating efficient usage of both computing capacity and power in large scale production systems.</blockquote></div></div></div></div><a href="includes/files/pap111s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">GPUs, Memory, NVRAM, Performance, System Software, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Non-Volatile Memory</div><div class="slot-entry"><a name="pap203"></a><div class="slot-title">Runtime Data Management on Non-Volatile Memory-Based Heterogeneous Memory for Task-Parallel Programs</div><div class="slot-authors">Kai Wu, Jie Ren, and Dong Li (University of California, Merced)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_384_1539825877_72" onclick="$('#vhsjs_view_384_1539825877_72').hide();
                $('#vhsjs_hide_384_1539825877_72').show();
                $('#383_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_384_1539825877_72" onclick="$('#383_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_384_1539825877_72').hide();
                $('#vhsjs_view_384_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="384_1539825877_72" id="383_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>Non-volatile memory (NVM) provides a scalable solution to replace DRAM as main memory. Because of relatively high latency and low bandwidth of NVM (comparing with DRAM), NVM often pairs with DRAM to build a  heterogeneous main memory system (HMS). Deciding data placement on NVM-based HMS is critical to enable future NVM-based HPC. In this paper, we study task-parallel programs and introduce a runtime system to address the data placement problem on NVM-based HMS. Leveraging semantics and execution mode of task-parallel programs, we efficiently characterize memory access patterns of tasks and reduce data movement overhead. We also introduce a performance model to predict performance for tasks with various data placements on HMS. Evaluating with a set of HPC benchmarks, we show that our runtime system achieves higher performance than a conventional HMS-oblivious runtime (24% improvement on average) and two state-of-the-art HMS-aware solutions (16% and 11% improvement on average, respectively).</blockquote></div></div></div></div><a href="includes/files/pap203s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap194"></a><div class="slot-title">DRAGON: Breaking GPU Memory Capacity Limits with Direct NVM Access</div><div class="slot-authors">Pak Markthub (Tokyo Institute of Technology); Mehmet E. Belviranli, Seyong Lee, and Jeffrey S. Vetter (Oak Ridge National Laboratory); and Satoshi Matsuoka (RIKEN, Tokyo Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_386_1539825877_72" onclick="$('#vhsjs_view_386_1539825877_72').hide();
                $('#vhsjs_hide_386_1539825877_72').show();
                $('#385_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_386_1539825877_72" onclick="$('#385_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_386_1539825877_72').hide();
                $('#vhsjs_view_386_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="386_1539825877_72" id="385_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>Heterogeneous computing with accelerators is growing in importance in high performance computing (HPC). Recently, application datasets have expanded beyond the memory capacity of these accelerators, and often beyond the capacity of their hosts. Meanwhile, nonvolatile memory (NVM) storage has emerged as a pervasive component in HPC systems because NVM provides massive amounts of memory capacity at affordable cost. Currently, for accelerator applications to use NVM, they must manually orchestrate data movement across multiple memories and this approach only performs well for applications with simple access behaviors. To address this issue, we developed DRAGON, a solution that enables all classes of GP-GPU applications to transparently compute on terabyte datasets residing in NVM. DRAGON leverages the page-faulting mechanism on the recent NVIDIA GPUs by extending capabilities of CUDA Unified Memory (UM). Our experimental results show that DRAGON transparently expands memory capacity and obtain additional speedups via automated I/O and data transfer overlapping.</blockquote></div></div></div></div><a href="includes/files/pap194s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap355"></a><div class="slot-title">Siena: Exploring the Design Space of Heterogeneous Memory Systems</div><div class="slot-authors">Ivy B. Peng and Jeffrey S. Vetter (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_388_1539825877_72" onclick="$('#vhsjs_view_388_1539825877_72').hide();
                $('#vhsjs_hide_388_1539825877_72').show();
                $('#387_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_388_1539825877_72" onclick="$('#387_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_388_1539825877_72').hide();
                $('#vhsjs_view_388_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="388_1539825877_72" id="387_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>Memory systems are crucial to the performance, power, and cost of high-performance computing systems. Recently, multiple factors are driving the need for more complex, deep memory hierarchies. However, architects and customers are struggling to design memory systems that effectively balance multiple, often competing, factors in this large, multidimensional, and fast-moving design space. In this paper, we systematically explore the organization of heterogeneous memory systems on a framework, called Siena. Siena facilitates quick exploration of memory architectures with flexible configurations of memory systems and realistic memory workloads. We perform a design space exploration on 22 proposed memory systems using eight relevant workloads. Our results show that horizontal organizations of memories can achieve higher performance than that of vertical organizations when the distribution of memory traffic balances the performance gap between memories. However, the coupling effects through shared resources and application behaviors could negate the advantage of high-performance memory in horizontal organizations.</blockquote></div></div></div></div><a href="includes/files/pap355s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, GPUs, Linear Algebra, Networks, Resiliency, Tech Program Reg Pass</span><br /><div class="session-title">Resilience III: GPUs</div><div class="slot-entry"><a name="pap247"></a><div class="slot-title">Optimizing Software-Directed Instruction Replication for GPU Error Detection</div><div class="slot-authors">Abdulrahman Mahmoud (University of Illinois) and Siva Kumar Sastry Hari, Michael B. Sullivan, Timothy Tsai, and Stephen W. Keckler (Nvidia Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_390_1539825877_72" onclick="$('#vhsjs_view_390_1539825877_72').hide();
                $('#vhsjs_hide_390_1539825877_72').show();
                $('#389_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_390_1539825877_72" onclick="$('#389_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_390_1539825877_72').hide();
                $('#vhsjs_view_390_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="390_1539825877_72" id="389_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>Application execution on safety-critical and high-performance computer systems must be resilient to transient errors. As GPUs become more pervasive in such systems, they must supplement ECC/parity for major storage structures with reliability techniques that cover more of the GPU hardware logic.  Instruction duplication has been explored for CPU resilience; however, it has never been studied in the context of GPUs, and it is unclear whether the performance and design choices it presents makes it a feasible GPU solution. This paper describes a practical methodology to employ instruction duplication for GPUs and identifies implementation challenges that can incur high overheads (69% on average). It explores GPU-specific software optimizations that trade fine-grained recoverability for performance. It also proposes simple ISA extensions with limited hardware changes and area costs to further improve performance, cutting the runtime overheads by more than half to an average of 30%.</blockquote></div></div></div></div><a href="includes/files/pap247s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap244"></a><div class="slot-title">Fault Tolerant One-Sided Matrix Decompositions on Heterogeneous Systems with GPUs</div><div class="slot-authors">Jieyang Chen, Hongbo Li, Sihuan Li, and Xin Liang (University of California, Riverside); Panruo Wu (University of Houston); Dingwen Tao (University of Alabama); Kaiming Ouyang, Yuanlai Liu, and Kai Zhao (University of California, Riverside); Qiang Guan (Kent State University); and Zizhong Chen (University of California, Riverside)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_392_1539825877_72" onclick="$('#vhsjs_view_392_1539825877_72').hide();
                $('#vhsjs_hide_392_1539825877_72').show();
                $('#391_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_392_1539825877_72" onclick="$('#391_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_392_1539825877_72').hide();
                $('#vhsjs_view_392_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="392_1539825877_72" id="391_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>Current algorithm-based fault tolerance (ABFT) approach for one-sided matrix decomposition on heterogeneous systems with GPUs have following limitations: (1) they do not provide sufficient protection as most of them only maintain checksum in one dimension; (2) their checking scheme is not efficient due to redundant checksum verifications; (3) they fail to protect PCIe communication; (4) the checksum calculation based on a special type of matrix multiplication is far from efficient. By overcoming the above limitations, we design an efficient ABFT approach providing stronger protection for one-sided matrix decomposition methods on heterogeneous systems. First, we provide full matrix protection by using checksums in two dimensions. Second, our checking scheme is more efficient by prioritizing the checksum verification according to the sensitivity of matrix operations to soft errors. Third, we protect PCIe communication by reordering checksum verifications and decomposition steps. Fourth, we accelerate the checksum calculation by 1.7x via better utilizing GPUs.</blockquote></div></div></div></div><a href="includes/files/pap244s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap430"></a><div class="slot-title">PRISM: Predicting Resilience of GPU Applications Using Statistical Methods</div><div class="slot-authors">Charu Kalra, Fritz Previlon, and Xiangyu Li (Northeastern University); Norman Rubin (Nvidia Corporation); and David Kaeli (Northeastern University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_394_1539825877_72" onclick="$('#vhsjs_view_394_1539825877_72').hide();
                $('#vhsjs_hide_394_1539825877_72').show();
                $('#393_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_394_1539825877_72" onclick="$('#393_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_394_1539825877_72').hide();
                $('#vhsjs_view_394_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="394_1539825877_72" id="393_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>As Graphics Processing Units (GPUs) become more pervasive in HPC and safety-critical domains, ensuring that GPU applications can be protected from data corruption grows in importance. Despite prior efforts to mitigate errors, we still lack a clear understanding of how resilient these applications are in the presence of transient faults.  Due to the random nature of these faults, predicting whether they will alter the program output is a challenging problem. In this paper, we build a framework named PRISM, which uses a systematic approach to predict failures in GPU programs. PRISM extracts micro-architecture agnostic features to characterize program resiliency, which serve as predictors in our statistical model. PRISM enables us to predict failures in applications without running exhaustive fault-injection campaigns on a GPU, thereby reducing the error estimation effort. PRISM can also be used to gain insight into potential architectural support required to improve the reliability of GPU applications.</blockquote></div></div></div></div><a href="includes/files/pap430s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack125"></a><div class="section-title">Graph Algorithms</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Graph Algorithms, Linear Algebra, Machine Learning, Sparse Computation, Tech Program Reg Pass</span><br /><div class="session-title">Algorithms on Sparse Data</div><div class="slot-entry"><a name="pap511"></a><div class="slot-title">HiCOO: Hierarchical Storage of Sparse Tensors</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Jiajia Li, Jimeng Sun, and Richard Vuduc (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_396_1539825877_72" onclick="$('#vhsjs_view_396_1539825877_72').hide();
                $('#vhsjs_hide_396_1539825877_72').show();
                $('#395_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_396_1539825877_72" onclick="$('#395_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_396_1539825877_72').hide();
                $('#vhsjs_view_396_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="396_1539825877_72" id="395_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>This paper proposes a new storage format for sparse tensors, called Hierarchical COOrdinate (HiCOO; pronounced: “haiku”). It derives from coordinate (COO) format, arguably the de facto standard for general sparse tensor storage. HiCOO improves upon COO by compressing the indices in units of sparse tensor blocks, with the goals of preserving the “mode-agnostic” simplicity of COO while reducing the bytes needed to represent the tensor and promoting data locality. We evaluate HiCOO by implementing a single-node, multicore-parallel version of the matricized tensor-times-Khatri-Rao product (MTTKRP) operation, which is the most expensive computational core in the widely used CANDECOMP/PARAFAC decomposition(CPD) algorithm. This MTTKRP implementation achieves up to 23.0× (6.8× on average) speedup over COO format and up to 15.6× (3.1× on average) speedup over another state-of-the-art format, compressed sparse fiber (CSF), by using less or comparable storage of them. When used within CPD, we also observe speedups against COO- and CSF-based implementations.</blockquote></div></div></div></div><a href="includes/files/pap511s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap273"></a><div class="slot-title">Distributed Memory Sparse Inverse Covariance Matrix Estimation on High-Performance Computing Architectures</div><div class="slot-authors">Aryan Eftekhari (University of Lugano), Matthias Bollhöfer (Braunschweig University of Technology), and Olaf Schenk (University of Lugano)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_398_1539825877_72" onclick="$('#vhsjs_view_398_1539825877_72').hide();
                $('#vhsjs_hide_398_1539825877_72').show();
                $('#397_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_398_1539825877_72" onclick="$('#397_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_398_1539825877_72').hide();
                $('#vhsjs_view_398_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="398_1539825877_72" id="397_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>We consider the problem of estimating sparse inverse covariance matrices for high-dimensional datasets using the l1-regularized Gaussian maximum likelihood method. This task is particularly challenging as the required computational resources increase superlinearly with the dimensionality of the dataset. We introduce a performant and scalable algorithm which builds on the current advancements of second-order, maximum likelihood methods. The routine leverages the intrinsic parallelism in the linear algebra operations and exploits the underlying sparsity of the problem. The computational bottlenecks are identified and the respective subroutines are parallelized using an MPI-OpenMP approach. Experiments conducted on a Cray XC50 system at the Swiss National Supercomputing Center show that, in comparison to the state-of-the-art algorithms, the proposed routine provides significant strong scaling speedup with ideal scalability up to 128 nodes. The developed framework is used to estimate the sparse inverse covariance matrix of both synthetic and real-world datasets with up to 10 million dimensions.</blockquote></div></div></div></div><a href="includes/files/pap273s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap466"></a><div class="slot-title">PruneJuice:  Pruning Trillion-Edge Graphs to a Precise Pattern-Matching Solution</div><div class="slot-authors">Tahsin Reza, Matei Ripeanu, and Nicolas Tripoul (University of British Columbia) and Geoffrey Sanders and Roger Pearce (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_400_1539825877_72" onclick="$('#vhsjs_view_400_1539825877_72').hide();
                $('#vhsjs_hide_400_1539825877_72').show();
                $('#399_1539825877_72').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_400_1539825877_72" onclick="$('#399_1539825877_72').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_400_1539825877_72').hide();
                $('#vhsjs_view_400_1539825877_72').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="400_1539825877_72" id="399_1539825877_72" style="display: none"><div class="arrow-slidedown"><blockquote>Pattern matching is a powerful graph analysis tool. Unfortunately, existing solutions have limited scalability, support only a limited set of search patterns, and/or focus on only a subset of the real-world problems associated with pattern matching. This paper presents a new algorithmic pipeline that: (i) enables highly scalable pattern matching on labeled graphs, (ii) supports arbitrary patterns, (iii) enables trade-offs between precision and time-to-solution (while always selecting all vertices and edges that participate in matches, thus offering 100% recall), and (iv) supports a set of popular data analytics scenarios. We implement our approach on top of HavoqGT and demonstrate its advantages through strong and weak scaling experiments on massive-scale real-world (up to 257 billion edges) and synthetic (up to 4.4 trillion edges) graphs, respectively, and at scales (1,024 nodes / 36,864 cores) orders of magnitude larger than used in the past for similar problems.</blockquote></div></div></div></div><a href="includes/files/pap466s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Applications, Graph Algorithms, Security, Tech Program Reg Pass</span><br /><div class="session-title">Graph Algorithms and Systems</div><div class="slot-entry"><a name="pap115"></a><div class="slot-title">iSpan: Parallel Identification of Strongly Connected Components with Spanning Trees</div><div class="slot-authors">Yuede Ji (George Washington University); Hang Liu (University of Massachusetts, Lowell); and H. Howie Huang (George Washington University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_402_1539825877_73" onclick="$('#vhsjs_view_402_1539825877_73').hide();
                $('#vhsjs_hide_402_1539825877_73').show();
                $('#401_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_402_1539825877_73" onclick="$('#401_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_402_1539825877_73').hide();
                $('#vhsjs_view_402_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="402_1539825877_73" id="401_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>Detecting strongly connected components (SCCs) in a directed graph is crucial for understanding the structure of graphs. Most real-world graphs have one large SCC that contains the majority of the vertices, and many small SCCs whose sizes are reversely proportional to the frequency of their occurrence. For both types of SCCs, current approaches that rely on depth or breadth first search (DFS or BFS) face the challenges of strict synchronization requirement and high computation cost. In this paper, we advocate a new paradigm of identifying SCCs with simple spanning trees, since SCC detection requires only the knowledge of connectivity among the vertices. We have developed a prototype called iSpan which consists of parallel, relaxed synchronization construction of spanning trees for detecting the large and small SCCs. The evaluations show that iSpan is able to significantly outperform current state-of-the-art DFS and BFS- based methods by average 18× and 4×, respectively.</blockquote></div></div></div></div><a href="includes/files/pap115s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap468"></a><div class="slot-title">Adaptive Anonymization of Data with b-Edge Covers</div><div class="slot-authors">Arif Khan (Pacific Northwest National Laboratory), Krzysztof Choromanski (Google LLC), Alex Pothen and S M Ferdous (Purdue University), and Mahantesh Halappanavar and Antonino Tumeo (Pacific Northwest National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_404_1539825877_73" onclick="$('#vhsjs_view_404_1539825877_73').hide();
                $('#vhsjs_hide_404_1539825877_73').show();
                $('#403_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_404_1539825877_73" onclick="$('#403_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_404_1539825877_73').hide();
                $('#vhsjs_view_404_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="404_1539825877_73" id="403_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>We explore the problem of sharing data that pertains to individuals with anonymity guarantees, where each user requires a desired level of privacy.  We propose the first shared-memory as well as distributed memory parallel algorithms for the adaptive anonymity problem that achieves this goal, and produces high quality anonymized datasets.  <br><br>The new algorithm is based on an optimization procedure that iteratively computes weights on the edges of a dissimilarity matrix, and at each iteration computes a minimum weighted b-Edge cover in the graph. We are able to solve adaptive anonymity problems with hundreds of thousands of instances and hundreds of features on a leadership-class supercomputer in under five minutes. Our algorithm scales up to 4K cores on a distributed memory supercomputer, while also providing good speedups on shared memory multiprocessors. On smaller problems, where an algorithm based on Belief Propagation is feasible, our algorithm is two orders of magnitude faster.</blockquote></div></div></div></div><a href="includes/files/pap468s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap167"></a><div class="slot-title">faimGraph: High Performance Management of Fully-Dynamic Graphs Under Tight Memory Constraints on the GPU</div><div class="slot-authors">Martin Winter and Daniel Mlakar (Graz University of Technology); Rhaleb Zayer and Hans-Peter Seidel (Max Planck Institute for Informatics); and Markus Steinberger (Graz University of Technology, Max Planck Institute for Informatics)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_406_1539825877_73" onclick="$('#vhsjs_view_406_1539825877_73').hide();
                $('#vhsjs_hide_406_1539825877_73').show();
                $('#405_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_406_1539825877_73" onclick="$('#405_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_406_1539825877_73').hide();
                $('#vhsjs_view_406_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="406_1539825877_73" id="405_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>In this paper, we present a fully-dynamic graph data structure for the Graphics Processing Unit (GPU). It delivers high update rates while keeping a low memory footprint using autonomous memory management directly on the GPU. The data structure is fully-dynamic, allowing not only for edge but also vertex updates. Performing the memory management on the GPU allows for fast initialization times and efficient update procedures without additional intervention or reallocation procedures from the host.  faimGraph is the first GPU graph framework that fully reclaims unused memory, permitting long time application with highly changing graph structures. Performance evaluations show that our approach outperforms that previous state-of-the-art in for all types of graph updates. Furthermore, evaluate algorithmic performance using a PageRank and a Static Triangle Counting (STC) implementation, demonstrating the suitability of the framework even for memory access intensive algorithms.</blockquote></div></div></div></div><a href="includes/files/pap167s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack129"></a><div class="section-title">I/O</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Clouds and Distributed Computing, File Systems, I/O, Storage, Tech Program Reg Pass</span><br /><div class="session-title">Data and Storage</div><div class="slot-entry"><a name="pap165"></a><div class="slot-title">SP-Cache: Load-Balanced, Redundancy-Free Cluster Caching with Selective Partition</div><div class="slot-authors">Yinghao Yu, Renfei Huang, Wei Wang, Jun Zhang, and Khaled Ben Letaief (Hong Kong University of Science and Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_408_1539825877_73" onclick="$('#vhsjs_view_408_1539825877_73').hide();
                $('#vhsjs_hide_408_1539825877_73').show();
                $('#407_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_408_1539825877_73" onclick="$('#407_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_408_1539825877_73').hide();
                $('#vhsjs_view_408_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="408_1539825877_73" id="407_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>Data-intensive clusters increasingly employ in-memory solutions to improve I/O performance. However, the routinely observed file popularity skew and load imbalance create hotspots, which significantly degrades the benefits of in-memory solutions. Common approaches to tame load imbalance include copying multiple replicas of hot files and creating parity chunks using storage codes. Yet, these techniques either suffer from high memory redundancy or incur non-trivial encoding/decoding overhead. In this paper, we propose a different approach to achieve load balancing without memory redundancy or encoding/decoding overhead. Our solution, termed SP-Cache, selectively partitions files based on their popularity and evenly caches those partitions across the cluster. We develop an efficient algorithm to determine the optimal number of partitions for hot files—too few partitions are incapable of mitigating hotspots, while too many are susceptible to stragglers. EC2 deployment and trace-driven simulations show that, compared with existing solutions, SP-Cache reduces the read latencies by up to 40%.</blockquote></div></div></div></div><a href="includes/files/pap165s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap585"></a><div class="slot-title">BESPOKV: Application Tailored Scale-Out Key-Value Stores</div><div class="slot-authors">Ali Anwar (IBM), Yue Cheng (George Mason University), Hai Huang (IBM), Jingoo Han (Virginia Tech), Hyogi Sim (Oak Ridge National Laboratory), Dongyoon Lee (Virginia Tech), Fred Douglis (Perspecta Labs), and Ali R. Butt (Virginia Tech)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_410_1539825877_73" onclick="$('#vhsjs_view_410_1539825877_73').hide();
                $('#vhsjs_hide_410_1539825877_73').show();
                $('#409_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_410_1539825877_73" onclick="$('#409_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_410_1539825877_73').hide();
                $('#vhsjs_view_410_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="410_1539825877_73" id="409_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>Enterprise KV stores are not well suited for HPC applications, and entail customization and cumbersome end-to-end KV design to extract the HPC application needs. In this paper we present BESPOKV, an adaptive, extensible, and scale-out KV store framework. BESPOKV decouples the KV store design into the control plane for distributed management and the data plane for local data store. BESPOKV takes as input a single-server KV store, called a datalet, and transparently enables a scalable and fault-tolerant distributed KV store service. The resulting distributed stores are also adaptive to consistency or topology requirement changes and can be easily extended for new types of services. Experiments show that BESPOKV-enabled distributed KV stores scale horizontally to a large number of nodes, and performs comparably and sometimes better than the state-of-the-art systems.</blockquote></div></div></div></div><a href="includes/files/pap585s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap450"></a><div class="slot-title">Scaling Embedded In Situ Indexing with DeltaFS</div><div class="slot-authors">Qing Zheng, Charles D. Cranor, Danhao Guo, Gregory R. Ganger, George Amvrosiadis, and Garth A. Gibson (Carnegie Mellon University) and Bradley W. Settlemyer, Gary Grider, and Fan Guo (Los Alamos National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_412_1539825877_73" onclick="$('#vhsjs_view_412_1539825877_73').hide();
                $('#vhsjs_hide_412_1539825877_73').show();
                $('#411_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_412_1539825877_73" onclick="$('#411_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_412_1539825877_73').hide();
                $('#vhsjs_view_412_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="412_1539825877_73" id="411_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>Analysis of large-scale simulation output is a core element of scientific inquiry, but analysis queries may experience significant I/O overhead when the data is not structured for efficient retrieval. While in-situ processing allows for improved time-to-insight for many applications, scaling in-situ frameworks to hundreds of thousands of cores can be difficult in practice. The DeltaFS in-situ indexing is a new approach for in-situ processing of massive amounts of data to achieve efficient point and small-range queries. This paper describes the challenges and lessons learned when scaling this in-situ processing function to hundreds of thousands of cores. We propose techniques for scalable all-to-all communication that is memory and bandwidth efficient, concurrent indexing, and specialized LSM-Tree formats. Combining these techniques allows DeltaFS to control the cost of in-situ processing while maintaining 3 orders of magnitude query speedup when scaling alongside the popular VPIC particle-in-cell code to 131,072 cores.</blockquote></div></div></div></div><a href="includes/files/pap450s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack132"></a><div class="section-title">Linear Algebra</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Graph Algorithms, Linear Algebra, Machine Learning, Sparse Computation, Tech Program Reg Pass</span><br /><div class="session-title">Algorithms on Sparse Data</div><div class="slot-entry"><a name="pap511"></a><div class="slot-title">HiCOO: Hierarchical Storage of Sparse Tensors</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Jiajia Li, Jimeng Sun, and Richard Vuduc (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_414_1539825877_73" onclick="$('#vhsjs_view_414_1539825877_73').hide();
                $('#vhsjs_hide_414_1539825877_73').show();
                $('#413_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_414_1539825877_73" onclick="$('#413_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_414_1539825877_73').hide();
                $('#vhsjs_view_414_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="414_1539825877_73" id="413_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>This paper proposes a new storage format for sparse tensors, called Hierarchical COOrdinate (HiCOO; pronounced: “haiku”). It derives from coordinate (COO) format, arguably the de facto standard for general sparse tensor storage. HiCOO improves upon COO by compressing the indices in units of sparse tensor blocks, with the goals of preserving the “mode-agnostic” simplicity of COO while reducing the bytes needed to represent the tensor and promoting data locality. We evaluate HiCOO by implementing a single-node, multicore-parallel version of the matricized tensor-times-Khatri-Rao product (MTTKRP) operation, which is the most expensive computational core in the widely used CANDECOMP/PARAFAC decomposition(CPD) algorithm. This MTTKRP implementation achieves up to 23.0× (6.8× on average) speedup over COO format and up to 15.6× (3.1× on average) speedup over another state-of-the-art format, compressed sparse fiber (CSF), by using less or comparable storage of them. When used within CPD, we also observe speedups against COO- and CSF-based implementations.</blockquote></div></div></div></div><a href="includes/files/pap511s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap273"></a><div class="slot-title">Distributed Memory Sparse Inverse Covariance Matrix Estimation on High-Performance Computing Architectures</div><div class="slot-authors">Aryan Eftekhari (University of Lugano), Matthias Bollhöfer (Braunschweig University of Technology), and Olaf Schenk (University of Lugano)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_416_1539825877_73" onclick="$('#vhsjs_view_416_1539825877_73').hide();
                $('#vhsjs_hide_416_1539825877_73').show();
                $('#415_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_416_1539825877_73" onclick="$('#415_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_416_1539825877_73').hide();
                $('#vhsjs_view_416_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="416_1539825877_73" id="415_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>We consider the problem of estimating sparse inverse covariance matrices for high-dimensional datasets using the l1-regularized Gaussian maximum likelihood method. This task is particularly challenging as the required computational resources increase superlinearly with the dimensionality of the dataset. We introduce a performant and scalable algorithm which builds on the current advancements of second-order, maximum likelihood methods. The routine leverages the intrinsic parallelism in the linear algebra operations and exploits the underlying sparsity of the problem. The computational bottlenecks are identified and the respective subroutines are parallelized using an MPI-OpenMP approach. Experiments conducted on a Cray XC50 system at the Swiss National Supercomputing Center show that, in comparison to the state-of-the-art algorithms, the proposed routine provides significant strong scaling speedup with ideal scalability up to 128 nodes. The developed framework is used to estimate the sparse inverse covariance matrix of both synthetic and real-world datasets with up to 10 million dimensions.</blockquote></div></div></div></div><a href="includes/files/pap273s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap466"></a><div class="slot-title">PruneJuice:  Pruning Trillion-Edge Graphs to a Precise Pattern-Matching Solution</div><div class="slot-authors">Tahsin Reza, Matei Ripeanu, and Nicolas Tripoul (University of British Columbia) and Geoffrey Sanders and Roger Pearce (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_418_1539825877_73" onclick="$('#vhsjs_view_418_1539825877_73').hide();
                $('#vhsjs_hide_418_1539825877_73').show();
                $('#417_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_418_1539825877_73" onclick="$('#417_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_418_1539825877_73').hide();
                $('#vhsjs_view_418_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="418_1539825877_73" id="417_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>Pattern matching is a powerful graph analysis tool. Unfortunately, existing solutions have limited scalability, support only a limited set of search patterns, and/or focus on only a subset of the real-world problems associated with pattern matching. This paper presents a new algorithmic pipeline that: (i) enables highly scalable pattern matching on labeled graphs, (ii) supports arbitrary patterns, (iii) enables trade-offs between precision and time-to-solution (while always selecting all vertices and edges that participate in matches, thus offering 100% recall), and (iv) supports a set of popular data analytics scenarios. We implement our approach on top of HavoqGT and demonstrate its advantages through strong and weak scaling experiments on massive-scale real-world (up to 257 billion edges) and synthetic (up to 4.4 trillion edges) graphs, respectively, and at scales (1,024 nodes / 36,864 cores) orders of magnitude larger than used in the past for similar problems.</blockquote></div></div></div></div><a href="includes/files/pap466s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Linear Algebra, Memory, MPI, OpenMP, Programming Systems, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Programming Systems Tools</div><div class="slot-entry"><a name="pap179"></a><div class="slot-title">Dynamic Data Race Detection for OpenMP Programs</div><div class="slot-authors">Yizi Gu and John Mellor-Crummey (Rice University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_420_1539825877_73" onclick="$('#vhsjs_view_420_1539825877_73').hide();
                $('#vhsjs_hide_420_1539825877_73').show();
                $('#419_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_420_1539825877_73" onclick="$('#419_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_420_1539825877_73').hide();
                $('#vhsjs_view_420_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="420_1539825877_73" id="419_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>Two concurrent accesses to a shared variable that are unordered by synchronization are said to be a data race if at least one access is a write. Data races cause shared memory parallel programs to behave unpredictably. This paper describes ROMP -- a tool for detecting data races in executions of scalable parallel applications that employ OpenMP for node-level parallelism. The complexity of OpenMP, which includes primitives for managing data environments, SPMD and SIMD parallelism, work sharing, tasking, mutual exclusion, and ordering, presents a formidable challenge for data race detection. ROMP is a hybrid data race detector that tracks accesses, access orderings, and mutual exclusion. Unlike other OpenMP race detectors, ROMP detects races with respect to logical parallelism rather than implementation threads. Experiments show that ROMP yields precise race reports for a broader set of OpenMP constructs than prior state-of-the-art race detectors.</blockquote></div></div></div></div><a href="includes/files/pap179s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap256"></a><div class="slot-title">ParSy: Inspection and Transformation of Sparse Matrix Computations for Parallelism</div><div class="slot-authors">Kazem Cheshmi (University of Toronto), Shoaib Kamil (Adobe Research), Michelle Mills Strout (University of Arizona), and Maryam Mehri Dehnavi (University of Toronto)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_422_1539825877_73" onclick="$('#vhsjs_view_422_1539825877_73').hide();
                $('#vhsjs_hide_422_1539825877_73').show();
                $('#421_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_422_1539825877_73" onclick="$('#421_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_422_1539825877_73').hide();
                $('#vhsjs_view_422_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="422_1539825877_73" id="421_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>In this work, we describe ParSy, a framework that uses a novel inspection strategy along with a simple code transformation to optimize parallel sparse algorithms for shared memory processors. Unlike existing approaches that can suffer from load imbalance and excessive synchronization, ParSy uses a novel task coarsening strategy to create well-balanced tasks that can execute in parallel, while maintaining locality of memory accesses. Code using the ParSy inspector and transformation outperforms existing highly-optimized sparse matrix algorithms such as Cholesky factorization on multi-core processors with speedups of 2.8× and 3.1× over the MKL Pardiso and PaStiX libraries respectively.</blockquote></div></div></div></div><a href="includes/files/pap256s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap382"></a><div class="slot-title">Detecting MPI Usage Anomalies via Partial Program Symbolic Execution</div><div class="slot-authors">Fangke Ye, Jisheng Zhao, and Vivek Sarkar (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_424_1539825877_73" onclick="$('#vhsjs_view_424_1539825877_73').hide();
                $('#vhsjs_hide_424_1539825877_73').show();
                $('#423_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_424_1539825877_73" onclick="$('#423_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_424_1539825877_73').hide();
                $('#vhsjs_view_424_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="424_1539825877_73" id="423_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>MPI is a message passing based programming model for distributed-memory parallelism that has been had been widely used for programming supercomputers for over 25 years. However, debugging and verification of MPI programs is widely recognized to be a deep technical challenge. This challenge is further exacerbated by a recent increase in the use of nonblocking MPI operations that bring new classes of bugs related to data races.<br><br>In this paper, we introduce a new MPI program debugging approach based on partial symbolic execution so as to avoid the false alarms inherent in the static analysis based methodology. Compared with the dynamic approach, our approach can be applied to incomplete programs and explore multiple execution paths, thereby bringing more flexibility and precision. By comparing with well known static/dynamic tools on real-world MPI applications, our approach shows same precision as the dynamic tool and avoids false positive produced by the static tool.</blockquote></div></div></div></div><a href="includes/files/pap382s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, GPUs, Linear Algebra, Networks, Resiliency, Tech Program Reg Pass</span><br /><div class="session-title">Resilience III: GPUs</div><div class="slot-entry"><a name="pap247"></a><div class="slot-title">Optimizing Software-Directed Instruction Replication for GPU Error Detection</div><div class="slot-authors">Abdulrahman Mahmoud (University of Illinois) and Siva Kumar Sastry Hari, Michael B. Sullivan, Timothy Tsai, and Stephen W. Keckler (Nvidia Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_426_1539825877_73" onclick="$('#vhsjs_view_426_1539825877_73').hide();
                $('#vhsjs_hide_426_1539825877_73').show();
                $('#425_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_426_1539825877_73" onclick="$('#425_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_426_1539825877_73').hide();
                $('#vhsjs_view_426_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="426_1539825877_73" id="425_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>Application execution on safety-critical and high-performance computer systems must be resilient to transient errors. As GPUs become more pervasive in such systems, they must supplement ECC/parity for major storage structures with reliability techniques that cover more of the GPU hardware logic.  Instruction duplication has been explored for CPU resilience; however, it has never been studied in the context of GPUs, and it is unclear whether the performance and design choices it presents makes it a feasible GPU solution. This paper describes a practical methodology to employ instruction duplication for GPUs and identifies implementation challenges that can incur high overheads (69% on average). It explores GPU-specific software optimizations that trade fine-grained recoverability for performance. It also proposes simple ISA extensions with limited hardware changes and area costs to further improve performance, cutting the runtime overheads by more than half to an average of 30%.</blockquote></div></div></div></div><a href="includes/files/pap247s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap244"></a><div class="slot-title">Fault Tolerant One-Sided Matrix Decompositions on Heterogeneous Systems with GPUs</div><div class="slot-authors">Jieyang Chen, Hongbo Li, Sihuan Li, and Xin Liang (University of California, Riverside); Panruo Wu (University of Houston); Dingwen Tao (University of Alabama); Kaiming Ouyang, Yuanlai Liu, and Kai Zhao (University of California, Riverside); Qiang Guan (Kent State University); and Zizhong Chen (University of California, Riverside)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_428_1539825877_73" onclick="$('#vhsjs_view_428_1539825877_73').hide();
                $('#vhsjs_hide_428_1539825877_73').show();
                $('#427_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_428_1539825877_73" onclick="$('#427_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_428_1539825877_73').hide();
                $('#vhsjs_view_428_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="428_1539825877_73" id="427_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>Current algorithm-based fault tolerance (ABFT) approach for one-sided matrix decomposition on heterogeneous systems with GPUs have following limitations: (1) they do not provide sufficient protection as most of them only maintain checksum in one dimension; (2) their checking scheme is not efficient due to redundant checksum verifications; (3) they fail to protect PCIe communication; (4) the checksum calculation based on a special type of matrix multiplication is far from efficient. By overcoming the above limitations, we design an efficient ABFT approach providing stronger protection for one-sided matrix decomposition methods on heterogeneous systems. First, we provide full matrix protection by using checksums in two dimensions. Second, our checking scheme is more efficient by prioritizing the checksum verification according to the sensitivity of matrix operations to soft errors. Third, we protect PCIe communication by reordering checksum verifications and decomposition steps. Fourth, we accelerate the checksum calculation by 1.7x via better utilizing GPUs.</blockquote></div></div></div></div><a href="includes/files/pap244s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap430"></a><div class="slot-title">PRISM: Predicting Resilience of GPU Applications Using Statistical Methods</div><div class="slot-authors">Charu Kalra, Fritz Previlon, and Xiangyu Li (Northeastern University); Norman Rubin (Nvidia Corporation); and David Kaeli (Northeastern University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_430_1539825877_73" onclick="$('#vhsjs_view_430_1539825877_73').hide();
                $('#vhsjs_hide_430_1539825877_73').show();
                $('#429_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_430_1539825877_73" onclick="$('#429_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_430_1539825877_73').hide();
                $('#vhsjs_view_430_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="430_1539825877_73" id="429_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>As Graphics Processing Units (GPUs) become more pervasive in HPC and safety-critical domains, ensuring that GPU applications can be protected from data corruption grows in importance. Despite prior efforts to mitigate errors, we still lack a clear understanding of how resilient these applications are in the presence of transient faults.  Due to the random nature of these faults, predicting whether they will alter the program output is a challenging problem. In this paper, we build a framework named PRISM, which uses a systematic approach to predict failures in GPU programs. PRISM extracts micro-architecture agnostic features to characterize program resiliency, which serve as predictors in our statistical model. PRISM enables us to predict failures in applications without running exhaustive fault-injection campaigns on a GPU, thereby reducing the error estimation effort. PRISM can also be used to gain insight into potential architectural support required to improve the reliability of GPU applications.</blockquote></div></div></div></div><a href="includes/files/pap430s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack133"></a><div class="section-title">Machine Learning</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Graph Algorithms, Linear Algebra, Machine Learning, Sparse Computation, Tech Program Reg Pass</span><br /><div class="session-title">Algorithms on Sparse Data</div><div class="slot-entry"><a name="pap511"></a><div class="slot-title">HiCOO: Hierarchical Storage of Sparse Tensors</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Jiajia Li, Jimeng Sun, and Richard Vuduc (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_432_1539825877_73" onclick="$('#vhsjs_view_432_1539825877_73').hide();
                $('#vhsjs_hide_432_1539825877_73').show();
                $('#431_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_432_1539825877_73" onclick="$('#431_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_432_1539825877_73').hide();
                $('#vhsjs_view_432_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="432_1539825877_73" id="431_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>This paper proposes a new storage format for sparse tensors, called Hierarchical COOrdinate (HiCOO; pronounced: “haiku”). It derives from coordinate (COO) format, arguably the de facto standard for general sparse tensor storage. HiCOO improves upon COO by compressing the indices in units of sparse tensor blocks, with the goals of preserving the “mode-agnostic” simplicity of COO while reducing the bytes needed to represent the tensor and promoting data locality. We evaluate HiCOO by implementing a single-node, multicore-parallel version of the matricized tensor-times-Khatri-Rao product (MTTKRP) operation, which is the most expensive computational core in the widely used CANDECOMP/PARAFAC decomposition(CPD) algorithm. This MTTKRP implementation achieves up to 23.0× (6.8× on average) speedup over COO format and up to 15.6× (3.1× on average) speedup over another state-of-the-art format, compressed sparse fiber (CSF), by using less or comparable storage of them. When used within CPD, we also observe speedups against COO- and CSF-based implementations.</blockquote></div></div></div></div><a href="includes/files/pap511s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap273"></a><div class="slot-title">Distributed Memory Sparse Inverse Covariance Matrix Estimation on High-Performance Computing Architectures</div><div class="slot-authors">Aryan Eftekhari (University of Lugano), Matthias Bollhöfer (Braunschweig University of Technology), and Olaf Schenk (University of Lugano)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_434_1539825877_73" onclick="$('#vhsjs_view_434_1539825877_73').hide();
                $('#vhsjs_hide_434_1539825877_73').show();
                $('#433_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_434_1539825877_73" onclick="$('#433_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_434_1539825877_73').hide();
                $('#vhsjs_view_434_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="434_1539825877_73" id="433_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>We consider the problem of estimating sparse inverse covariance matrices for high-dimensional datasets using the l1-regularized Gaussian maximum likelihood method. This task is particularly challenging as the required computational resources increase superlinearly with the dimensionality of the dataset. We introduce a performant and scalable algorithm which builds on the current advancements of second-order, maximum likelihood methods. The routine leverages the intrinsic parallelism in the linear algebra operations and exploits the underlying sparsity of the problem. The computational bottlenecks are identified and the respective subroutines are parallelized using an MPI-OpenMP approach. Experiments conducted on a Cray XC50 system at the Swiss National Supercomputing Center show that, in comparison to the state-of-the-art algorithms, the proposed routine provides significant strong scaling speedup with ideal scalability up to 128 nodes. The developed framework is used to estimate the sparse inverse covariance matrix of both synthetic and real-world datasets with up to 10 million dimensions.</blockquote></div></div></div></div><a href="includes/files/pap273s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap466"></a><div class="slot-title">PruneJuice:  Pruning Trillion-Edge Graphs to a Precise Pattern-Matching Solution</div><div class="slot-authors">Tahsin Reza, Matei Ripeanu, and Nicolas Tripoul (University of British Columbia) and Geoffrey Sanders and Roger Pearce (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_436_1539825877_73" onclick="$('#vhsjs_view_436_1539825877_73').hide();
                $('#vhsjs_hide_436_1539825877_73').show();
                $('#435_1539825877_73').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_436_1539825877_73" onclick="$('#435_1539825877_73').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_436_1539825877_73').hide();
                $('#vhsjs_view_436_1539825877_73').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="436_1539825877_73" id="435_1539825877_73" style="display: none"><div class="arrow-slidedown"><blockquote>Pattern matching is a powerful graph analysis tool. Unfortunately, existing solutions have limited scalability, support only a limited set of search patterns, and/or focus on only a subset of the real-world problems associated with pattern matching. This paper presents a new algorithmic pipeline that: (i) enables highly scalable pattern matching on labeled graphs, (ii) supports arbitrary patterns, (iii) enables trade-offs between precision and time-to-solution (while always selecting all vertices and edges that participate in matches, thus offering 100% recall), and (iv) supports a set of popular data analytics scenarios. We implement our approach on top of HavoqGT and demonstrate its advantages through strong and weak scaling experiments on massive-scale real-world (up to 257 billion edges) and synthetic (up to 4.4 trillion edges) graphs, respectively, and at scales (1,024 nodes / 36,864 cores) orders of magnitude larger than used in the past for similar problems.</blockquote></div></div></div></div><a href="includes/files/pap466s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Applications, Cosmology, Data Analytics, Deep Learning, Machine Learning, Programming Systems, Storage, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Deep Learning</div><div class="slot-entry"><a name="pap425"></a><div class="slot-title">Exploring Flexible Communications for Streamlining DNN Ensemble Training Pipelines</div><div class="slot-authors">Randall Pittman, Hui Guan, and Xipeng Shen (North Carolina State University) and Seung-Hwan Lim and Robert M. Patton (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_438_1539825877_74" onclick="$('#vhsjs_view_438_1539825877_74').hide();
                $('#vhsjs_hide_438_1539825877_74').show();
                $('#437_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_438_1539825877_74" onclick="$('#437_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_438_1539825877_74').hide();
                $('#vhsjs_view_438_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="438_1539825877_74" id="437_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>Parallel training of a Deep Neural Network (DNN) ensemble on a cluster of nodes is a common practice to train multiple models in order to construct a model with a higher prediction accuracy. Existing ensemble training pipelines can perform a great deal of redundant operations, resulting in unnecessary CPU usage, or even poor pipeline performance.  In order to remove these redundancies, we need pipelines with more communication flexibility than existing DNN frameworks provide.<br><br>This project investigates a series of designs to improve pipeline flexibility and adaptivity, while also increasing performance. We implement our designs using Tensorflow with Horovod, and test it using several large DNNs. Our results show that the CPU time spent during training is reduced by 2-11X. Furthermore, our implementation can achieve up to 10X speedups when CPU core limits are imposed. Our best pipeline also reduces the average power draw of the ensemble training process by 5-16%.</blockquote></div></div></div></div><a href="includes/files/pap425s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap429"></a><div class="slot-title">CosmoFlow: Using Deep Learning to Learn the Universe at Scale</div><div class="slot-authors">Amrita Mathuriya (Intel Corporation); Deborah Bard (National Energy Research Scientific Computing Center (NERSC), Lawrence Berkeley National Laboratory); Pete Mendygral (Cray Inc); Lawrence Meadows (Intel Corporation); James Arnemann (University of California, Berkeley); Lei Shao (Intel Corporation); Siyu He (Carnegie Mellon University); Tuomas Karna (Intel Corporation); Diana Moise (Cray Inc); Simon J. Pennycook (Intel Corporation); Kristyn Maschhoff (Cray Inc); Jason Sewall and Nalini Kumar (Intel Corporation); Shirley Ho (Lawrence Berkeley National Laboratory, Carnegie Mellon University); Michael F. Ringenburg (Cray Inc); Mr Prabhat (Lawrence Berkeley National Laboratory, National Energy Research Scientific Computing Center (NERSC)); and Victor Lee (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_440_1539825877_74" onclick="$('#vhsjs_view_440_1539825877_74').hide();
                $('#vhsjs_hide_440_1539825877_74').show();
                $('#439_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_440_1539825877_74" onclick="$('#439_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_440_1539825877_74').hide();
                $('#vhsjs_view_440_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="440_1539825877_74" id="439_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>Deep learning is a promising tool to determine the physical model that describes our universe.   To handle the considerable computational cost of this problem, we present CosmoFlow: a highly scalable deep learning application built on top of the TensorFlow framework.<br><br>CosmoFlow uses efficient implementations of 3D convolution and pooling primitives, together with improvements in threading for many element-wise operations, to improve training performance on Intel Xeon Phi processors.  We also utilize the Cray PE Machine Learning Plugin for efficient scaling to multiple nodes. We demonstrate fully synchronous data-parallel training on 8192 nodes of Cori with 77% parallel efficiency, achieving 3.5 Pflop/s sustained performance. <br><br>To our knowledge, this is the first large-scale science application of the TensorFlow framework at supercomputer scale with fully-synchronous training. These enhancements enable us to process large 3D dark matter distribution and predict the cosmological parameters Omega_M, sigma_8 and N_s with unprecedented accuracy.</blockquote></div></div></div></div><a href="includes/files/pap429s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap322"></a><div class="slot-title">Anatomy of High-Performance Deep Learning Convolutions on SIMD Architectures</div><div class="slot-authors">Evangelos Georganas, Sasikanth Avancha, Kunal Banerjee, Dhiraj Kalamkar, Greg Henry, Hans Pabst, and Alexander Heinecke (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_442_1539825877_74" onclick="$('#vhsjs_view_442_1539825877_74').hide();
                $('#vhsjs_hide_442_1539825877_74').show();
                $('#441_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_442_1539825877_74" onclick="$('#441_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_442_1539825877_74').hide();
                $('#vhsjs_view_442_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="442_1539825877_74" id="441_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>Convolution layers are prevalent in many classes of deep neural networks, including Convolutional Neural Networks (CNNs) which provide state-of-the-art results for tasks like image recognition, neural machine translation, and speech recognition. The computationally expensive nature of a convolution operation has led to the proliferation of implementations including matrix-matrix multiplication formulation, and direct convolution primarily targeting GPUs. In this paper, we introduce direct convolution kernels for x86 architectures, in particular for Xeon and Xeon Phi systems, which are implemented via a dynamic compilation approach. Our JIT-based implementation shows close to theoretical peak performance, depending on the setting and the CPU architecture at hand. We additionally demonstrate how these JIT-optimized kernels can be integrated into a light-weight multi-node graph execution model. This illustrates that single- and multi-node runs yield high efficiencies and high image-throughputs  when executing state of the art image recognition tasks on CPUs.</blockquote></div></div></div></div><a href="includes/files/pap322s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack134"></a><div class="section-title">Memory</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">GPUs, Memory, NVRAM, Performance, System Software, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Non-Volatile Memory</div><div class="slot-entry"><a name="pap203"></a><div class="slot-title">Runtime Data Management on Non-Volatile Memory-Based Heterogeneous Memory for Task-Parallel Programs</div><div class="slot-authors">Kai Wu, Jie Ren, and Dong Li (University of California, Merced)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_444_1539825877_74" onclick="$('#vhsjs_view_444_1539825877_74').hide();
                $('#vhsjs_hide_444_1539825877_74').show();
                $('#443_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_444_1539825877_74" onclick="$('#443_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_444_1539825877_74').hide();
                $('#vhsjs_view_444_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="444_1539825877_74" id="443_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>Non-volatile memory (NVM) provides a scalable solution to replace DRAM as main memory. Because of relatively high latency and low bandwidth of NVM (comparing with DRAM), NVM often pairs with DRAM to build a  heterogeneous main memory system (HMS). Deciding data placement on NVM-based HMS is critical to enable future NVM-based HPC. In this paper, we study task-parallel programs and introduce a runtime system to address the data placement problem on NVM-based HMS. Leveraging semantics and execution mode of task-parallel programs, we efficiently characterize memory access patterns of tasks and reduce data movement overhead. We also introduce a performance model to predict performance for tasks with various data placements on HMS. Evaluating with a set of HPC benchmarks, we show that our runtime system achieves higher performance than a conventional HMS-oblivious runtime (24% improvement on average) and two state-of-the-art HMS-aware solutions (16% and 11% improvement on average, respectively).</blockquote></div></div></div></div><a href="includes/files/pap203s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap194"></a><div class="slot-title">DRAGON: Breaking GPU Memory Capacity Limits with Direct NVM Access</div><div class="slot-authors">Pak Markthub (Tokyo Institute of Technology); Mehmet E. Belviranli, Seyong Lee, and Jeffrey S. Vetter (Oak Ridge National Laboratory); and Satoshi Matsuoka (RIKEN, Tokyo Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_446_1539825877_74" onclick="$('#vhsjs_view_446_1539825877_74').hide();
                $('#vhsjs_hide_446_1539825877_74').show();
                $('#445_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_446_1539825877_74" onclick="$('#445_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_446_1539825877_74').hide();
                $('#vhsjs_view_446_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="446_1539825877_74" id="445_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>Heterogeneous computing with accelerators is growing in importance in high performance computing (HPC). Recently, application datasets have expanded beyond the memory capacity of these accelerators, and often beyond the capacity of their hosts. Meanwhile, nonvolatile memory (NVM) storage has emerged as a pervasive component in HPC systems because NVM provides massive amounts of memory capacity at affordable cost. Currently, for accelerator applications to use NVM, they must manually orchestrate data movement across multiple memories and this approach only performs well for applications with simple access behaviors. To address this issue, we developed DRAGON, a solution that enables all classes of GP-GPU applications to transparently compute on terabyte datasets residing in NVM. DRAGON leverages the page-faulting mechanism on the recent NVIDIA GPUs by extending capabilities of CUDA Unified Memory (UM). Our experimental results show that DRAGON transparently expands memory capacity and obtain additional speedups via automated I/O and data transfer overlapping.</blockquote></div></div></div></div><a href="includes/files/pap194s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap355"></a><div class="slot-title">Siena: Exploring the Design Space of Heterogeneous Memory Systems</div><div class="slot-authors">Ivy B. Peng and Jeffrey S. Vetter (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_448_1539825877_74" onclick="$('#vhsjs_view_448_1539825877_74').hide();
                $('#vhsjs_hide_448_1539825877_74').show();
                $('#447_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_448_1539825877_74" onclick="$('#447_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_448_1539825877_74').hide();
                $('#vhsjs_view_448_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="448_1539825877_74" id="447_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>Memory systems are crucial to the performance, power, and cost of high-performance computing systems. Recently, multiple factors are driving the need for more complex, deep memory hierarchies. However, architects and customers are struggling to design memory systems that effectively balance multiple, often competing, factors in this large, multidimensional, and fast-moving design space. In this paper, we systematically explore the organization of heterogeneous memory systems on a framework, called Siena. Siena facilitates quick exploration of memory architectures with flexible configurations of memory systems and realistic memory workloads. We perform a design space exploration on 22 proposed memory systems using eight relevant workloads. Our results show that horizontal organizations of memories can achieve higher performance than that of vertical organizations when the distribution of memory traffic balances the performance gap between memories. However, the coupling effects through shared resources and application behaviors could negate the advantage of high-performance memory in horizontal organizations.</blockquote></div></div></div></div><a href="includes/files/pap355s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, Memory, Networks, Parallel Programming Languages, Libraries, and Models, Power, Programming Systems, Scheduling, Tech Program Reg Pass</span><br /><div class="session-title">Task-Based Programming</div><div class="slot-entry"><a name="pap490"></a><div class="slot-title">Dynamic Tracing: Memoization of Task Graphs for Dynamic Task-Based Runtimes</div><div class="slot-authors">Wonchan Lee (Stanford University), Elliott Slaughter (SLAC National Accelerator Laboratory), Michael Bauer and Sean Treichler (Nvidia Corporation), Todd Warszawski (Stanford University), Michael Garland (Nvidia Corporation), and Alex Aiken (Stanford University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_450_1539825877_74" onclick="$('#vhsjs_view_450_1539825877_74').hide();
                $('#vhsjs_hide_450_1539825877_74').show();
                $('#449_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_450_1539825877_74" onclick="$('#449_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_450_1539825877_74').hide();
                $('#vhsjs_view_450_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="450_1539825877_74" id="449_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>Many recent programming systems for both supercomputing and data center workloads generate task graphs to express computations that run on parallel and distributed machines. Due to the overhead associated with constructing these graphs the dependence analysis that generates them is often statically computed and memoized, and the resulting graph executed repeatedly at runtime. However, many applications require a dynamic dependence analysis due to data dependent behavior, but there are new challenges in capturing and re-executing task graphs at runtime. In this work, we introduce dynamic tracing, a technique to capture a dynamic dependence analysis of a trace that generates a task graph, and replay it. We show that an implementation of dynamic tracing improves strong scaling by an average of 4.9X and up to 7.0X on a suite of already optimized benchmarks.</blockquote></div></div></div></div><a href="includes/files/pap490s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap338"></a><div class="slot-title">Runtime-Assisted Cache Coherence Deactivation in Task Parallel Programs</div><div class="slot-authors">Paul Caheny (Barcelona Supercomputing Center, Polytechnic University of Catalonia); Lluc Alvarez (Barcelona Supercomputing Center); Mateo Valero and Miquel Moretó (Barcelona Supercomputing Center, Polytechnic University of Catalonia); and Marc Casas (Barcelona Supercomputing Center)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_452_1539825877_74" onclick="$('#vhsjs_view_452_1539825877_74').hide();
                $('#vhsjs_hide_452_1539825877_74').show();
                $('#451_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_452_1539825877_74" onclick="$('#451_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_452_1539825877_74').hide();
                $('#vhsjs_view_452_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="452_1539825877_74" id="451_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>With increasing core counts, the scalability of directory-based cache coherence has become a challenging problem. To reduce the area and power needs of the directory, recent proposals reduce its size by classifying data as private or shared, and disable coherence for private data. However, existing classification methods suffer from inaccuracies and require complex hardware support with limited scalability.<br><br>This paper proposes a hardware/software co-designed approach: the runtime system identifies data that is guaranteed by the programming model semantics to not require coherence and notifies the microarchitecture. The microarchitecture deactivates coherence for this private data and powers off unused directory capacity. Our proposal reduces directory accesses to just 26% of the baseline system and supports a 64× smaller directory with only 2.8% performance degradation. By dynamically calibrating the directory size, our proposal saves 86% of dynamic energy consumption in the directory without harming performance.</blockquote></div></div></div></div><a href="includes/files/pap338s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap547"></a><div class="slot-title">A Divide and Conquer Algorithm for DAG Scheduling Under Power Constraints</div><div class="slot-authors">Gökalp Demirci, Ivana Marincic, and Henry Hoffmann (University of Chicago)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_454_1539825877_74" onclick="$('#vhsjs_view_454_1539825877_74').hide();
                $('#vhsjs_hide_454_1539825877_74').show();
                $('#453_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_454_1539825877_74" onclick="$('#453_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_454_1539825877_74').hide();
                $('#vhsjs_view_454_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="454_1539825877_74" id="453_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>We consider the problem of scheduling a parallel computation–represented as a directed acyclic graph (DAG)–on a distributed parallel system with a global resource constraint–specifically a global power budget–and configurable resources, allowing a range of different power/performance tradeoffs. There is a rich body of literature on the independent problems of (1) scheduling DAGs and (2) scheduling independent applications under resource constraints. Very little, however, is known about the combined problem of scheduling DAGs under resource constraints. We present a novel approximation algorithm using a divide-and-conquer method for minimizing application execution time. We prove that the length of the schedule returned by our algorithm is always within O(log n)-factor of the optimum that can be achieved with selection of configurations for the tasks. We implement and test our algorithm on simulations of real application DAGs. We find that our divide-and-conquer method improves performance by up to 75% compared to greedy scheduling algorithms.</blockquote></div></div></div></div><a href="includes/files/pap547s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Linear Algebra, Memory, MPI, OpenMP, Programming Systems, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Programming Systems Tools</div><div class="slot-entry"><a name="pap179"></a><div class="slot-title">Dynamic Data Race Detection for OpenMP Programs</div><div class="slot-authors">Yizi Gu and John Mellor-Crummey (Rice University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_456_1539825877_74" onclick="$('#vhsjs_view_456_1539825877_74').hide();
                $('#vhsjs_hide_456_1539825877_74').show();
                $('#455_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_456_1539825877_74" onclick="$('#455_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_456_1539825877_74').hide();
                $('#vhsjs_view_456_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="456_1539825877_74" id="455_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>Two concurrent accesses to a shared variable that are unordered by synchronization are said to be a data race if at least one access is a write. Data races cause shared memory parallel programs to behave unpredictably. This paper describes ROMP -- a tool for detecting data races in executions of scalable parallel applications that employ OpenMP for node-level parallelism. The complexity of OpenMP, which includes primitives for managing data environments, SPMD and SIMD parallelism, work sharing, tasking, mutual exclusion, and ordering, presents a formidable challenge for data race detection. ROMP is a hybrid data race detector that tracks accesses, access orderings, and mutual exclusion. Unlike other OpenMP race detectors, ROMP detects races with respect to logical parallelism rather than implementation threads. Experiments show that ROMP yields precise race reports for a broader set of OpenMP constructs than prior state-of-the-art race detectors.</blockquote></div></div></div></div><a href="includes/files/pap179s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap256"></a><div class="slot-title">ParSy: Inspection and Transformation of Sparse Matrix Computations for Parallelism</div><div class="slot-authors">Kazem Cheshmi (University of Toronto), Shoaib Kamil (Adobe Research), Michelle Mills Strout (University of Arizona), and Maryam Mehri Dehnavi (University of Toronto)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_458_1539825877_74" onclick="$('#vhsjs_view_458_1539825877_74').hide();
                $('#vhsjs_hide_458_1539825877_74').show();
                $('#457_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_458_1539825877_74" onclick="$('#457_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_458_1539825877_74').hide();
                $('#vhsjs_view_458_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="458_1539825877_74" id="457_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>In this work, we describe ParSy, a framework that uses a novel inspection strategy along with a simple code transformation to optimize parallel sparse algorithms for shared memory processors. Unlike existing approaches that can suffer from load imbalance and excessive synchronization, ParSy uses a novel task coarsening strategy to create well-balanced tasks that can execute in parallel, while maintaining locality of memory accesses. Code using the ParSy inspector and transformation outperforms existing highly-optimized sparse matrix algorithms such as Cholesky factorization on multi-core processors with speedups of 2.8× and 3.1× over the MKL Pardiso and PaStiX libraries respectively.</blockquote></div></div></div></div><a href="includes/files/pap256s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap382"></a><div class="slot-title">Detecting MPI Usage Anomalies via Partial Program Symbolic Execution</div><div class="slot-authors">Fangke Ye, Jisheng Zhao, and Vivek Sarkar (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_460_1539825877_74" onclick="$('#vhsjs_view_460_1539825877_74').hide();
                $('#vhsjs_hide_460_1539825877_74').show();
                $('#459_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_460_1539825877_74" onclick="$('#459_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_460_1539825877_74').hide();
                $('#vhsjs_view_460_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="460_1539825877_74" id="459_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>MPI is a message passing based programming model for distributed-memory parallelism that has been had been widely used for programming supercomputers for over 25 years. However, debugging and verification of MPI programs is widely recognized to be a deep technical challenge. This challenge is further exacerbated by a recent increase in the use of nonblocking MPI operations that bring new classes of bugs related to data races.<br><br>In this paper, we introduce a new MPI program debugging approach based on partial symbolic execution so as to avoid the false alarms inherent in the static analysis based methodology. Compared with the dynamic approach, our approach can be applied to incomplete programs and explore multiple execution paths, thereby bringing more flexibility and precision. By comparing with well known static/dynamic tools on real-world MPI applications, our approach shows same precision as the dynamic tool and avoids false positive produced by the static tool.</blockquote></div></div></div></div><a href="includes/files/pap382s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack186"></a><div class="section-title">MPI</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, MPI, Networks, Performance, Programming Systems, State of the Practice, Tech Program Reg Pass</span><br /><div class="session-title">MPI Optimization and Characterization</div><div class="slot-entry"><a name="pap504"></a><div class="slot-title">Cooperative Rendezvous Protocols for Improved Performance and Overlap</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">S. Chakraborty, M. Bayatpour, J. Hashmi, H. Subramoni, and D. K. Panda (Ohio State University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_462_1539825877_74" onclick="$('#vhsjs_view_462_1539825877_74').hide();
                $('#vhsjs_hide_462_1539825877_74').show();
                $('#461_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_462_1539825877_74" onclick="$('#461_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_462_1539825877_74').hide();
                $('#vhsjs_view_462_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="462_1539825877_74" id="461_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>With the emergence of larger multi-/many-core clusters, performance of large message communication is becoming more important. MPI libraries use different Rendezvous protocols to perform large message communication. However, existing Rendezvous protocols do not consider the overall communication pattern and make optimal use of the Sender and the Receiver CPUs. In this work, we propose a cooperative Rendezvous protocol that can provide up to 2x improvement in intra-node bandwidth and latency for large messages. We also propose a scheme to dynamically choose the best Rendezvous protocol for each message based on the communication pattern.  Finally, we show how these improvements can increase the overlap of computation with intra-node and inter-node communication, and lead to application level benefits. We evaluate proposed designs on three different architectures including Intel Xeon, Knights Landing, and OpenPOWER with different HPC applications and obtain benefits up to 19% with Graph500, 16% with CoMD, and 10% with MiniGhost.</blockquote></div></div></div></div><a href="includes/files/pap504s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap463"></a><div class="slot-title">Framework for Scalable Intra-Node Collective Operations Using Shared Memory</div><div class="slot-authors">Surabhi Jain, Rashid Kaleem, Marc Gamell Balmana, Akhil Langer, Dmitry Durnov, Alexander Sannikov, and Maria Garzaran (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_464_1539825877_74" onclick="$('#vhsjs_view_464_1539825877_74').hide();
                $('#vhsjs_hide_464_1539825877_74').show();
                $('#463_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_464_1539825877_74" onclick="$('#463_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_464_1539825877_74').hide();
                $('#vhsjs_view_464_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="464_1539825877_74" id="463_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>Collective operations are used in MPI programs to express common communication patterns, collective computations, or synchronizations. In many collectives, such as barrier or allreduce, the intra-node component of the collective is in the critical path, as the inter-node communication cannot start until the intra-node component has been executed. Thus, with increasing number of core counts in each node, intra-node optimizations that leverage the intra-node shared memory become increasingly important.<br><br>In this paper, we focus on the performance benefit of optimizing intra-node collectives using shared memory. We optimize several collectives using the primitives in broadcast and reduce as building blocks for other collectives. A comparison of our implementation on top of MPICH shows significant performance speedups with respect to the original MPICH implementation, MVAPICH, and OpenMPI, among others.</blockquote></div></div></div></div><a href="includes/files/pap463s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap537"></a><div class="slot-title">Characterization of MPI Usage on a Production Supercomputer</div><div class="slot-authors">Sudheer Chunduri, Scott Parker, Pavan Balaji, Kevin Harms, and Kalyan Kumaran (Argonne National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_466_1539825877_74" onclick="$('#vhsjs_view_466_1539825877_74').hide();
                $('#vhsjs_hide_466_1539825877_74').show();
                $('#465_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_466_1539825877_74" onclick="$('#465_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_466_1539825877_74').hide();
                $('#vhsjs_view_466_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="466_1539825877_74" id="465_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>MPI is the most prominent programming model used in scientific computing today.  Despite it's importance, however, how scientific applications use it in production is not very well understood due to the lack of low overhead profiling tools.   We used a lightweight profiling tool, called autoperf, to log the MPI usage characteristics of production applications on a large supercomputing system (Mira) and its corresponding development system (Cetus).  Autoperf limits the amount of information that it records in order to keep the overhead to a minimum while still storing enough data to derive useful insights.  MPI usage statistics have been collected for over 100K jobs that were run within a 2-year period and are analyzed.  The analysis of this data is intended as a mechanism to provide useful insights for MPI developers and network hardware developers for their next generation of improvements, and for supercomputing center operators for their next system procurements.</blockquote></div></div></div></div><a href="includes/files/pap537s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Linear Algebra, Memory, MPI, OpenMP, Programming Systems, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Programming Systems Tools</div><div class="slot-entry"><a name="pap179"></a><div class="slot-title">Dynamic Data Race Detection for OpenMP Programs</div><div class="slot-authors">Yizi Gu and John Mellor-Crummey (Rice University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_468_1539825877_74" onclick="$('#vhsjs_view_468_1539825877_74').hide();
                $('#vhsjs_hide_468_1539825877_74').show();
                $('#467_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_468_1539825877_74" onclick="$('#467_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_468_1539825877_74').hide();
                $('#vhsjs_view_468_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="468_1539825877_74" id="467_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>Two concurrent accesses to a shared variable that are unordered by synchronization are said to be a data race if at least one access is a write. Data races cause shared memory parallel programs to behave unpredictably. This paper describes ROMP -- a tool for detecting data races in executions of scalable parallel applications that employ OpenMP for node-level parallelism. The complexity of OpenMP, which includes primitives for managing data environments, SPMD and SIMD parallelism, work sharing, tasking, mutual exclusion, and ordering, presents a formidable challenge for data race detection. ROMP is a hybrid data race detector that tracks accesses, access orderings, and mutual exclusion. Unlike other OpenMP race detectors, ROMP detects races with respect to logical parallelism rather than implementation threads. Experiments show that ROMP yields precise race reports for a broader set of OpenMP constructs than prior state-of-the-art race detectors.</blockquote></div></div></div></div><a href="includes/files/pap179s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap256"></a><div class="slot-title">ParSy: Inspection and Transformation of Sparse Matrix Computations for Parallelism</div><div class="slot-authors">Kazem Cheshmi (University of Toronto), Shoaib Kamil (Adobe Research), Michelle Mills Strout (University of Arizona), and Maryam Mehri Dehnavi (University of Toronto)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_470_1539825877_74" onclick="$('#vhsjs_view_470_1539825877_74').hide();
                $('#vhsjs_hide_470_1539825877_74').show();
                $('#469_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_470_1539825877_74" onclick="$('#469_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_470_1539825877_74').hide();
                $('#vhsjs_view_470_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="470_1539825877_74" id="469_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>In this work, we describe ParSy, a framework that uses a novel inspection strategy along with a simple code transformation to optimize parallel sparse algorithms for shared memory processors. Unlike existing approaches that can suffer from load imbalance and excessive synchronization, ParSy uses a novel task coarsening strategy to create well-balanced tasks that can execute in parallel, while maintaining locality of memory accesses. Code using the ParSy inspector and transformation outperforms existing highly-optimized sparse matrix algorithms such as Cholesky factorization on multi-core processors with speedups of 2.8× and 3.1× over the MKL Pardiso and PaStiX libraries respectively.</blockquote></div></div></div></div><a href="includes/files/pap256s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap382"></a><div class="slot-title">Detecting MPI Usage Anomalies via Partial Program Symbolic Execution</div><div class="slot-authors">Fangke Ye, Jisheng Zhao, and Vivek Sarkar (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_472_1539825877_74" onclick="$('#vhsjs_view_472_1539825877_74').hide();
                $('#vhsjs_hide_472_1539825877_74').show();
                $('#471_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_472_1539825877_74" onclick="$('#471_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_472_1539825877_74').hide();
                $('#vhsjs_view_472_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="472_1539825877_74" id="471_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>MPI is a message passing based programming model for distributed-memory parallelism that has been had been widely used for programming supercomputers for over 25 years. However, debugging and verification of MPI programs is widely recognized to be a deep technical challenge. This challenge is further exacerbated by a recent increase in the use of nonblocking MPI operations that bring new classes of bugs related to data races.<br><br>In this paper, we introduce a new MPI program debugging approach based on partial symbolic execution so as to avoid the false alarms inherent in the static analysis based methodology. Compared with the dynamic approach, our approach can be applied to incomplete programs and explore multiple execution paths, thereby bringing more flexibility and precision. By comparing with well known static/dynamic tools on real-world MPI applications, our approach shows same precision as the dynamic tool and avoids false positive produced by the static tool.</blockquote></div></div></div></div><a href="includes/files/pap382s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack135"></a><div class="section-title">NVRAM</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">GPUs, Memory, NVRAM, Performance, System Software, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Non-Volatile Memory</div><div class="slot-entry"><a name="pap203"></a><div class="slot-title">Runtime Data Management on Non-Volatile Memory-Based Heterogeneous Memory for Task-Parallel Programs</div><div class="slot-authors">Kai Wu, Jie Ren, and Dong Li (University of California, Merced)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_474_1539825877_74" onclick="$('#vhsjs_view_474_1539825877_74').hide();
                $('#vhsjs_hide_474_1539825877_74').show();
                $('#473_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_474_1539825877_74" onclick="$('#473_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_474_1539825877_74').hide();
                $('#vhsjs_view_474_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="474_1539825877_74" id="473_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>Non-volatile memory (NVM) provides a scalable solution to replace DRAM as main memory. Because of relatively high latency and low bandwidth of NVM (comparing with DRAM), NVM often pairs with DRAM to build a  heterogeneous main memory system (HMS). Deciding data placement on NVM-based HMS is critical to enable future NVM-based HPC. In this paper, we study task-parallel programs and introduce a runtime system to address the data placement problem on NVM-based HMS. Leveraging semantics and execution mode of task-parallel programs, we efficiently characterize memory access patterns of tasks and reduce data movement overhead. We also introduce a performance model to predict performance for tasks with various data placements on HMS. Evaluating with a set of HPC benchmarks, we show that our runtime system achieves higher performance than a conventional HMS-oblivious runtime (24% improvement on average) and two state-of-the-art HMS-aware solutions (16% and 11% improvement on average, respectively).</blockquote></div></div></div></div><a href="includes/files/pap203s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap194"></a><div class="slot-title">DRAGON: Breaking GPU Memory Capacity Limits with Direct NVM Access</div><div class="slot-authors">Pak Markthub (Tokyo Institute of Technology); Mehmet E. Belviranli, Seyong Lee, and Jeffrey S. Vetter (Oak Ridge National Laboratory); and Satoshi Matsuoka (RIKEN, Tokyo Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_476_1539825877_74" onclick="$('#vhsjs_view_476_1539825877_74').hide();
                $('#vhsjs_hide_476_1539825877_74').show();
                $('#475_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_476_1539825877_74" onclick="$('#475_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_476_1539825877_74').hide();
                $('#vhsjs_view_476_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="476_1539825877_74" id="475_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>Heterogeneous computing with accelerators is growing in importance in high performance computing (HPC). Recently, application datasets have expanded beyond the memory capacity of these accelerators, and often beyond the capacity of their hosts. Meanwhile, nonvolatile memory (NVM) storage has emerged as a pervasive component in HPC systems because NVM provides massive amounts of memory capacity at affordable cost. Currently, for accelerator applications to use NVM, they must manually orchestrate data movement across multiple memories and this approach only performs well for applications with simple access behaviors. To address this issue, we developed DRAGON, a solution that enables all classes of GP-GPU applications to transparently compute on terabyte datasets residing in NVM. DRAGON leverages the page-faulting mechanism on the recent NVIDIA GPUs by extending capabilities of CUDA Unified Memory (UM). Our experimental results show that DRAGON transparently expands memory capacity and obtain additional speedups via automated I/O and data transfer overlapping.</blockquote></div></div></div></div><a href="includes/files/pap194s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap355"></a><div class="slot-title">Siena: Exploring the Design Space of Heterogeneous Memory Systems</div><div class="slot-authors">Ivy B. Peng and Jeffrey S. Vetter (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_478_1539825877_74" onclick="$('#vhsjs_view_478_1539825877_74').hide();
                $('#vhsjs_hide_478_1539825877_74').show();
                $('#477_1539825877_74').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_478_1539825877_74" onclick="$('#477_1539825877_74').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_478_1539825877_74').hide();
                $('#vhsjs_view_478_1539825877_74').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="478_1539825877_74" id="477_1539825877_74" style="display: none"><div class="arrow-slidedown"><blockquote>Memory systems are crucial to the performance, power, and cost of high-performance computing systems. Recently, multiple factors are driving the need for more complex, deep memory hierarchies. However, architects and customers are struggling to design memory systems that effectively balance multiple, often competing, factors in this large, multidimensional, and fast-moving design space. In this paper, we systematically explore the organization of heterogeneous memory systems on a framework, called Siena. Siena facilitates quick exploration of memory architectures with flexible configurations of memory systems and realistic memory workloads. We perform a design space exploration on 22 proposed memory systems using eight relevant workloads. Our results show that horizontal organizations of memories can achieve higher performance than that of vertical organizations when the distribution of memory traffic balances the performance gap between memories. However, the coupling effects through shared resources and application behaviors could negate the advantage of high-performance memory in horizontal organizations.</blockquote></div></div></div></div><a href="includes/files/pap355s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack137"></a><div class="section-title">Networks</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Data Analytics, Networks, Tech Program Reg Pass</span><br /><div class="session-title">Next-Generation Networking</div><div class="slot-entry"><a name="pap147"></a><div class="slot-title">Exploiting Idle Resources in a High-Radix Switch for Supplemental Storage</div><div><span class="BP award">Best Paper Finalists</span></div><div class="slot-authors">Matthias A. Blumrich, Nan Jiang, and Larry R. Dennison (Nvidia Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_480_1539825877_75" onclick="$('#vhsjs_view_480_1539825877_75').hide();
                $('#vhsjs_hide_480_1539825877_75').show();
                $('#479_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_480_1539825877_75" onclick="$('#479_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_480_1539825877_75').hide();
                $('#vhsjs_view_480_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="480_1539825877_75" id="479_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>A general-purpose switch for a high-performance network is usually designed with symmetric ports providing credit-based flow control and error recovery via link-level retransmission. Because port buffers must be sized for the longest links and modern asymmetric network topologies have a wide range of link lengths, we observe that there can be a significant amount of unused buffer memory, particularly in edge switches. We also observe that the tiled architecture used in many high-radix switches contains an abundance of internal bandwidth. We combine these observations to create a new switch architecture that allows ports to stash packets in unused buffers on other ports, accessible via excess internal bandwidth in the tiled switch. We explore this architecture through two use cases: end-to-end resilience and congestion mitigation. We find that stashing is highly effective and does not negatively impact network performance.</blockquote></div></div></div></div><a href="includes/files/pap147s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap163"></a><div class="slot-title">Fine-Grained, Multi-Domain Network Resource Abstraction as a Fundamental Primitive to Enable High-Performance, Collaborative Data Sciences</div><div class="slot-authors">Qiao Xiang (Yale University); J. Jensen Zhang, X. Tony Wang, and Y. Jace Liu (Tongji University); Chin Guok (Lawrence Berkeley National Laboratory); Franck Le (IBM); John MacAuley (Lawrence Berkeley National Laboratory); Harvey Newman (California Institute of Technology); and Y. Richard Yang (Yale University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_482_1539825877_75" onclick="$('#vhsjs_view_482_1539825877_75').hide();
                $('#vhsjs_hide_482_1539825877_75').show();
                $('#481_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_482_1539825877_75" onclick="$('#481_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_482_1539825877_75').hide();
                $('#vhsjs_view_482_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="482_1539825877_75" id="481_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>Multi-domain network resource reservation systems are being deployed, driven by the demand and substantial benefits of providing predictable network resources. However, a major lack of existing systems is their coarse granularity, due to the participating networks’ concern of revealing sensitive information, which can result in substantial inefficiencies. This paper presents Mercator, a novel multi-domain network resource discovery system to provide fine-grained, global network resource information, for collaborative sciences. The foundation of Mercator is a resource abstraction through algebraic-expression enumeration (i.e., linear inequalities/equations), as a compact representation of the available bandwidth in multi-domain networks. In addition, we develop an obfuscating protocol, to address the privacy concerns by ensuring that no participant can associate the algebraic expressions with the corresponding member networks. We also introduce a superset projection technique to increase Mercator’s scalability. Finally, we implement Mercator and demonstrate both its efficiency and efficacy through extensive experiments using real topologies and traces.</blockquote></div></div></div></div><a href="includes/files/pap163s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap157"></a><div class="slot-title">Light-Weight Protocols for Wire-Speed Ordering</div><div class="slot-authors">Hans Eberle and Larry Dennison (Nvidia Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_484_1539825877_75" onclick="$('#vhsjs_view_484_1539825877_75').hide();
                $('#vhsjs_hide_484_1539825877_75').show();
                $('#483_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_484_1539825877_75" onclick="$('#483_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_484_1539825877_75').hide();
                $('#vhsjs_view_484_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="484_1539825877_75" id="483_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>We describe light-weight protocols for selective packet ordering in out-of-order networks that carry memory traffic. The protocols are designed for heterogeneous high-performance systems, in particular, accelerated systems with endpoints that have few resources available for interfacing the network.<br><br>The protocols preserve the semantics of a relaxed memory ordering model as adopted by highly-threaded many-core processors and accelerators.<br><br>The protocols achieve link-rate performance through the following techniques: (1) speculative connection setup avoids round-trip delays found in protocols with little knowledge about endpoint resources, (2) target-side ordering avoids round-trip delays found in source-side ordering mechanisms, (3) fine-grained ordering removes dependencies unwarranted by program code avoiding cumulative ordering dependencies caused by coarse-grained ordering, (4) ordering relaxations and optimizations for producer/consumer communication patterns.<br><br>We describe two ordering protocols that provide (1) strict sequential ordering and (2) relaxed ordering for multi-packet transfers. The protocols impose no restrictions on routing, including multipath routing.</blockquote></div></div></div></div><a href="includes/files/pap157s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, Data Analytics, Deep Learning, Networks, Scientific Computing, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Large-Scale Algorithms</div><div class="slot-entry"><a name="pap171"></a><div class="slot-title">Large-Scale Hierarchical K-Means for Heterogeneous Many-Core Supercomputers</div><div class="slot-authors">Liandeng Li (Tsinghua University; National Supercomputing Center, Wuxi); Teng Yu (University of St Andrews); Wenlai Zhao and Haohuan Fu (Tsinghua University; National Supercomputing Center, Wuxi); Chenyu Wang (University of St Andrews; National Supercomputing Center, Wuxi); Li Tan (Beijing Technology and Business University); Guangwen Yang (Tsinghua University; National Supercomputing Center, Wuxi); and John Thomson (University of St Andrews)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_486_1539825877_75" onclick="$('#vhsjs_view_486_1539825877_75').hide();
                $('#vhsjs_hide_486_1539825877_75').show();
                $('#485_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_486_1539825877_75" onclick="$('#485_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_486_1539825877_75').hide();
                $('#vhsjs_view_486_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="486_1539825877_75" id="485_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>This paper presents a novel design and implementation of k-means clustering algorithm targeting the Sunway TaihuLight supercomputer. We introduce a multi-level parallel partition approach that not only partitions by dataflow and centroid, but also by dimension. Our multi-level (nkd) approach unlocks the potential of the hierarchical parallelism in the SW26010 heterogeneous many-core processor and the system architecture of the supercomputer. <br><br>Our design is able to process large-scale clustering problems with up to 196,608 dimensions and over 160,000 targeting centroids, while maintaining high performance and high scalability, significantly improving the capability of k-means over previous approaches. The evaluation shows our implementation achieves performance of less than 18 seconds per iteration for a large-scale clustering case with 196,608 data dimensions and 2,000 centroids by applying 4,096 nodes (1,064,496 cores) in parallel, making k-means a more feasible solution for complex scenarios.</blockquote></div></div></div></div><a href="includes/files/pap171s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap140"></a><div class="slot-title">TriCore: Parallel Triangle Counting on GPUs</div><div class="slot-authors">Yang Hu (George Washington University); Hang Liu (University of Massachusetts, Lowell); and H. Howie Huang (George Washington University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_488_1539825877_75" onclick="$('#vhsjs_view_488_1539825877_75').hide();
                $('#vhsjs_hide_488_1539825877_75').show();
                $('#487_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_488_1539825877_75" onclick="$('#487_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_488_1539825877_75').hide();
                $('#vhsjs_view_488_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="488_1539825877_75" id="487_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>Triangle counting algorithm enumerates the triangles in a graph by identifying the common neighbors between two vertices of every edge. In this work, we present TriCore, a new GPU-based high-performance and scalable triangle counting system that consists of three main techniques. First, we design a binary search based counting algorithm that tremendously increases both thread parallelism and memory performance. Second, TriCore exploits a 2-D partition method to distribute the CSR representation across multiple GPUs, combined with a new streaming buffer to load the edge list from outside of GPUs. Third, we develop a dynamic workload management technique to balance the workload across multiple GPUs. Our evaluation demonstrates TriCore is 22× faster than the state-of-the-art parallel triangle counting projects. In addition, TriCore can not only process big graphs that are significant larger than the memory size of one GPU but also achieve 24× speedup when scaling to 32 GPUs.</blockquote></div></div></div></div><a href="includes/files/pap140s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap141"></a><div class="slot-title">Distributed-Memory Hierarchical Compression of Dense SPD Matrices</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Chenhan D. Yu (University of Texas), Severin Reiz (Technical University Munich), and George Biros (University of Texas)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_490_1539825877_75" onclick="$('#vhsjs_view_490_1539825877_75').hide();
                $('#vhsjs_hide_490_1539825877_75').show();
                $('#489_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_490_1539825877_75" onclick="$('#489_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_490_1539825877_75').hide();
                $('#vhsjs_view_490_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="490_1539825877_75" id="489_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>We present a distributed-memory algorithm for the hierarchical compression of SPD matrices. Our method is based on GOFMM, an algorithm that appeared in doi:10.1145/3126908.3126921.<br><br>For many SPD matrices, GOFMM enables compression and approximate matrix-vector multiplication in NlogN time---as opposed to quadratic work required for a dense matrix. But GOFMM supports only shared memory parallelism. In this paper, we use the message passing interface, extending the ideas of GOFMM to the distributed memory setting. We also introduce an asynchronous algorithm for faster multiplication. We present different usage scenarios of SPD matrices that are related to graphs, neural-networks, and covariance operators. We also compare with STRUMPACK, which, to our knowledge, is the only other parallel software that can compress arbitrary SPD matrices. In our largest run, we were able to compress a 67M-by-67M matrix within three minutes and perform a multiplication with 512 vectors within 5 seconds on 6,144 Intel Skylake cores.</blockquote></div></div></div></div><a href="includes/files/pap141s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Networks, Resource Management, Scheduling, State of the Practice, System Software, Tech Program Reg Pass</span><br /><div class="session-title">Resource Management and Interference</div><div class="slot-entry"><a name="pap360"></a><div class="slot-title">RM-Replay: A High-Fidelity Tuning, Optimization and Exploration Tool for Resource Management</div><div class="slot-authors">Maxime Martinasso, Miguel Gila, Mauro Bianco, Sadaf R. Alam, Colin McMurtrie, and Thomas C. Schulthess (Swiss National Supercomputing Centre)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_492_1539825877_75" onclick="$('#vhsjs_view_492_1539825877_75').hide();
                $('#vhsjs_hide_492_1539825877_75').show();
                $('#491_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_492_1539825877_75" onclick="$('#491_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_492_1539825877_75').hide();
                $('#vhsjs_view_492_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="492_1539825877_75" id="491_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>Leading hybrid and heterogeneous supercomputing systems process hundreds of thousands of jobs using complex scheduling algorithms and parameters. The centers operating these systems aim to achieve higher levels of resource utilization while being restricted by compliance with policy constraints. There is a critical need for a high-fidelity, high-performance tool with familiar interfaces that allows not only tuning and optimization of the operational job scheduler but also enables exploration of new resource management algorithms. We propose a new methodology and a tool called RM-Replay which is not a simulator but instead a fast replay engine for production workloads. Slurm is used as a platform to demonstrate the capabilities of our replay engine.<br><br>The tool accuracy is discussed and our investigation shows that, by providing better job runtime estimation or using topology-aware allocation, scheduling metric values vary. The presented methodology to create fast replay engines can be extended to other complex systems.</blockquote></div></div></div></div><a href="includes/files/pap360s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap541"></a><div class="slot-title">Evaluation of an Interference-Free Node Allocation Policy on Fat-Tree Clusters</div><div class="slot-authors">Samuel D. Pollard (University of Oregon) and Nikhil Jain, Stephen Herbein, and Abhinav Bhatele (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_494_1539825877_75" onclick="$('#vhsjs_view_494_1539825877_75').hide();
                $('#vhsjs_hide_494_1539825877_75').show();
                $('#493_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_494_1539825877_75" onclick="$('#493_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_494_1539825877_75').hide();
                $('#vhsjs_view_494_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="494_1539825877_75" id="493_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>Interference between jobs competing for network bandwidth on a fat-tree cluster can cause significant variability and degradation in performance. These performance issues can be mitigated or completely eliminated if the resource allocation policy takes the network topology into account when allocating nodes to jobs. We implement a fat-tree network topology aware node allocation policy that allocates isolated partitions to jobs in order to eliminate inter-job interference. We compare the impact of this node allocation policy to a topology-oblivious policy with respect to the execution time of individual jobs with different communication patterns. We also evaluate the cluster's quality of service using metrics such as system utilization, schedule makespan, and job wait time for both policies. The results obtained for production workloads indicate that a topology-aware node allocation can provide interference-free execution without negatively impacting the cluster's quality of service.</blockquote></div></div></div></div><a href="includes/files/pap541s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap311"></a><div class="slot-title">Mitigating Inter-Job Interference Using Adaptive Flow-Aware Routing</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Staci A. Smith, Clara E. Cromey, and David K. Lowenthal (University of Arizona); Jens Domke (Tokyo Institute of Technology); and Nikhil Jain, Jayaraman J. Thiagarajan, and Abhinav Bhatele (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_496_1539825877_75" onclick="$('#vhsjs_view_496_1539825877_75').hide();
                $('#vhsjs_hide_496_1539825877_75').show();
                $('#495_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_496_1539825877_75" onclick="$('#495_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_496_1539825877_75').hide();
                $('#vhsjs_view_496_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="496_1539825877_75" id="495_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>On most high performance computing platforms, applications share network resources with other jobs running concurrently on the system.  Inter-job network interference can have a significant impact on the performance of communication-intensive applications, and no satisfactory solutions yet exist for mitigating this degradation.<br><br>In this paper, we analyze network congestion caused by multi-job workloads on two production systems that use popular network topologies---fat-tree and dragonfly. For each system, we establish a regression model to relate network hotspots to application performance degradation, showing that current routing strategies are insufficient to load-balance network traffic and mitigate interference on production systems.  We then propose an alternative type of adaptive routing strategy, which we call adaptive flow-aware routing.  We implement a prototype of our strategy, and tests on the fat-tree system show up to a 46% improvement in job run time when compared to the default routing.</blockquote></div></div></div></div><a href="includes/files/pap311s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, MPI, Networks, Performance, Programming Systems, State of the Practice, Tech Program Reg Pass</span><br /><div class="session-title">MPI Optimization and Characterization</div><div class="slot-entry"><a name="pap504"></a><div class="slot-title">Cooperative Rendezvous Protocols for Improved Performance and Overlap</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">S. Chakraborty, M. Bayatpour, J. Hashmi, H. Subramoni, and D. K. Panda (Ohio State University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_498_1539825877_75" onclick="$('#vhsjs_view_498_1539825877_75').hide();
                $('#vhsjs_hide_498_1539825877_75').show();
                $('#497_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_498_1539825877_75" onclick="$('#497_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_498_1539825877_75').hide();
                $('#vhsjs_view_498_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="498_1539825877_75" id="497_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>With the emergence of larger multi-/many-core clusters, performance of large message communication is becoming more important. MPI libraries use different Rendezvous protocols to perform large message communication. However, existing Rendezvous protocols do not consider the overall communication pattern and make optimal use of the Sender and the Receiver CPUs. In this work, we propose a cooperative Rendezvous protocol that can provide up to 2x improvement in intra-node bandwidth and latency for large messages. We also propose a scheme to dynamically choose the best Rendezvous protocol for each message based on the communication pattern.  Finally, we show how these improvements can increase the overlap of computation with intra-node and inter-node communication, and lead to application level benefits. We evaluate proposed designs on three different architectures including Intel Xeon, Knights Landing, and OpenPOWER with different HPC applications and obtain benefits up to 19% with Graph500, 16% with CoMD, and 10% with MiniGhost.</blockquote></div></div></div></div><a href="includes/files/pap504s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap463"></a><div class="slot-title">Framework for Scalable Intra-Node Collective Operations Using Shared Memory</div><div class="slot-authors">Surabhi Jain, Rashid Kaleem, Marc Gamell Balmana, Akhil Langer, Dmitry Durnov, Alexander Sannikov, and Maria Garzaran (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_500_1539825877_75" onclick="$('#vhsjs_view_500_1539825877_75').hide();
                $('#vhsjs_hide_500_1539825877_75').show();
                $('#499_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_500_1539825877_75" onclick="$('#499_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_500_1539825877_75').hide();
                $('#vhsjs_view_500_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="500_1539825877_75" id="499_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>Collective operations are used in MPI programs to express common communication patterns, collective computations, or synchronizations. In many collectives, such as barrier or allreduce, the intra-node component of the collective is in the critical path, as the inter-node communication cannot start until the intra-node component has been executed. Thus, with increasing number of core counts in each node, intra-node optimizations that leverage the intra-node shared memory become increasingly important.<br><br>In this paper, we focus on the performance benefit of optimizing intra-node collectives using shared memory. We optimize several collectives using the primitives in broadcast and reduce as building blocks for other collectives. A comparison of our implementation on top of MPICH shows significant performance speedups with respect to the original MPICH implementation, MVAPICH, and OpenMPI, among others.</blockquote></div></div></div></div><a href="includes/files/pap463s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap537"></a><div class="slot-title">Characterization of MPI Usage on a Production Supercomputer</div><div class="slot-authors">Sudheer Chunduri, Scott Parker, Pavan Balaji, Kevin Harms, and Kalyan Kumaran (Argonne National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_502_1539825877_75" onclick="$('#vhsjs_view_502_1539825877_75').hide();
                $('#vhsjs_hide_502_1539825877_75').show();
                $('#501_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_502_1539825877_75" onclick="$('#501_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_502_1539825877_75').hide();
                $('#vhsjs_view_502_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="502_1539825877_75" id="501_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>MPI is the most prominent programming model used in scientific computing today.  Despite it's importance, however, how scientific applications use it in production is not very well understood due to the lack of low overhead profiling tools.   We used a lightweight profiling tool, called autoperf, to log the MPI usage characteristics of production applications on a large supercomputing system (Mira) and its corresponding development system (Cetus).  Autoperf limits the amount of information that it records in order to keep the overhead to a minimum while still storing enough data to derive useful insights.  MPI usage statistics have been collected for over 100K jobs that were run within a 2-year period and are analyzed.  The analysis of this data is intended as a mechanism to provide useful insights for MPI developers and network hardware developers for their next generation of improvements, and for supercomputing center operators for their next system procurements.</blockquote></div></div></div></div><a href="includes/files/pap537s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, Memory, Networks, Parallel Programming Languages, Libraries, and Models, Power, Programming Systems, Scheduling, Tech Program Reg Pass</span><br /><div class="session-title">Task-Based Programming</div><div class="slot-entry"><a name="pap490"></a><div class="slot-title">Dynamic Tracing: Memoization of Task Graphs for Dynamic Task-Based Runtimes</div><div class="slot-authors">Wonchan Lee (Stanford University), Elliott Slaughter (SLAC National Accelerator Laboratory), Michael Bauer and Sean Treichler (Nvidia Corporation), Todd Warszawski (Stanford University), Michael Garland (Nvidia Corporation), and Alex Aiken (Stanford University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_504_1539825877_75" onclick="$('#vhsjs_view_504_1539825877_75').hide();
                $('#vhsjs_hide_504_1539825877_75').show();
                $('#503_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_504_1539825877_75" onclick="$('#503_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_504_1539825877_75').hide();
                $('#vhsjs_view_504_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="504_1539825877_75" id="503_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>Many recent programming systems for both supercomputing and data center workloads generate task graphs to express computations that run on parallel and distributed machines. Due to the overhead associated with constructing these graphs the dependence analysis that generates them is often statically computed and memoized, and the resulting graph executed repeatedly at runtime. However, many applications require a dynamic dependence analysis due to data dependent behavior, but there are new challenges in capturing and re-executing task graphs at runtime. In this work, we introduce dynamic tracing, a technique to capture a dynamic dependence analysis of a trace that generates a task graph, and replay it. We show that an implementation of dynamic tracing improves strong scaling by an average of 4.9X and up to 7.0X on a suite of already optimized benchmarks.</blockquote></div></div></div></div><a href="includes/files/pap490s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap338"></a><div class="slot-title">Runtime-Assisted Cache Coherence Deactivation in Task Parallel Programs</div><div class="slot-authors">Paul Caheny (Barcelona Supercomputing Center, Polytechnic University of Catalonia); Lluc Alvarez (Barcelona Supercomputing Center); Mateo Valero and Miquel Moretó (Barcelona Supercomputing Center, Polytechnic University of Catalonia); and Marc Casas (Barcelona Supercomputing Center)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_506_1539825877_75" onclick="$('#vhsjs_view_506_1539825877_75').hide();
                $('#vhsjs_hide_506_1539825877_75').show();
                $('#505_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_506_1539825877_75" onclick="$('#505_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_506_1539825877_75').hide();
                $('#vhsjs_view_506_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="506_1539825877_75" id="505_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>With increasing core counts, the scalability of directory-based cache coherence has become a challenging problem. To reduce the area and power needs of the directory, recent proposals reduce its size by classifying data as private or shared, and disable coherence for private data. However, existing classification methods suffer from inaccuracies and require complex hardware support with limited scalability.<br><br>This paper proposes a hardware/software co-designed approach: the runtime system identifies data that is guaranteed by the programming model semantics to not require coherence and notifies the microarchitecture. The microarchitecture deactivates coherence for this private data and powers off unused directory capacity. Our proposal reduces directory accesses to just 26% of the baseline system and supports a 64× smaller directory with only 2.8% performance degradation. By dynamically calibrating the directory size, our proposal saves 86% of dynamic energy consumption in the directory without harming performance.</blockquote></div></div></div></div><a href="includes/files/pap338s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap547"></a><div class="slot-title">A Divide and Conquer Algorithm for DAG Scheduling Under Power Constraints</div><div class="slot-authors">Gökalp Demirci, Ivana Marincic, and Henry Hoffmann (University of Chicago)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_508_1539825877_75" onclick="$('#vhsjs_view_508_1539825877_75').hide();
                $('#vhsjs_hide_508_1539825877_75').show();
                $('#507_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_508_1539825877_75" onclick="$('#507_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_508_1539825877_75').hide();
                $('#vhsjs_view_508_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="508_1539825877_75" id="507_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>We consider the problem of scheduling a parallel computation–represented as a directed acyclic graph (DAG)–on a distributed parallel system with a global resource constraint–specifically a global power budget–and configurable resources, allowing a range of different power/performance tradeoffs. There is a rich body of literature on the independent problems of (1) scheduling DAGs and (2) scheduling independent applications under resource constraints. Very little, however, is known about the combined problem of scheduling DAGs under resource constraints. We present a novel approximation algorithm using a divide-and-conquer method for minimizing application execution time. We prove that the length of the schedule returned by our algorithm is always within O(log n)-factor of the optimum that can be achieved with selection of configurations for the tasks. We implement and test our algorithm on simulations of real application DAGs. We find that our divide-and-conquer method improves performance by up to 75% compared to greedy scheduling algorithms.</blockquote></div></div></div></div><a href="includes/files/pap547s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Networks, Performance, Scientific Computing, State of the Practice, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Large Scale System Deployments</div><div class="slot-entry"><a name="pap277"></a><div class="slot-title">The Design, Deployment, and Evaluation of the CORAL Pre-Exascale Systems</div><div class="slot-authors">Sudharshan S. Vazhkudai (Oak Ridge National Laboratory); Bronis R. de Supinski (Lawrence Livermore National Laboratory); Arthur S. Bland and Al Geist (Oak Ridge National Laboratory); James Sexton and Jim Kahle (IBM); Christopher J. Zimmer, Scott Atchley, Sarp H. Oral, Don E. Maxwell, and Veronica G. Vergara Larrea (Oak Ridge National Laboratory); Adam Bertsch and Robin Goldstone (Lawrence Livermore National Laboratory); Wayne Joubert (Oak Ridge National Laboratory); Chris Chambreau (Lawrence Livermore National Laboratory); David Appelhans and Robert Blackmore (IBM); Ben Casses (Lawrence Livermore National Laboratory); George Chochia and Gene Davison (IBM); Matthew A. Ezell (Oak Ridge National Laboratory); Tom Gooding (IBM); Elsa Gonsiorowski (Lawrence Livermore National Laboratory); Leopold Grinberg, Bill Hanson, and Bill Hartner (IBM); Ian Karlin and Matthew L. Leininger (Lawrence Livermore National Laboratory); Dustin Leverman (Oak Ridge National Laboratory); Chris Marroquin (IBM); Adam Moody (Lawrence Livermore National Laboratory); Martin Ohmacht (IBM); Ramesh Pankajakshan (Lawrence Livermore National Laboratory); Fernando Pizzano (IBM); James H. Rogers (Oak Ridge National Laboratory); Bryan Rosenburg (IBM); Drew Schmidt, Mallikarjun Shankar, and Feiyi Wang (Oak Ridge National Laboratory); Py Watson (Lawrence Livermore National Laboratory); Bob Walkup (IBM); Lance D. Weems (Lawrence Livermore National Laboratory); and Junqi Yin (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_510_1539825877_75" onclick="$('#vhsjs_view_510_1539825877_75').hide();
                $('#vhsjs_hide_510_1539825877_75').show();
                $('#509_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_510_1539825877_75" onclick="$('#509_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_510_1539825877_75').hide();
                $('#vhsjs_view_510_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="510_1539825877_75" id="509_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>CORAL, the Collaboration of Oak Ridge, Argonne and Livermore, is fielding two similar IBM systems, Summit and Sierra, with NVIDIA GPUs that will replace the existing Titan and Sequoia systems. Summit and Sierra are currently ranked No. 1 and No. 3, respectively, on the Top500 list. We discuss the design and key differences of the systems. Our evaluation of the systems highlights the following. Applications that fit in HBM see the most benefit and may prefer more GPUs; however, for some applications, the CPU-GPU bandwidth is more important than the number of GPUs. The node-local burst buffer scales linearly, and can achieve a 4X improvement over the parallel file system for large jobs; smaller jobs, however, may benefit from writing directly to the PFS. Finally, several CPU, network and memory bound analytics and GPU-bound deep learning codes achieve up to a 11X and 79X speedup/node, respectively over Titan.</blockquote></div></div></div></div><a href="includes/files/pap277s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap449"></a><div class="slot-title">Best Practices and Lessons from Deploying and Operating a Sustained-Petascale System: The Blue Waters Experience</div><div class="slot-authors">Gregory H. Bauer, Brett Bode, Jeremy Enos, William T. Kramer, Scott Lathrop, Celso L. Mendes, and Roberto R. Sisneros (University of Illinois, National Center for Supercomputing Applications)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_512_1539825877_75" onclick="$('#vhsjs_view_512_1539825877_75').hide();
                $('#vhsjs_hide_512_1539825877_75').show();
                $('#511_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_512_1539825877_75" onclick="$('#511_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_512_1539825877_75').hide();
                $('#vhsjs_view_512_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="512_1539825877_75" id="511_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>Building and operating versatile extreme-scale computing systems that work productively for a range of frontier research domains present many challenges and opportunities. Solutions created, experiences acquired, and lessons learned, while rarely published, could drive the development of new methods and practices and raise the bar for all organizations supporting research, scholarship, and education. This paper describes the methods and procedures developed for deploying, supporting, and continuously improving the Blue Waters system and its services during the last five years. Being the first US sustained-petascale computing platform available to the open-science community, the Blue Waters project pioneered various unique practices that we are sharing to be adopted and further improved by the community. We present our support and service methodologies, and the leadership practices employed for ensuring that the system stays highly efficient and productive. We also provide the return on investment summaries related to deploying and operating the system.</blockquote></div></div></div></div><a href="includes/files/pap449s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap346"></a><div class="slot-title">Performance Evaluation of a Vector Supercomputer SX-Aurora TSUBASA</div><div class="slot-authors">Kazuhiko Komatsu (Tohoku University); Shintaro Momose, Yoko Isobe, Osamu Watanabe, and Akihiro Musa (Tohoku University, NEC Corporation); Mitsuo Yokokawa (Kobe University, NEC Corporation); Toshikazu Aoyama (NEC Corporation); and Masayuki Sato and Hiroaki Kobayashi (Tohoku University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_514_1539825877_75" onclick="$('#vhsjs_view_514_1539825877_75').hide();
                $('#vhsjs_hide_514_1539825877_75').show();
                $('#513_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_514_1539825877_75" onclick="$('#513_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_514_1539825877_75').hide();
                $('#vhsjs_view_514_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="514_1539825877_75" id="513_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>A new SX-Aurora TSUBASA vector supercomputer has been released with a new system architecture and a new execution model to achieve high sustained performance, especially for memory-intensive applications. In SX-Aurora TSUBASA, the vector host (VH) of a standard x86 Linux node is attached to the vector engine (VE) of a newly developed vector processor.  An application is executed on the VE, and only system calls are offloaded to the VH. This new execution model can avoid redundant data transfers between a VH and a VE that can easily become a bottleneck in the conventional execution model. This paper examines the potential of SX-Aurora TSUBASA. First, the basic performance of SX-Aurora TSUBASA is clarified by evaluating benchmark programs. Then, the effectiveness of the new execution model is examined by using a microbenchmark.  Finally, the high potential of SX-Aurora TSUBASA is clarified through evaluations of practical applications.</blockquote></div></div></div></div><a href="includes/files/pap346s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, GPUs, Linear Algebra, Networks, Resiliency, Tech Program Reg Pass</span><br /><div class="session-title">Resilience III: GPUs</div><div class="slot-entry"><a name="pap247"></a><div class="slot-title">Optimizing Software-Directed Instruction Replication for GPU Error Detection</div><div class="slot-authors">Abdulrahman Mahmoud (University of Illinois) and Siva Kumar Sastry Hari, Michael B. Sullivan, Timothy Tsai, and Stephen W. Keckler (Nvidia Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_516_1539825877_75" onclick="$('#vhsjs_view_516_1539825877_75').hide();
                $('#vhsjs_hide_516_1539825877_75').show();
                $('#515_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_516_1539825877_75" onclick="$('#515_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_516_1539825877_75').hide();
                $('#vhsjs_view_516_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="516_1539825877_75" id="515_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>Application execution on safety-critical and high-performance computer systems must be resilient to transient errors. As GPUs become more pervasive in such systems, they must supplement ECC/parity for major storage structures with reliability techniques that cover more of the GPU hardware logic.  Instruction duplication has been explored for CPU resilience; however, it has never been studied in the context of GPUs, and it is unclear whether the performance and design choices it presents makes it a feasible GPU solution. This paper describes a practical methodology to employ instruction duplication for GPUs and identifies implementation challenges that can incur high overheads (69% on average). It explores GPU-specific software optimizations that trade fine-grained recoverability for performance. It also proposes simple ISA extensions with limited hardware changes and area costs to further improve performance, cutting the runtime overheads by more than half to an average of 30%.</blockquote></div></div></div></div><a href="includes/files/pap247s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap244"></a><div class="slot-title">Fault Tolerant One-Sided Matrix Decompositions on Heterogeneous Systems with GPUs</div><div class="slot-authors">Jieyang Chen, Hongbo Li, Sihuan Li, and Xin Liang (University of California, Riverside); Panruo Wu (University of Houston); Dingwen Tao (University of Alabama); Kaiming Ouyang, Yuanlai Liu, and Kai Zhao (University of California, Riverside); Qiang Guan (Kent State University); and Zizhong Chen (University of California, Riverside)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_518_1539825877_75" onclick="$('#vhsjs_view_518_1539825877_75').hide();
                $('#vhsjs_hide_518_1539825877_75').show();
                $('#517_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_518_1539825877_75" onclick="$('#517_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_518_1539825877_75').hide();
                $('#vhsjs_view_518_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="518_1539825877_75" id="517_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>Current algorithm-based fault tolerance (ABFT) approach for one-sided matrix decomposition on heterogeneous systems with GPUs have following limitations: (1) they do not provide sufficient protection as most of them only maintain checksum in one dimension; (2) their checking scheme is not efficient due to redundant checksum verifications; (3) they fail to protect PCIe communication; (4) the checksum calculation based on a special type of matrix multiplication is far from efficient. By overcoming the above limitations, we design an efficient ABFT approach providing stronger protection for one-sided matrix decomposition methods on heterogeneous systems. First, we provide full matrix protection by using checksums in two dimensions. Second, our checking scheme is more efficient by prioritizing the checksum verification according to the sensitivity of matrix operations to soft errors. Third, we protect PCIe communication by reordering checksum verifications and decomposition steps. Fourth, we accelerate the checksum calculation by 1.7x via better utilizing GPUs.</blockquote></div></div></div></div><a href="includes/files/pap244s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap430"></a><div class="slot-title">PRISM: Predicting Resilience of GPU Applications Using Statistical Methods</div><div class="slot-authors">Charu Kalra, Fritz Previlon, and Xiangyu Li (Northeastern University); Norman Rubin (Nvidia Corporation); and David Kaeli (Northeastern University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_520_1539825877_75" onclick="$('#vhsjs_view_520_1539825877_75').hide();
                $('#vhsjs_hide_520_1539825877_75').show();
                $('#519_1539825877_75').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_520_1539825877_75" onclick="$('#519_1539825877_75').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_520_1539825877_75').hide();
                $('#vhsjs_view_520_1539825877_75').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="520_1539825877_75" id="519_1539825877_75" style="display: none"><div class="arrow-slidedown"><blockquote>As Graphics Processing Units (GPUs) become more pervasive in HPC and safety-critical domains, ensuring that GPU applications can be protected from data corruption grows in importance. Despite prior efforts to mitigate errors, we still lack a clear understanding of how resilient these applications are in the presence of transient faults.  Due to the random nature of these faults, predicting whether they will alter the program output is a challenging problem. In this paper, we build a framework named PRISM, which uses a systematic approach to predict failures in GPU programs. PRISM extracts micro-architecture agnostic features to characterize program resiliency, which serve as predictors in our statistical model. PRISM enables us to predict failures in applications without running exhaustive fault-injection campaigns on a GPU, thereby reducing the error estimation effort. PRISM can also be used to gain insight into potential architectural support required to improve the reliability of GPU applications.</blockquote></div></div></div></div><a href="includes/files/pap430s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Data Management, File Systems, Networks, State of the Practice, System Software, Workflows, Tech Program Reg Pass</span><br /><div class="session-title">File Systems: Data Movement and Provenance</div><div class="slot-entry"><a name="pap407"></a><div class="slot-title">Dac-Man: Data Change Management for Scientific Datasets on HPC Systems</div><div class="slot-authors">Devarshi Ghoshal, Lavanya Ramakrishnan, and Deborah Agarwal (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_522_1539825877_76" onclick="$('#vhsjs_view_522_1539825877_76').hide();
                $('#vhsjs_hide_522_1539825877_76').show();
                $('#521_1539825877_76').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_522_1539825877_76" onclick="$('#521_1539825877_76').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_522_1539825877_76').hide();
                $('#vhsjs_view_522_1539825877_76').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="522_1539825877_76" id="521_1539825877_76" style="display: none"><div class="arrow-slidedown"><blockquote>Scientific data is growing rapidly and often changes due to instrument configurations, software updates, or quality assessments. These changes in datasets can result in significant waste of compute and storage resources on HPC systems as downstream pipelines are reprocessed. Data changes need to be detected, tracked, and analyzed for understanding the impact of data change, managing data provenance, and making efficient and effective decisions about reprocessing and use of HPC resources. Existing methods for identifying and capturing change are often manual, domain-specific, and error-prone and do not scale to large scientific datasets. In this paper, we describe the design and implementation of Dac-Man framework, which identifies, captures, and manages change in large scientific datasets, and enables plug-in of domain-specific change analysis with minimal user effort. Our evaluations show that it can retrieve file changes from directories containing millions of files and terabytes of data in less than a minute.</blockquote></div></div></div></div><a href="includes/files/pap407s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap521"></a><div class="slot-title">Stacker: An Autonomic Data Movement Engine for Extreme-Scale Data Staging-Based In Situ Workflows</div><div class="slot-authors">Pradeep Subedi, Philip Davis, and Shaohua Duan (Rutgers University); Scott Klasky (Oak Ridge National Laboratory); Hemanth Kolla (Sandia National Laboratories); and Manish Parashar (Rutgers University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_524_1539825877_76" onclick="$('#vhsjs_view_524_1539825877_76').hide();
                $('#vhsjs_hide_524_1539825877_76').show();
                $('#523_1539825877_76').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_524_1539825877_76" onclick="$('#523_1539825877_76').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_524_1539825877_76').hide();
                $('#vhsjs_view_524_1539825877_76').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="524_1539825877_76" id="523_1539825877_76" style="display: none"><div class="arrow-slidedown"><blockquote>Data staging and in situ workflows are being explored extensively as an approach to address data-related costs at very large scales. However, the impact of emerging storage architectures (e.g., deep memory hierarchies and burst buffers) upon data staging solutions remains a challenge. In this paper, we investigate how burst buffers can be effectively used by data staging solutions, for example, as a persistence storage tier of the memory hierarchy. Furthermore, we use machine learning based prefetching techniques to move data between the storage levels in an autonomous manner. We also present Stacker, a prototype of the proposed solutions implemented within the Data\-Spaces data staging service, and experimentally evaluate its performance and scalability using the S3D combustion workflow on current leadership class platforms. Our experiments demonstrate that Stacker achieves low latency, high volume data-staging with low overhead as compared to in-memory staging services for production scientific workflows.</blockquote></div></div></div></div><a href="includes/files/pap521s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap206"></a><div class="slot-title">A Year in the Life of a Parallel File System</div><div class="slot-authors">Glenn K. Lockwood (Lawrence Berkeley National Laboratory), Shane Snyder (Argonne National Laboratory), Teng Wang and Suren Byna (Lawrence Berkeley National Laboratory), Philip Carns (Argonne National Laboratory), and Nicholas J. Wright (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_526_1539825877_76" onclick="$('#vhsjs_view_526_1539825877_76').hide();
                $('#vhsjs_hide_526_1539825877_76').show();
                $('#525_1539825877_76').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_526_1539825877_76" onclick="$('#525_1539825877_76').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_526_1539825877_76').hide();
                $('#vhsjs_view_526_1539825877_76').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="526_1539825877_76" id="525_1539825877_76" style="display: none"><div class="arrow-slidedown"><blockquote>I/O performance is a critical aspect of data-intensive scientific computing.  We seek to advance the state of the practice in understanding and diagnosing I/O performance issues through investigation of a comprehensive I/O performance data set that captures a full year of production storage activity at two leadership-scale computing facilities.  We demonstrate techniques to identify regions of interest, perform focused investigations of both long-term trends and transient anomalies, and uncover the contributing factors that lead to performance fluctuation.<br><br>We find that a year in the life of a parallel file system is comprised of distinct regions of long-term performance variation in addition to short-term performance transients.  We demonstrate how systematic identification of these performance regions, combined with comprehensive analysis, allows us to isolate the factors contributing to different performance maladies at different time scales.  From this, we present specific lessons learned and important considerations for HPC storage practitioners.</blockquote></div></div></div></div><a href="includes/files/pap206s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack190"></a><div class="section-title">OpenMP</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">OpenMP, Performance, Power, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Performance and Energy Analysis</div><div class="slot-entry"><a name="pap175"></a><div class="slot-title">A Parallelism Profiler with What-If Analyses for OpenMP Programs</div><div class="slot-authors">Nader Boushehrinejadmoradi, Adarsh Yoga, and Santosh Nagarakatte (Rutgers University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_528_1539825877_76" onclick="$('#vhsjs_view_528_1539825877_76').hide();
                $('#vhsjs_hide_528_1539825877_76').show();
                $('#527_1539825877_76').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_528_1539825877_76" onclick="$('#527_1539825877_76').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_528_1539825877_76').hide();
                $('#vhsjs_view_528_1539825877_76').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="528_1539825877_76" id="527_1539825877_76" style="display: none"><div class="arrow-slidedown"><blockquote>This paper proposes OMP-WHIP, a profiler that measures inherent parallelism in the program for a given input and provides what-if analyses to estimate improvements in parallelism. We propose a novel OpenMP series parallel graph representation (OSPG) that precisely captures series-parallel relations induced by various directives between different fragments of dynamic execution. OMP-WHIP constructs the OSPG and measures the computation performed by each dynamic fragment using hardware performance counters. This series-parallel representation along with the fine-grained measurement of computation is a performance model of the program for a given input, which enables computation of inherent parallelism. This novel performance model also enables what-if analyses where a programmer can estimate improvements in parallelism when bottlenecks are parallelized. We have used OMP-WHIP to identify parallelism bottlenecks in more than forty applications and then designed strategies to improve the speedup in seven applications.</blockquote></div></div></div></div><a href="includes/files/pap175s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap186"></a><div class="slot-title">Energy Efficiency Modeling of Parallel Applications</div><div class="slot-authors">Mark Endrei, Chao Jin, Minh Ngoc Dinh, and David Abramson (University of Queensland); Heidi Poxon and Luiz DeRose (Cray Inc); and Bronis R. de Supinski (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_530_1539825877_76" onclick="$('#vhsjs_view_530_1539825877_76').hide();
                $('#vhsjs_hide_530_1539825877_76').show();
                $('#529_1539825877_76').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_530_1539825877_76" onclick="$('#529_1539825877_76').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_530_1539825877_76').hide();
                $('#vhsjs_view_530_1539825877_76').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="530_1539825877_76" id="529_1539825877_76" style="display: none"><div class="arrow-slidedown"><blockquote>Energy efficiency has become increasingly important in high performance computing (HPC), as power constraints and costs escalate. Workload and system characteristics form a complex optimization search space in which optimal settings for energy efficiency and performance often diverge. Thus, we must identify trade-off options to find the desired balance. We present an innovative statistical model that accurately predicts the Pareto optimal trade-off options using only user-controllable parameters. Our approach can also tolerate both measurement and model errors. We study model training and validation using several HPC kernels, then with more complex workloads, including AMG and LAMMPS. We can calibrate an accurate model from as few as 12 runs, with prediction error of less than 10%. Our results identify trade-off options allowing up to 40% energy efficiency improvement at the cost of under 20% performance loss. For AMG, we reduce the required sample measurement time from 13 hours to 74 minutes.</blockquote></div></div></div></div><a href="includes/files/pap186s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap421"></a><div class="slot-title">HPL and DGEMM Performance Variability on the Xeon Platinum 8160 Processor</div><div class="slot-authors">John D. McCalpin (University of Texas, Texas Advanced Computing Center)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_532_1539825877_76" onclick="$('#vhsjs_view_532_1539825877_76').hide();
                $('#vhsjs_hide_532_1539825877_76').show();
                $('#531_1539825877_76').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_532_1539825877_76" onclick="$('#531_1539825877_76').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_532_1539825877_76').hide();
                $('#vhsjs_view_532_1539825877_76').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="532_1539825877_76" id="531_1539825877_76" style="display: none"><div class="arrow-slidedown"><blockquote>During initial testing of a large cluster equipped with Xeon Platinum 8160 processors, we observed infrequent, but significant, performance drops in HPL benchmark results. The variability was seen in both single node and multi-node runs, with approximately 0.4% of results more than 10% slower than the median. We were able to reproduce this behavior with a single-socket (24-core) DGEMM benchmark. Performance counter analysis of several thousand DGEMM runs showed that increased DRAM read traffic is the primary driver of increased execution time. Increased DRAM traffic in this benchmark is primarily generated by dramatically elevated snoop filter evictions, which arise due to the interaction of high-order (physical) address bits with the hash used to map addresses across the 24 coherence agents on the processor. These conflicts (and the associated performance variability) were effectively eliminated (for both DGEMM and HPL) by using 1 GiB large pages.</blockquote></div></div></div></div><a href="includes/files/pap421s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Linear Algebra, Memory, MPI, OpenMP, Programming Systems, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Programming Systems Tools</div><div class="slot-entry"><a name="pap179"></a><div class="slot-title">Dynamic Data Race Detection for OpenMP Programs</div><div class="slot-authors">Yizi Gu and John Mellor-Crummey (Rice University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_534_1539825877_76" onclick="$('#vhsjs_view_534_1539825877_76').hide();
                $('#vhsjs_hide_534_1539825877_76').show();
                $('#533_1539825877_76').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_534_1539825877_76" onclick="$('#533_1539825877_76').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_534_1539825877_76').hide();
                $('#vhsjs_view_534_1539825877_76').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="534_1539825877_76" id="533_1539825877_76" style="display: none"><div class="arrow-slidedown"><blockquote>Two concurrent accesses to a shared variable that are unordered by synchronization are said to be a data race if at least one access is a write. Data races cause shared memory parallel programs to behave unpredictably. This paper describes ROMP -- a tool for detecting data races in executions of scalable parallel applications that employ OpenMP for node-level parallelism. The complexity of OpenMP, which includes primitives for managing data environments, SPMD and SIMD parallelism, work sharing, tasking, mutual exclusion, and ordering, presents a formidable challenge for data race detection. ROMP is a hybrid data race detector that tracks accesses, access orderings, and mutual exclusion. Unlike other OpenMP race detectors, ROMP detects races with respect to logical parallelism rather than implementation threads. Experiments show that ROMP yields precise race reports for a broader set of OpenMP constructs than prior state-of-the-art race detectors.</blockquote></div></div></div></div><a href="includes/files/pap179s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap256"></a><div class="slot-title">ParSy: Inspection and Transformation of Sparse Matrix Computations for Parallelism</div><div class="slot-authors">Kazem Cheshmi (University of Toronto), Shoaib Kamil (Adobe Research), Michelle Mills Strout (University of Arizona), and Maryam Mehri Dehnavi (University of Toronto)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_536_1539825877_76" onclick="$('#vhsjs_view_536_1539825877_76').hide();
                $('#vhsjs_hide_536_1539825877_76').show();
                $('#535_1539825877_76').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_536_1539825877_76" onclick="$('#535_1539825877_76').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_536_1539825877_76').hide();
                $('#vhsjs_view_536_1539825877_76').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="536_1539825877_76" id="535_1539825877_76" style="display: none"><div class="arrow-slidedown"><blockquote>In this work, we describe ParSy, a framework that uses a novel inspection strategy along with a simple code transformation to optimize parallel sparse algorithms for shared memory processors. Unlike existing approaches that can suffer from load imbalance and excessive synchronization, ParSy uses a novel task coarsening strategy to create well-balanced tasks that can execute in parallel, while maintaining locality of memory accesses. Code using the ParSy inspector and transformation outperforms existing highly-optimized sparse matrix algorithms such as Cholesky factorization on multi-core processors with speedups of 2.8× and 3.1× over the MKL Pardiso and PaStiX libraries respectively.</blockquote></div></div></div></div><a href="includes/files/pap256s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap382"></a><div class="slot-title">Detecting MPI Usage Anomalies via Partial Program Symbolic Execution</div><div class="slot-authors">Fangke Ye, Jisheng Zhao, and Vivek Sarkar (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_538_1539825877_76" onclick="$('#vhsjs_view_538_1539825877_76').hide();
                $('#vhsjs_hide_538_1539825877_76').show();
                $('#537_1539825877_76').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_538_1539825877_76" onclick="$('#537_1539825877_76').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_538_1539825877_76').hide();
                $('#vhsjs_view_538_1539825877_76').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="538_1539825877_76" id="537_1539825877_76" style="display: none"><div class="arrow-slidedown"><blockquote>MPI is a message passing based programming model for distributed-memory parallelism that has been had been widely used for programming supercomputers for over 25 years. However, debugging and verification of MPI programs is widely recognized to be a deep technical challenge. This challenge is further exacerbated by a recent increase in the use of nonblocking MPI operations that bring new classes of bugs related to data races.<br><br>In this paper, we introduce a new MPI program debugging approach based on partial symbolic execution so as to avoid the false alarms inherent in the static analysis based methodology. Compared with the dynamic approach, our approach can be applied to incomplete programs and explore multiple execution paths, thereby bringing more flexibility and precision. By comparing with well known static/dynamic tools on real-world MPI applications, our approach shows same precision as the dynamic tool and avoids false positive produced by the static tool.</blockquote></div></div></div></div><a href="includes/files/pap382s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack140"></a><div class="section-title">Parallel Programming Languages, Libraries, and Models</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, Memory, Networks, Parallel Programming Languages, Libraries, and Models, Power, Programming Systems, Scheduling, Tech Program Reg Pass</span><br /><div class="session-title">Task-Based Programming</div><div class="slot-entry"><a name="pap490"></a><div class="slot-title">Dynamic Tracing: Memoization of Task Graphs for Dynamic Task-Based Runtimes</div><div class="slot-authors">Wonchan Lee (Stanford University), Elliott Slaughter (SLAC National Accelerator Laboratory), Michael Bauer and Sean Treichler (Nvidia Corporation), Todd Warszawski (Stanford University), Michael Garland (Nvidia Corporation), and Alex Aiken (Stanford University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_540_1539825877_76" onclick="$('#vhsjs_view_540_1539825877_76').hide();
                $('#vhsjs_hide_540_1539825877_76').show();
                $('#539_1539825877_76').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_540_1539825877_76" onclick="$('#539_1539825877_76').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_540_1539825877_76').hide();
                $('#vhsjs_view_540_1539825877_76').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="540_1539825877_76" id="539_1539825877_76" style="display: none"><div class="arrow-slidedown"><blockquote>Many recent programming systems for both supercomputing and data center workloads generate task graphs to express computations that run on parallel and distributed machines. Due to the overhead associated with constructing these graphs the dependence analysis that generates them is often statically computed and memoized, and the resulting graph executed repeatedly at runtime. However, many applications require a dynamic dependence analysis due to data dependent behavior, but there are new challenges in capturing and re-executing task graphs at runtime. In this work, we introduce dynamic tracing, a technique to capture a dynamic dependence analysis of a trace that generates a task graph, and replay it. We show that an implementation of dynamic tracing improves strong scaling by an average of 4.9X and up to 7.0X on a suite of already optimized benchmarks.</blockquote></div></div></div></div><a href="includes/files/pap490s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap338"></a><div class="slot-title">Runtime-Assisted Cache Coherence Deactivation in Task Parallel Programs</div><div class="slot-authors">Paul Caheny (Barcelona Supercomputing Center, Polytechnic University of Catalonia); Lluc Alvarez (Barcelona Supercomputing Center); Mateo Valero and Miquel Moretó (Barcelona Supercomputing Center, Polytechnic University of Catalonia); and Marc Casas (Barcelona Supercomputing Center)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_542_1539825877_76" onclick="$('#vhsjs_view_542_1539825877_76').hide();
                $('#vhsjs_hide_542_1539825877_76').show();
                $('#541_1539825877_76').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_542_1539825877_76" onclick="$('#541_1539825877_76').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_542_1539825877_76').hide();
                $('#vhsjs_view_542_1539825877_76').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="542_1539825877_76" id="541_1539825877_76" style="display: none"><div class="arrow-slidedown"><blockquote>With increasing core counts, the scalability of directory-based cache coherence has become a challenging problem. To reduce the area and power needs of the directory, recent proposals reduce its size by classifying data as private or shared, and disable coherence for private data. However, existing classification methods suffer from inaccuracies and require complex hardware support with limited scalability.<br><br>This paper proposes a hardware/software co-designed approach: the runtime system identifies data that is guaranteed by the programming model semantics to not require coherence and notifies the microarchitecture. The microarchitecture deactivates coherence for this private data and powers off unused directory capacity. Our proposal reduces directory accesses to just 26% of the baseline system and supports a 64× smaller directory with only 2.8% performance degradation. By dynamically calibrating the directory size, our proposal saves 86% of dynamic energy consumption in the directory without harming performance.</blockquote></div></div></div></div><a href="includes/files/pap338s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap547"></a><div class="slot-title">A Divide and Conquer Algorithm for DAG Scheduling Under Power Constraints</div><div class="slot-authors">Gökalp Demirci, Ivana Marincic, and Henry Hoffmann (University of Chicago)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_544_1539825877_76" onclick="$('#vhsjs_view_544_1539825877_76').hide();
                $('#vhsjs_hide_544_1539825877_76').show();
                $('#543_1539825877_76').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_544_1539825877_76" onclick="$('#543_1539825877_76').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_544_1539825877_76').hide();
                $('#vhsjs_view_544_1539825877_76').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="544_1539825877_76" id="543_1539825877_76" style="display: none"><div class="arrow-slidedown"><blockquote>We consider the problem of scheduling a parallel computation–represented as a directed acyclic graph (DAG)–on a distributed parallel system with a global resource constraint–specifically a global power budget–and configurable resources, allowing a range of different power/performance tradeoffs. There is a rich body of literature on the independent problems of (1) scheduling DAGs and (2) scheduling independent applications under resource constraints. Very little, however, is known about the combined problem of scheduling DAGs under resource constraints. We present a novel approximation algorithm using a divide-and-conquer method for minimizing application execution time. We prove that the length of the schedule returned by our algorithm is always within O(log n)-factor of the optimum that can be achieved with selection of configurations for the tasks. We implement and test our algorithm on simulations of real application DAGs. We find that our divide-and-conquer method improves performance by up to 75% compared to greedy scheduling algorithms.</blockquote></div></div></div></div><a href="includes/files/pap547s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack141"></a><div class="section-title">Performance</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">OpenMP, Performance, Power, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Performance and Energy Analysis</div><div class="slot-entry"><a name="pap175"></a><div class="slot-title">A Parallelism Profiler with What-If Analyses for OpenMP Programs</div><div class="slot-authors">Nader Boushehrinejadmoradi, Adarsh Yoga, and Santosh Nagarakatte (Rutgers University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_546_1539825877_76" onclick="$('#vhsjs_view_546_1539825877_76').hide();
                $('#vhsjs_hide_546_1539825877_76').show();
                $('#545_1539825877_76').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_546_1539825877_76" onclick="$('#545_1539825877_76').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_546_1539825877_76').hide();
                $('#vhsjs_view_546_1539825877_76').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="546_1539825877_76" id="545_1539825877_76" style="display: none"><div class="arrow-slidedown"><blockquote>This paper proposes OMP-WHIP, a profiler that measures inherent parallelism in the program for a given input and provides what-if analyses to estimate improvements in parallelism. We propose a novel OpenMP series parallel graph representation (OSPG) that precisely captures series-parallel relations induced by various directives between different fragments of dynamic execution. OMP-WHIP constructs the OSPG and measures the computation performed by each dynamic fragment using hardware performance counters. This series-parallel representation along with the fine-grained measurement of computation is a performance model of the program for a given input, which enables computation of inherent parallelism. This novel performance model also enables what-if analyses where a programmer can estimate improvements in parallelism when bottlenecks are parallelized. We have used OMP-WHIP to identify parallelism bottlenecks in more than forty applications and then designed strategies to improve the speedup in seven applications.</blockquote></div></div></div></div><a href="includes/files/pap175s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap186"></a><div class="slot-title">Energy Efficiency Modeling of Parallel Applications</div><div class="slot-authors">Mark Endrei, Chao Jin, Minh Ngoc Dinh, and David Abramson (University of Queensland); Heidi Poxon and Luiz DeRose (Cray Inc); and Bronis R. de Supinski (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_548_1539825877_76" onclick="$('#vhsjs_view_548_1539825877_76').hide();
                $('#vhsjs_hide_548_1539825877_76').show();
                $('#547_1539825877_76').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_548_1539825877_76" onclick="$('#547_1539825877_76').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_548_1539825877_76').hide();
                $('#vhsjs_view_548_1539825877_76').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="548_1539825877_76" id="547_1539825877_76" style="display: none"><div class="arrow-slidedown"><blockquote>Energy efficiency has become increasingly important in high performance computing (HPC), as power constraints and costs escalate. Workload and system characteristics form a complex optimization search space in which optimal settings for energy efficiency and performance often diverge. Thus, we must identify trade-off options to find the desired balance. We present an innovative statistical model that accurately predicts the Pareto optimal trade-off options using only user-controllable parameters. Our approach can also tolerate both measurement and model errors. We study model training and validation using several HPC kernels, then with more complex workloads, including AMG and LAMMPS. We can calibrate an accurate model from as few as 12 runs, with prediction error of less than 10%. Our results identify trade-off options allowing up to 40% energy efficiency improvement at the cost of under 20% performance loss. For AMG, we reduce the required sample measurement time from 13 hours to 74 minutes.</blockquote></div></div></div></div><a href="includes/files/pap186s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap421"></a><div class="slot-title">HPL and DGEMM Performance Variability on the Xeon Platinum 8160 Processor</div><div class="slot-authors">John D. McCalpin (University of Texas, Texas Advanced Computing Center)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_550_1539825877_76" onclick="$('#vhsjs_view_550_1539825877_76').hide();
                $('#vhsjs_hide_550_1539825877_76').show();
                $('#549_1539825877_76').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_550_1539825877_76" onclick="$('#549_1539825877_76').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_550_1539825877_76').hide();
                $('#vhsjs_view_550_1539825877_76').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="550_1539825877_76" id="549_1539825877_76" style="display: none"><div class="arrow-slidedown"><blockquote>During initial testing of a large cluster equipped with Xeon Platinum 8160 processors, we observed infrequent, but significant, performance drops in HPL benchmark results. The variability was seen in both single node and multi-node runs, with approximately 0.4% of results more than 10% slower than the median. We were able to reproduce this behavior with a single-socket (24-core) DGEMM benchmark. Performance counter analysis of several thousand DGEMM runs showed that increased DRAM read traffic is the primary driver of increased execution time. Increased DRAM traffic in this benchmark is primarily generated by dramatically elevated snoop filter evictions, which arise due to the interaction of high-order (physical) address bits with the hash used to map addresses across the 24 coherence agents on the processor. These conflicts (and the associated performance variability) were effectively eliminated (for both DGEMM and HPL) by using 1 GiB large pages.</blockquote></div></div></div></div><a href="includes/files/pap421s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Data Analytics, Performance, Programming Systems, Storage, Tools, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Performance Optimization Studies</div><div class="slot-entry"><a name="pap335"></a><div class="slot-title">Many-Core Graph Workload Analysis</div><div class="slot-authors">Stijn Eyerman, Wim Heirman, Kristof Du Bois, Joshua B. Fryman, and Ibrahim Hur (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_552_1539825877_76" onclick="$('#vhsjs_view_552_1539825877_76').hide();
                $('#vhsjs_hide_552_1539825877_76').show();
                $('#551_1539825877_76').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_552_1539825877_76" onclick="$('#551_1539825877_76').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_552_1539825877_76').hide();
                $('#vhsjs_view_552_1539825877_76').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="552_1539825877_76" id="551_1539825877_76" style="display: none"><div class="arrow-slidedown"><blockquote>Graph applications have specific characteristics that are not common in other application domains. In this paper, we analyze multiple graph applications on current multi- and many-core processors and provide conclusions and recommendations for future designs. We provide new insights on executing graph applications on many-core processors.<br><br>Our main novel observations are (i) some memory streams do show locality, while others show no locality, (ii) thread imbalance becomes a major problem with many threads, and (iii) many threads are required to saturate high-bandwidth memories. We recommend a selective memory access policy, where accesses with locality are cached and prefetched, while accesses without locality can remain uncached to save cache capacity. Additionally, more threads are needed, but they are not used efficiently due to thread imbalance. Our recommendation is to revise the graph analysis algorithms to provide more parallelism, and to provide a few high-performance cores that speedup sections with low parallelism.</blockquote></div></div></div></div><a href="includes/files/pap335s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap488"></a><div class="slot-title">Lessons Learned from Analyzing Dynamic Promotion for User-Level Threading</div><div class="slot-authors">Shintaro Iwasaki (University of Tokyo), Abdelhalim Amer (Argonne National Laboratory), Kenjiro Taura (University of Tokyo), and Pavan Balaji (Argonne National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_554_1539825877_77" onclick="$('#vhsjs_view_554_1539825877_77').hide();
                $('#vhsjs_hide_554_1539825877_77').show();
                $('#553_1539825877_77').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_554_1539825877_77" onclick="$('#553_1539825877_77').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_554_1539825877_77').hide();
                $('#vhsjs_view_554_1539825877_77').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="554_1539825877_77" id="553_1539825877_77" style="display: none"><div class="arrow-slidedown"><blockquote>A performance vs. practicality trade-off exists between user-level threading techniques. The community has settled mostly on a black-and-white perspective; fully fledged threads assume that suspension is imminent and incur overheads when suspension does not take place, and run-to-completion threads are more lightweight but less practical since they cannot suspend. Gray areas exist, however, whereby threads can start with minimal capabilities and then can be dynamically promoted to acquire additional capabilities when needed. This paper investigates the full spectrum of threading techniques from a performance vs. practicality trade-off perspective on modern multicore and many-core systems. Our results indicate that achieving the best trade-off highly depends on the suspension likelihood; dynamic promotion is more appropriate when suspension is unlikely and represents a solid replacement for run to completion, thanks to its lower programming constraints, while fully fledged threads remain the technique of choice when suspension likelihood is high.</blockquote></div></div></div></div><a href="includes/files/pap488s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap484"></a><div class="slot-title">Topology-Aware Space-Shared Co-Analysis of Large-Scale Molecular Dynamics Simulations</div><div class="slot-authors">Preeti Malakar (Indian Institute of Technology Kanpur); Todd Munson, Christopher Knight, and Venkatram Vishwanath (Argonne National Laboratory); and Michael E. Papka (Argonne National Laboratory, Northern Illinois University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_556_1539825877_77" onclick="$('#vhsjs_view_556_1539825877_77').hide();
                $('#vhsjs_hide_556_1539825877_77').show();
                $('#555_1539825877_77').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_556_1539825877_77" onclick="$('#555_1539825877_77').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_556_1539825877_77').hide();
                $('#vhsjs_view_556_1539825877_77').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="556_1539825877_77" id="555_1539825877_77" style="display: none"><div class="arrow-slidedown"><blockquote>Analysis of scientific simulation data can be concurrently executed with simulation either in time- or space-shared mode. This mitigates the I/O bottleneck.  However it results in either stalling the simulation for performing the analysis or transferring data for analysis. In this paper, we improve the throughput of space-shared in situ analysis of large-scale simulations by topology-aware mapping and optimal process decomposition. We propose node interconnect topology-aware process placement for simulation and analysis to reduce the data movement time. We also present an integer linear program for optimal 3D decompositions of simulation and analysis processes. We demonstrate our approach using molecular dynamics simulation on Mira, Cori and Theta supercomputers. Our mapping schemes, combined with optimal 3D process decomposition and code optimizations resulted in up to 30% lower execution times for space-shared in situ analysis than the default approach. Our mappings also reduce MPI collective I/O times by 10-40%.</blockquote></div></div></div></div><a href="includes/files/pap484s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, MPI, Networks, Performance, Programming Systems, State of the Practice, Tech Program Reg Pass</span><br /><div class="session-title">MPI Optimization and Characterization</div><div class="slot-entry"><a name="pap504"></a><div class="slot-title">Cooperative Rendezvous Protocols for Improved Performance and Overlap</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">S. Chakraborty, M. Bayatpour, J. Hashmi, H. Subramoni, and D. K. Panda (Ohio State University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_558_1539825877_77" onclick="$('#vhsjs_view_558_1539825877_77').hide();
                $('#vhsjs_hide_558_1539825877_77').show();
                $('#557_1539825877_77').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_558_1539825877_77" onclick="$('#557_1539825877_77').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_558_1539825877_77').hide();
                $('#vhsjs_view_558_1539825877_77').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="558_1539825877_77" id="557_1539825877_77" style="display: none"><div class="arrow-slidedown"><blockquote>With the emergence of larger multi-/many-core clusters, performance of large message communication is becoming more important. MPI libraries use different Rendezvous protocols to perform large message communication. However, existing Rendezvous protocols do not consider the overall communication pattern and make optimal use of the Sender and the Receiver CPUs. In this work, we propose a cooperative Rendezvous protocol that can provide up to 2x improvement in intra-node bandwidth and latency for large messages. We also propose a scheme to dynamically choose the best Rendezvous protocol for each message based on the communication pattern.  Finally, we show how these improvements can increase the overlap of computation with intra-node and inter-node communication, and lead to application level benefits. We evaluate proposed designs on three different architectures including Intel Xeon, Knights Landing, and OpenPOWER with different HPC applications and obtain benefits up to 19% with Graph500, 16% with CoMD, and 10% with MiniGhost.</blockquote></div></div></div></div><a href="includes/files/pap504s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap463"></a><div class="slot-title">Framework for Scalable Intra-Node Collective Operations Using Shared Memory</div><div class="slot-authors">Surabhi Jain, Rashid Kaleem, Marc Gamell Balmana, Akhil Langer, Dmitry Durnov, Alexander Sannikov, and Maria Garzaran (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_560_1539825877_77" onclick="$('#vhsjs_view_560_1539825877_77').hide();
                $('#vhsjs_hide_560_1539825877_77').show();
                $('#559_1539825877_77').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_560_1539825877_77" onclick="$('#559_1539825877_77').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_560_1539825877_77').hide();
                $('#vhsjs_view_560_1539825877_77').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="560_1539825877_77" id="559_1539825877_77" style="display: none"><div class="arrow-slidedown"><blockquote>Collective operations are used in MPI programs to express common communication patterns, collective computations, or synchronizations. In many collectives, such as barrier or allreduce, the intra-node component of the collective is in the critical path, as the inter-node communication cannot start until the intra-node component has been executed. Thus, with increasing number of core counts in each node, intra-node optimizations that leverage the intra-node shared memory become increasingly important.<br><br>In this paper, we focus on the performance benefit of optimizing intra-node collectives using shared memory. We optimize several collectives using the primitives in broadcast and reduce as building blocks for other collectives. A comparison of our implementation on top of MPICH shows significant performance speedups with respect to the original MPICH implementation, MVAPICH, and OpenMPI, among others.</blockquote></div></div></div></div><a href="includes/files/pap463s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap537"></a><div class="slot-title">Characterization of MPI Usage on a Production Supercomputer</div><div class="slot-authors">Sudheer Chunduri, Scott Parker, Pavan Balaji, Kevin Harms, and Kalyan Kumaran (Argonne National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_562_1539825877_77" onclick="$('#vhsjs_view_562_1539825877_77').hide();
                $('#vhsjs_hide_562_1539825877_77').show();
                $('#561_1539825877_77').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_562_1539825877_77" onclick="$('#561_1539825877_77').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_562_1539825877_77').hide();
                $('#vhsjs_view_562_1539825877_77').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="562_1539825877_77" id="561_1539825877_77" style="display: none"><div class="arrow-slidedown"><blockquote>MPI is the most prominent programming model used in scientific computing today.  Despite it's importance, however, how scientific applications use it in production is not very well understood due to the lack of low overhead profiling tools.   We used a lightweight profiling tool, called autoperf, to log the MPI usage characteristics of production applications on a large supercomputing system (Mira) and its corresponding development system (Cetus).  Autoperf limits the amount of information that it records in order to keep the overhead to a minimum while still storing enough data to derive useful insights.  MPI usage statistics have been collected for over 100K jobs that were run within a 2-year period and are analyzed.  The analysis of this data is intended as a mechanism to provide useful insights for MPI developers and network hardware developers for their next generation of improvements, and for supercomputing center operators for their next system procurements.</blockquote></div></div></div></div><a href="includes/files/pap537s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">GPUs, Memory, NVRAM, Performance, System Software, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Non-Volatile Memory</div><div class="slot-entry"><a name="pap203"></a><div class="slot-title">Runtime Data Management on Non-Volatile Memory-Based Heterogeneous Memory for Task-Parallel Programs</div><div class="slot-authors">Kai Wu, Jie Ren, and Dong Li (University of California, Merced)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_564_1539825877_77" onclick="$('#vhsjs_view_564_1539825877_77').hide();
                $('#vhsjs_hide_564_1539825877_77').show();
                $('#563_1539825877_77').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_564_1539825877_77" onclick="$('#563_1539825877_77').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_564_1539825877_77').hide();
                $('#vhsjs_view_564_1539825877_77').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="564_1539825877_77" id="563_1539825877_77" style="display: none"><div class="arrow-slidedown"><blockquote>Non-volatile memory (NVM) provides a scalable solution to replace DRAM as main memory. Because of relatively high latency and low bandwidth of NVM (comparing with DRAM), NVM often pairs with DRAM to build a  heterogeneous main memory system (HMS). Deciding data placement on NVM-based HMS is critical to enable future NVM-based HPC. In this paper, we study task-parallel programs and introduce a runtime system to address the data placement problem on NVM-based HMS. Leveraging semantics and execution mode of task-parallel programs, we efficiently characterize memory access patterns of tasks and reduce data movement overhead. We also introduce a performance model to predict performance for tasks with various data placements on HMS. Evaluating with a set of HPC benchmarks, we show that our runtime system achieves higher performance than a conventional HMS-oblivious runtime (24% improvement on average) and two state-of-the-art HMS-aware solutions (16% and 11% improvement on average, respectively).</blockquote></div></div></div></div><a href="includes/files/pap203s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap194"></a><div class="slot-title">DRAGON: Breaking GPU Memory Capacity Limits with Direct NVM Access</div><div class="slot-authors">Pak Markthub (Tokyo Institute of Technology); Mehmet E. Belviranli, Seyong Lee, and Jeffrey S. Vetter (Oak Ridge National Laboratory); and Satoshi Matsuoka (RIKEN, Tokyo Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_566_1539825877_77" onclick="$('#vhsjs_view_566_1539825877_77').hide();
                $('#vhsjs_hide_566_1539825877_77').show();
                $('#565_1539825877_77').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_566_1539825877_77" onclick="$('#565_1539825877_77').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_566_1539825877_77').hide();
                $('#vhsjs_view_566_1539825877_77').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="566_1539825877_77" id="565_1539825877_77" style="display: none"><div class="arrow-slidedown"><blockquote>Heterogeneous computing with accelerators is growing in importance in high performance computing (HPC). Recently, application datasets have expanded beyond the memory capacity of these accelerators, and often beyond the capacity of their hosts. Meanwhile, nonvolatile memory (NVM) storage has emerged as a pervasive component in HPC systems because NVM provides massive amounts of memory capacity at affordable cost. Currently, for accelerator applications to use NVM, they must manually orchestrate data movement across multiple memories and this approach only performs well for applications with simple access behaviors. To address this issue, we developed DRAGON, a solution that enables all classes of GP-GPU applications to transparently compute on terabyte datasets residing in NVM. DRAGON leverages the page-faulting mechanism on the recent NVIDIA GPUs by extending capabilities of CUDA Unified Memory (UM). Our experimental results show that DRAGON transparently expands memory capacity and obtain additional speedups via automated I/O and data transfer overlapping.</blockquote></div></div></div></div><a href="includes/files/pap194s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap355"></a><div class="slot-title">Siena: Exploring the Design Space of Heterogeneous Memory Systems</div><div class="slot-authors">Ivy B. Peng and Jeffrey S. Vetter (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_568_1539825877_77" onclick="$('#vhsjs_view_568_1539825877_77').hide();
                $('#vhsjs_hide_568_1539825877_77').show();
                $('#567_1539825877_77').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_568_1539825877_77" onclick="$('#567_1539825877_77').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_568_1539825877_77').hide();
                $('#vhsjs_view_568_1539825877_77').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="568_1539825877_77" id="567_1539825877_77" style="display: none"><div class="arrow-slidedown"><blockquote>Memory systems are crucial to the performance, power, and cost of high-performance computing systems. Recently, multiple factors are driving the need for more complex, deep memory hierarchies. However, architects and customers are struggling to design memory systems that effectively balance multiple, often competing, factors in this large, multidimensional, and fast-moving design space. In this paper, we systematically explore the organization of heterogeneous memory systems on a framework, called Siena. Siena facilitates quick exploration of memory architectures with flexible configurations of memory systems and realistic memory workloads. We perform a design space exploration on 22 proposed memory systems using eight relevant workloads. Our results show that horizontal organizations of memories can achieve higher performance than that of vertical organizations when the distribution of memory traffic balances the performance gap between memories. However, the coupling effects through shared resources and application behaviors could negate the advantage of high-performance memory in horizontal organizations.</blockquote></div></div></div></div><a href="includes/files/pap355s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Performance, Resiliency, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Resilience II</div><div class="slot-entry"><a name="pap392"></a><div class="slot-title">Lessons Learned from Memory Errors Observed Over the Lifetime of Cielo</div><div class="slot-authors">Scott Levy and Kurt B. Ferreira (Sandia National Laboratories), Nathan DeBardeleben (Los Alamos National Laboratory), Taniya Siddiqua and Vilas Sridharan (Advanced Micro Devices Inc), and Elisabeth Baseman (Los Alamos National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_570_1539825877_77" onclick="$('#vhsjs_view_570_1539825877_77').hide();
                $('#vhsjs_hide_570_1539825877_77').show();
                $('#569_1539825877_77').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_570_1539825877_77" onclick="$('#569_1539825877_77').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_570_1539825877_77').hide();
                $('#vhsjs_view_570_1539825877_77').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="570_1539825877_77" id="569_1539825877_77" style="display: none"><div class="arrow-slidedown"><blockquote>Maintaining the performance of high-performance computing (HPC) applications as failures increase is a major challenge for next-generation extreme-scale systems. Recent research demonstrates that hardware failures are expected to become more common due to increased component counts, reduced device-feature sizes, and tightly-constrained power budgets. Few existing studies, however, have examined failures in the context of the entire lifetime of a single platform. In this paper, we analyze failure data collected over the entire lifetime of Cielo, a leadership-class HPC system. Our analysis reveals several key findings, including: (i) Cielo’s memory (DRAM and SRAM) exhibited no discernible aging effects; (ii) correctable memory faults are not predictive of future uncorrectable memory faults; (iii) developing more comprehensive logging facilities will improve failure analysis on future machines; (iv) continued advances will be required to ensure current failure mitigation techniques remain a viable option for future platforms.</blockquote></div></div></div></div><a href="includes/files/pap392s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap381"></a><div class="slot-title">Partial Redundancy in HPC Systems with Non-Uniform Node Reliabilities</div><div class="slot-authors">Zaeem Hussain, Taieb Znati, and Rami Melhem (University of Pittsburgh)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_572_1539825877_77" onclick="$('#vhsjs_view_572_1539825877_77').hide();
                $('#vhsjs_hide_572_1539825877_77').show();
                $('#571_1539825877_77').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_572_1539825877_77" onclick="$('#571_1539825877_77').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_572_1539825877_77').hide();
                $('#vhsjs_view_572_1539825877_77').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="572_1539825877_77" id="571_1539825877_77" style="display: none"><div class="arrow-slidedown"><blockquote>We study the usefulness of partial redundancy in HPC message passing systems where individual node failure distributions are not identical. Prior research works on fault tolerance have generally assumed identical failure distributions for the nodes of the system. In such settings, partial replication has never been shown to outperform the two extremes (full and no-replication) for any significant range of node counts. We argue that partial redundancy may provide the best performance under the more realistic assumption of non-identical node failure distributions. We provide theoretical results on arranging nodes with different reliability values among replicas such that system reliability is maximized. Moreover, using system reliability to compute MTTI (mean-time-to-interrupt) and expected completion time of a partially replicated system, we numerically determine the optimal partial replication degree. Our results indicate that partial replication can be a more efficient alternative to full replication at system scales where Checkpoint/Restart alone is not sufficient.</blockquote></div></div></div></div><a href="includes/files/pap381s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap386"></a><div class="slot-title">Evaluating and Accelerating High-Fidelity Error Injection for HPC</div><div class="slot-authors">Chun-Kai Chang, Sangkug Lym, and Nicholas Kelly (University of Texas); Michael B. Sullivan (Nvidia Corporation); and Mattan Erez (University of Texas)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_574_1539825877_77" onclick="$('#vhsjs_view_574_1539825877_77').hide();
                $('#vhsjs_hide_574_1539825877_77').show();
                $('#573_1539825877_77').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_574_1539825877_77" onclick="$('#573_1539825877_77').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_574_1539825877_77').hide();
                $('#vhsjs_view_574_1539825877_77').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="574_1539825877_77" id="573_1539825877_77" style="display: none"><div class="arrow-slidedown"><blockquote>We address two important concerns in the analysis of the behavior of applications in the presence of hardware errors: (1) when is it important to model how hardware faults lead to erroneous values (instruction-level errors) with high fidelity, as opposed to using simple bit-flipping models, and (2) how to enable fast high-fidelity error injection campaigns, in particular when error detectors are employed. We present and verify a new nested Monte Carlo methodology for evaluating high-fidelity gate-level fault models and error-detector coverage, which is orders of magnitude faster than current approaches. We use that methodology to demonstrate that, without detectors, simple error models suffice for evaluating errors in 9 HPC benchmarks.</blockquote></div></div></div></div><a href="includes/files/pap386s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Networks, Performance, Scientific Computing, State of the Practice, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Large Scale System Deployments</div><div class="slot-entry"><a name="pap277"></a><div class="slot-title">The Design, Deployment, and Evaluation of the CORAL Pre-Exascale Systems</div><div class="slot-authors">Sudharshan S. Vazhkudai (Oak Ridge National Laboratory); Bronis R. de Supinski (Lawrence Livermore National Laboratory); Arthur S. Bland and Al Geist (Oak Ridge National Laboratory); James Sexton and Jim Kahle (IBM); Christopher J. Zimmer, Scott Atchley, Sarp H. Oral, Don E. Maxwell, and Veronica G. Vergara Larrea (Oak Ridge National Laboratory); Adam Bertsch and Robin Goldstone (Lawrence Livermore National Laboratory); Wayne Joubert (Oak Ridge National Laboratory); Chris Chambreau (Lawrence Livermore National Laboratory); David Appelhans and Robert Blackmore (IBM); Ben Casses (Lawrence Livermore National Laboratory); George Chochia and Gene Davison (IBM); Matthew A. Ezell (Oak Ridge National Laboratory); Tom Gooding (IBM); Elsa Gonsiorowski (Lawrence Livermore National Laboratory); Leopold Grinberg, Bill Hanson, and Bill Hartner (IBM); Ian Karlin and Matthew L. Leininger (Lawrence Livermore National Laboratory); Dustin Leverman (Oak Ridge National Laboratory); Chris Marroquin (IBM); Adam Moody (Lawrence Livermore National Laboratory); Martin Ohmacht (IBM); Ramesh Pankajakshan (Lawrence Livermore National Laboratory); Fernando Pizzano (IBM); James H. Rogers (Oak Ridge National Laboratory); Bryan Rosenburg (IBM); Drew Schmidt, Mallikarjun Shankar, and Feiyi Wang (Oak Ridge National Laboratory); Py Watson (Lawrence Livermore National Laboratory); Bob Walkup (IBM); Lance D. Weems (Lawrence Livermore National Laboratory); and Junqi Yin (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_576_1539825877_77" onclick="$('#vhsjs_view_576_1539825877_77').hide();
                $('#vhsjs_hide_576_1539825877_77').show();
                $('#575_1539825877_77').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_576_1539825877_77" onclick="$('#575_1539825877_77').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_576_1539825877_77').hide();
                $('#vhsjs_view_576_1539825877_77').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="576_1539825877_77" id="575_1539825877_77" style="display: none"><div class="arrow-slidedown"><blockquote>CORAL, the Collaboration of Oak Ridge, Argonne and Livermore, is fielding two similar IBM systems, Summit and Sierra, with NVIDIA GPUs that will replace the existing Titan and Sequoia systems. Summit and Sierra are currently ranked No. 1 and No. 3, respectively, on the Top500 list. We discuss the design and key differences of the systems. Our evaluation of the systems highlights the following. Applications that fit in HBM see the most benefit and may prefer more GPUs; however, for some applications, the CPU-GPU bandwidth is more important than the number of GPUs. The node-local burst buffer scales linearly, and can achieve a 4X improvement over the parallel file system for large jobs; smaller jobs, however, may benefit from writing directly to the PFS. Finally, several CPU, network and memory bound analytics and GPU-bound deep learning codes achieve up to a 11X and 79X speedup/node, respectively over Titan.</blockquote></div></div></div></div><a href="includes/files/pap277s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap449"></a><div class="slot-title">Best Practices and Lessons from Deploying and Operating a Sustained-Petascale System: The Blue Waters Experience</div><div class="slot-authors">Gregory H. Bauer, Brett Bode, Jeremy Enos, William T. Kramer, Scott Lathrop, Celso L. Mendes, and Roberto R. Sisneros (University of Illinois, National Center for Supercomputing Applications)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_578_1539825877_77" onclick="$('#vhsjs_view_578_1539825877_77').hide();
                $('#vhsjs_hide_578_1539825877_77').show();
                $('#577_1539825877_77').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_578_1539825877_77" onclick="$('#577_1539825877_77').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_578_1539825877_77').hide();
                $('#vhsjs_view_578_1539825877_77').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="578_1539825877_77" id="577_1539825877_77" style="display: none"><div class="arrow-slidedown"><blockquote>Building and operating versatile extreme-scale computing systems that work productively for a range of frontier research domains present many challenges and opportunities. Solutions created, experiences acquired, and lessons learned, while rarely published, could drive the development of new methods and practices and raise the bar for all organizations supporting research, scholarship, and education. This paper describes the methods and procedures developed for deploying, supporting, and continuously improving the Blue Waters system and its services during the last five years. Being the first US sustained-petascale computing platform available to the open-science community, the Blue Waters project pioneered various unique practices that we are sharing to be adopted and further improved by the community. We present our support and service methodologies, and the leadership practices employed for ensuring that the system stays highly efficient and productive. We also provide the return on investment summaries related to deploying and operating the system.</blockquote></div></div></div></div><a href="includes/files/pap449s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap346"></a><div class="slot-title">Performance Evaluation of a Vector Supercomputer SX-Aurora TSUBASA</div><div class="slot-authors">Kazuhiko Komatsu (Tohoku University); Shintaro Momose, Yoko Isobe, Osamu Watanabe, and Akihiro Musa (Tohoku University, NEC Corporation); Mitsuo Yokokawa (Kobe University, NEC Corporation); Toshikazu Aoyama (NEC Corporation); and Masayuki Sato and Hiroaki Kobayashi (Tohoku University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_580_1539825877_77" onclick="$('#vhsjs_view_580_1539825877_77').hide();
                $('#vhsjs_hide_580_1539825877_77').show();
                $('#579_1539825877_77').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_580_1539825877_77" onclick="$('#579_1539825877_77').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_580_1539825877_77').hide();
                $('#vhsjs_view_580_1539825877_77').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="580_1539825877_77" id="579_1539825877_77" style="display: none"><div class="arrow-slidedown"><blockquote>A new SX-Aurora TSUBASA vector supercomputer has been released with a new system architecture and a new execution model to achieve high sustained performance, especially for memory-intensive applications. In SX-Aurora TSUBASA, the vector host (VH) of a standard x86 Linux node is attached to the vector engine (VE) of a newly developed vector processor.  An application is executed on the VE, and only system calls are offloaded to the VH. This new execution model can avoid redundant data transfers between a VH and a VE that can easily become a bottleneck in the conventional execution model. This paper examines the potential of SX-Aurora TSUBASA. First, the basic performance of SX-Aurora TSUBASA is clarified by evaluating benchmark programs. Then, the effectiveness of the new execution model is examined by using a microbenchmark.  Finally, the high potential of SX-Aurora TSUBASA is clarified through evaluations of practical applications.</blockquote></div></div></div></div><a href="includes/files/pap346s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Architectures, Compiler Analysis and Optimization, Floating Point, Performance, Precision, Programming Systems, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Arithmetic and Optimization</div><div class="slot-entry"><a name="pap431"></a><div class="slot-title">Associative Instruction Reordering to Alleviate Register Pressure</div><div class="slot-authors">Prashant Singh Rawat, Aravind Sukumaran-Rajam, and Atanas Rountev (Ohio State University); Fabrice Rastello (French Institute for Research in Computer Science and Automation (INRIA)); Louis-Noel Pouchet (Colorado State University); and P. Sadayappan (Ohio State University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_582_1539825877_77" onclick="$('#vhsjs_view_582_1539825877_77').hide();
                $('#vhsjs_hide_582_1539825877_77').show();
                $('#581_1539825877_77').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_582_1539825877_77" onclick="$('#581_1539825877_77').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_582_1539825877_77').hide();
                $('#vhsjs_view_582_1539825877_77').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="582_1539825877_77" id="581_1539825877_77" style="display: none"><div class="arrow-slidedown"><blockquote>Register allocation is generally considered a practically solved problem. For most applications, the register allocation strategies in production compilers are very effective in controlling the number of loads/stores and register spills. However, existing register allocation strategies are not effective and result in excessive register spilling for computation patterns with a high degree of many-to-many data reuse, e.g., high-order stencils and tensor contractions.  We develop a source-to-source instruction reordering strategy that exploits the flexibility of reordering associative operations to alleviate register pressure.  The developed transformation module implements an adaptable strategy that can appropriately control the degree of instruction-level parallelism, while relieving register pressure.  The effectiveness of the approach is demonstrated through experimental results using multiple production compilers (GCC, Clang/LLVM) and target platforms (Intel Xeon Phi, and Intel x86 multi-core).</blockquote></div></div></div></div><a href="includes/files/pap431s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap464"></a><div class="slot-title">Harnessing GPU's Tensor Cores Fast FP16 Arithmetic to Speedup Mixed-Precision Iterative Refinement Solvers</div><div class="slot-authors">Azzam Haidar (University of Tennessee, Innovative Computing Laboratory); Stan Tomov and Jack Dongarra (University of Tennessee); and Nicholas Higham (University of Manchester, School of Mathematics)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_584_1539825877_77" onclick="$('#vhsjs_view_584_1539825877_77').hide();
                $('#vhsjs_hide_584_1539825877_77').show();
                $('#583_1539825877_77').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_584_1539825877_77" onclick="$('#583_1539825877_77').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_584_1539825877_77').hide();
                $('#vhsjs_view_584_1539825877_77').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="584_1539825877_77" id="583_1539825877_77" style="display: none"><div class="arrow-slidedown"><blockquote>The use of low-precision arithmetic in computing methods has been a powerful tool to accelerate numerous scientific computing applications including Artificial Intelligence. We present an investigation showing that other HPC applications can harness this power too, and in particular, the general HPC problem of solving Ax = b, where A is a large dense matrix, and the solution is needed in FP64 accuracy. Our approach is based on the mixed-precision (FP16->FP64) iterative refinement technique – we generalize and extend prior advances into a framework, for which we develop architecture-specific algorithms and highly-tuned implementations where we show how the use of FP16-TC (tensor cores) arithmetic can provide up to 4X speedup and improve the energy consumption by a factor of 5 achieving 74 Gflop/Watt. This is due to the performance boost that the FP16 (Tensor Cores) provide and to its better accuracy that outperforms the classical FP16.</blockquote></div></div></div></div><a href="includes/files/pap464s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap503"></a><div class="slot-title">ADAPT: Algorithmic Differentiation Applied to Floating-Point Precision Tuning</div><div class="slot-authors">Harshitha Menon (Lawrence Livermore National Laboratory); Michael O. Lam (James Madison University, Lawrence Livermore National Laboratory); and Daniel Osei-Kuffuor, Markus Schordan, Scott Lloyd, Kathryn Mohror, and Jeffrey Hittinger (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_586_1539825877_77" onclick="$('#vhsjs_view_586_1539825877_77').hide();
                $('#vhsjs_hide_586_1539825877_77').show();
                $('#585_1539825877_77').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_586_1539825877_77" onclick="$('#585_1539825877_77').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_586_1539825877_77').hide();
                $('#vhsjs_view_586_1539825877_77').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="586_1539825877_77" id="585_1539825877_77" style="display: none"><div class="arrow-slidedown"><blockquote>HPC applications extensively use floating point arithmetic operations to solve computational problems in various domains. Mixed precision computing, use of lowest precision data type sufficient to achieve a desired accuracy, have been explored to improve performance, reduce power consumption and data movement. Manually optimizing the program to use mixed precision is challenging. In this work, we present ADAPT, an approach for mixed precision analysis on HPC workloads while providing guarantees about the final output error. Our approach uses algorithmic differentiation to accurately estimate the output error for mixed precision configuration. ADAPT provides floating-point precision sensitivity of programs, which highlights regions of the code that that can potentially be converted to lower precision, is used to make algorithmic choices and develop mixed precision configurations. We evaluate ADAPT on six benchmarks and a proxy application and show that we are able to achieve a speedup of 1.2x on the proxy application, LULESH.</blockquote></div></div></div></div><a href="includes/files/pap503s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack142"></a><div class="section-title">Power</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">OpenMP, Performance, Power, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Performance and Energy Analysis</div><div class="slot-entry"><a name="pap175"></a><div class="slot-title">A Parallelism Profiler with What-If Analyses for OpenMP Programs</div><div class="slot-authors">Nader Boushehrinejadmoradi, Adarsh Yoga, and Santosh Nagarakatte (Rutgers University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_588_1539825877_78" onclick="$('#vhsjs_view_588_1539825877_78').hide();
                $('#vhsjs_hide_588_1539825877_78').show();
                $('#587_1539825877_78').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_588_1539825877_78" onclick="$('#587_1539825877_78').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_588_1539825877_78').hide();
                $('#vhsjs_view_588_1539825877_78').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="588_1539825877_78" id="587_1539825877_78" style="display: none"><div class="arrow-slidedown"><blockquote>This paper proposes OMP-WHIP, a profiler that measures inherent parallelism in the program for a given input and provides what-if analyses to estimate improvements in parallelism. We propose a novel OpenMP series parallel graph representation (OSPG) that precisely captures series-parallel relations induced by various directives between different fragments of dynamic execution. OMP-WHIP constructs the OSPG and measures the computation performed by each dynamic fragment using hardware performance counters. This series-parallel representation along with the fine-grained measurement of computation is a performance model of the program for a given input, which enables computation of inherent parallelism. This novel performance model also enables what-if analyses where a programmer can estimate improvements in parallelism when bottlenecks are parallelized. We have used OMP-WHIP to identify parallelism bottlenecks in more than forty applications and then designed strategies to improve the speedup in seven applications.</blockquote></div></div></div></div><a href="includes/files/pap175s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap186"></a><div class="slot-title">Energy Efficiency Modeling of Parallel Applications</div><div class="slot-authors">Mark Endrei, Chao Jin, Minh Ngoc Dinh, and David Abramson (University of Queensland); Heidi Poxon and Luiz DeRose (Cray Inc); and Bronis R. de Supinski (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_590_1539825877_78" onclick="$('#vhsjs_view_590_1539825877_78').hide();
                $('#vhsjs_hide_590_1539825877_78').show();
                $('#589_1539825877_78').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_590_1539825877_78" onclick="$('#589_1539825877_78').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_590_1539825877_78').hide();
                $('#vhsjs_view_590_1539825877_78').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="590_1539825877_78" id="589_1539825877_78" style="display: none"><div class="arrow-slidedown"><blockquote>Energy efficiency has become increasingly important in high performance computing (HPC), as power constraints and costs escalate. Workload and system characteristics form a complex optimization search space in which optimal settings for energy efficiency and performance often diverge. Thus, we must identify trade-off options to find the desired balance. We present an innovative statistical model that accurately predicts the Pareto optimal trade-off options using only user-controllable parameters. Our approach can also tolerate both measurement and model errors. We study model training and validation using several HPC kernels, then with more complex workloads, including AMG and LAMMPS. We can calibrate an accurate model from as few as 12 runs, with prediction error of less than 10%. Our results identify trade-off options allowing up to 40% energy efficiency improvement at the cost of under 20% performance loss. For AMG, we reduce the required sample measurement time from 13 hours to 74 minutes.</blockquote></div></div></div></div><a href="includes/files/pap186s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap421"></a><div class="slot-title">HPL and DGEMM Performance Variability on the Xeon Platinum 8160 Processor</div><div class="slot-authors">John D. McCalpin (University of Texas, Texas Advanced Computing Center)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_592_1539825877_78" onclick="$('#vhsjs_view_592_1539825877_78').hide();
                $('#vhsjs_hide_592_1539825877_78').show();
                $('#591_1539825877_78').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_592_1539825877_78" onclick="$('#591_1539825877_78').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_592_1539825877_78').hide();
                $('#vhsjs_view_592_1539825877_78').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="592_1539825877_78" id="591_1539825877_78" style="display: none"><div class="arrow-slidedown"><blockquote>During initial testing of a large cluster equipped with Xeon Platinum 8160 processors, we observed infrequent, but significant, performance drops in HPL benchmark results. The variability was seen in both single node and multi-node runs, with approximately 0.4% of results more than 10% slower than the median. We were able to reproduce this behavior with a single-socket (24-core) DGEMM benchmark. Performance counter analysis of several thousand DGEMM runs showed that increased DRAM read traffic is the primary driver of increased execution time. Increased DRAM traffic in this benchmark is primarily generated by dramatically elevated snoop filter evictions, which arise due to the interaction of high-order (physical) address bits with the hash used to map addresses across the 24 coherence agents on the processor. These conflicts (and the associated performance variability) were effectively eliminated (for both DGEMM and HPL) by using 1 GiB large pages.</blockquote></div></div></div></div><a href="includes/files/pap421s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, Memory, Networks, Parallel Programming Languages, Libraries, and Models, Power, Programming Systems, Scheduling, Tech Program Reg Pass</span><br /><div class="session-title">Task-Based Programming</div><div class="slot-entry"><a name="pap490"></a><div class="slot-title">Dynamic Tracing: Memoization of Task Graphs for Dynamic Task-Based Runtimes</div><div class="slot-authors">Wonchan Lee (Stanford University), Elliott Slaughter (SLAC National Accelerator Laboratory), Michael Bauer and Sean Treichler (Nvidia Corporation), Todd Warszawski (Stanford University), Michael Garland (Nvidia Corporation), and Alex Aiken (Stanford University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_594_1539825877_78" onclick="$('#vhsjs_view_594_1539825877_78').hide();
                $('#vhsjs_hide_594_1539825877_78').show();
                $('#593_1539825877_78').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_594_1539825877_78" onclick="$('#593_1539825877_78').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_594_1539825877_78').hide();
                $('#vhsjs_view_594_1539825877_78').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="594_1539825877_78" id="593_1539825877_78" style="display: none"><div class="arrow-slidedown"><blockquote>Many recent programming systems for both supercomputing and data center workloads generate task graphs to express computations that run on parallel and distributed machines. Due to the overhead associated with constructing these graphs the dependence analysis that generates them is often statically computed and memoized, and the resulting graph executed repeatedly at runtime. However, many applications require a dynamic dependence analysis due to data dependent behavior, but there are new challenges in capturing and re-executing task graphs at runtime. In this work, we introduce dynamic tracing, a technique to capture a dynamic dependence analysis of a trace that generates a task graph, and replay it. We show that an implementation of dynamic tracing improves strong scaling by an average of 4.9X and up to 7.0X on a suite of already optimized benchmarks.</blockquote></div></div></div></div><a href="includes/files/pap490s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap338"></a><div class="slot-title">Runtime-Assisted Cache Coherence Deactivation in Task Parallel Programs</div><div class="slot-authors">Paul Caheny (Barcelona Supercomputing Center, Polytechnic University of Catalonia); Lluc Alvarez (Barcelona Supercomputing Center); Mateo Valero and Miquel Moretó (Barcelona Supercomputing Center, Polytechnic University of Catalonia); and Marc Casas (Barcelona Supercomputing Center)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_596_1539825877_78" onclick="$('#vhsjs_view_596_1539825877_78').hide();
                $('#vhsjs_hide_596_1539825877_78').show();
                $('#595_1539825877_78').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_596_1539825877_78" onclick="$('#595_1539825877_78').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_596_1539825877_78').hide();
                $('#vhsjs_view_596_1539825877_78').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="596_1539825877_78" id="595_1539825877_78" style="display: none"><div class="arrow-slidedown"><blockquote>With increasing core counts, the scalability of directory-based cache coherence has become a challenging problem. To reduce the area and power needs of the directory, recent proposals reduce its size by classifying data as private or shared, and disable coherence for private data. However, existing classification methods suffer from inaccuracies and require complex hardware support with limited scalability.<br><br>This paper proposes a hardware/software co-designed approach: the runtime system identifies data that is guaranteed by the programming model semantics to not require coherence and notifies the microarchitecture. The microarchitecture deactivates coherence for this private data and powers off unused directory capacity. Our proposal reduces directory accesses to just 26% of the baseline system and supports a 64× smaller directory with only 2.8% performance degradation. By dynamically calibrating the directory size, our proposal saves 86% of dynamic energy consumption in the directory without harming performance.</blockquote></div></div></div></div><a href="includes/files/pap338s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap547"></a><div class="slot-title">A Divide and Conquer Algorithm for DAG Scheduling Under Power Constraints</div><div class="slot-authors">Gökalp Demirci, Ivana Marincic, and Henry Hoffmann (University of Chicago)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_598_1539825877_78" onclick="$('#vhsjs_view_598_1539825877_78').hide();
                $('#vhsjs_hide_598_1539825877_78').show();
                $('#597_1539825877_78').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_598_1539825877_78" onclick="$('#597_1539825877_78').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_598_1539825877_78').hide();
                $('#vhsjs_view_598_1539825877_78').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="598_1539825877_78" id="597_1539825877_78" style="display: none"><div class="arrow-slidedown"><blockquote>We consider the problem of scheduling a parallel computation–represented as a directed acyclic graph (DAG)–on a distributed parallel system with a global resource constraint–specifically a global power budget–and configurable resources, allowing a range of different power/performance tradeoffs. There is a rich body of literature on the independent problems of (1) scheduling DAGs and (2) scheduling independent applications under resource constraints. Very little, however, is known about the combined problem of scheduling DAGs under resource constraints. We present a novel approximation algorithm using a divide-and-conquer method for minimizing application execution time. We prove that the length of the schedule returned by our algorithm is always within O(log n)-factor of the optimum that can be achieved with selection of configurations for the tasks. We implement and test our algorithm on simulations of real application DAGs. We find that our divide-and-conquer method improves performance by up to 75% compared to greedy scheduling algorithms.</blockquote></div></div></div></div><a href="includes/files/pap547s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack189"></a><div class="section-title">Precision</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Architectures, Compiler Analysis and Optimization, Floating Point, Performance, Precision, Programming Systems, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Arithmetic and Optimization</div><div class="slot-entry"><a name="pap431"></a><div class="slot-title">Associative Instruction Reordering to Alleviate Register Pressure</div><div class="slot-authors">Prashant Singh Rawat, Aravind Sukumaran-Rajam, and Atanas Rountev (Ohio State University); Fabrice Rastello (French Institute for Research in Computer Science and Automation (INRIA)); Louis-Noel Pouchet (Colorado State University); and P. Sadayappan (Ohio State University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_600_1539825877_78" onclick="$('#vhsjs_view_600_1539825877_78').hide();
                $('#vhsjs_hide_600_1539825877_78').show();
                $('#599_1539825877_78').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_600_1539825877_78" onclick="$('#599_1539825877_78').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_600_1539825877_78').hide();
                $('#vhsjs_view_600_1539825877_78').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="600_1539825877_78" id="599_1539825877_78" style="display: none"><div class="arrow-slidedown"><blockquote>Register allocation is generally considered a practically solved problem. For most applications, the register allocation strategies in production compilers are very effective in controlling the number of loads/stores and register spills. However, existing register allocation strategies are not effective and result in excessive register spilling for computation patterns with a high degree of many-to-many data reuse, e.g., high-order stencils and tensor contractions.  We develop a source-to-source instruction reordering strategy that exploits the flexibility of reordering associative operations to alleviate register pressure.  The developed transformation module implements an adaptable strategy that can appropriately control the degree of instruction-level parallelism, while relieving register pressure.  The effectiveness of the approach is demonstrated through experimental results using multiple production compilers (GCC, Clang/LLVM) and target platforms (Intel Xeon Phi, and Intel x86 multi-core).</blockquote></div></div></div></div><a href="includes/files/pap431s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap464"></a><div class="slot-title">Harnessing GPU's Tensor Cores Fast FP16 Arithmetic to Speedup Mixed-Precision Iterative Refinement Solvers</div><div class="slot-authors">Azzam Haidar (University of Tennessee, Innovative Computing Laboratory); Stan Tomov and Jack Dongarra (University of Tennessee); and Nicholas Higham (University of Manchester, School of Mathematics)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_602_1539825877_78" onclick="$('#vhsjs_view_602_1539825877_78').hide();
                $('#vhsjs_hide_602_1539825877_78').show();
                $('#601_1539825877_78').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_602_1539825877_78" onclick="$('#601_1539825877_78').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_602_1539825877_78').hide();
                $('#vhsjs_view_602_1539825877_78').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="602_1539825877_78" id="601_1539825877_78" style="display: none"><div class="arrow-slidedown"><blockquote>The use of low-precision arithmetic in computing methods has been a powerful tool to accelerate numerous scientific computing applications including Artificial Intelligence. We present an investigation showing that other HPC applications can harness this power too, and in particular, the general HPC problem of solving Ax = b, where A is a large dense matrix, and the solution is needed in FP64 accuracy. Our approach is based on the mixed-precision (FP16->FP64) iterative refinement technique – we generalize and extend prior advances into a framework, for which we develop architecture-specific algorithms and highly-tuned implementations where we show how the use of FP16-TC (tensor cores) arithmetic can provide up to 4X speedup and improve the energy consumption by a factor of 5 achieving 74 Gflop/Watt. This is due to the performance boost that the FP16 (Tensor Cores) provide and to its better accuracy that outperforms the classical FP16.</blockquote></div></div></div></div><a href="includes/files/pap464s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap503"></a><div class="slot-title">ADAPT: Algorithmic Differentiation Applied to Floating-Point Precision Tuning</div><div class="slot-authors">Harshitha Menon (Lawrence Livermore National Laboratory); Michael O. Lam (James Madison University, Lawrence Livermore National Laboratory); and Daniel Osei-Kuffuor, Markus Schordan, Scott Lloyd, Kathryn Mohror, and Jeffrey Hittinger (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_604_1539825877_78" onclick="$('#vhsjs_view_604_1539825877_78').hide();
                $('#vhsjs_hide_604_1539825877_78').show();
                $('#603_1539825877_78').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_604_1539825877_78" onclick="$('#603_1539825877_78').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_604_1539825877_78').hide();
                $('#vhsjs_view_604_1539825877_78').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="604_1539825877_78" id="603_1539825877_78" style="display: none"><div class="arrow-slidedown"><blockquote>HPC applications extensively use floating point arithmetic operations to solve computational problems in various domains. Mixed precision computing, use of lowest precision data type sufficient to achieve a desired accuracy, have been explored to improve performance, reduce power consumption and data movement. Manually optimizing the program to use mixed precision is challenging. In this work, we present ADAPT, an approach for mixed precision analysis on HPC workloads while providing guarantees about the final output error. Our approach uses algorithmic differentiation to accurately estimate the output error for mixed precision configuration. ADAPT provides floating-point precision sensitivity of programs, which highlights regions of the code that that can potentially be converted to lower precision, is used to make algorithmic choices and develop mixed precision configurations. We evaluate ADAPT on six benchmarks and a proxy application and show that we are able to achieve a speedup of 1.2x on the proxy application, LULESH.</blockquote></div></div></div></div><a href="includes/files/pap503s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack147"></a><div class="section-title">Programming Systems</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Data Analytics, Performance, Programming Systems, Storage, Tools, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Performance Optimization Studies</div><div class="slot-entry"><a name="pap335"></a><div class="slot-title">Many-Core Graph Workload Analysis</div><div class="slot-authors">Stijn Eyerman, Wim Heirman, Kristof Du Bois, Joshua B. Fryman, and Ibrahim Hur (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_606_1539825877_78" onclick="$('#vhsjs_view_606_1539825877_78').hide();
                $('#vhsjs_hide_606_1539825877_78').show();
                $('#605_1539825877_78').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_606_1539825877_78" onclick="$('#605_1539825877_78').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_606_1539825877_78').hide();
                $('#vhsjs_view_606_1539825877_78').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="606_1539825877_78" id="605_1539825877_78" style="display: none"><div class="arrow-slidedown"><blockquote>Graph applications have specific characteristics that are not common in other application domains. In this paper, we analyze multiple graph applications on current multi- and many-core processors and provide conclusions and recommendations for future designs. We provide new insights on executing graph applications on many-core processors.<br><br>Our main novel observations are (i) some memory streams do show locality, while others show no locality, (ii) thread imbalance becomes a major problem with many threads, and (iii) many threads are required to saturate high-bandwidth memories. We recommend a selective memory access policy, where accesses with locality are cached and prefetched, while accesses without locality can remain uncached to save cache capacity. Additionally, more threads are needed, but they are not used efficiently due to thread imbalance. Our recommendation is to revise the graph analysis algorithms to provide more parallelism, and to provide a few high-performance cores that speedup sections with low parallelism.</blockquote></div></div></div></div><a href="includes/files/pap335s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap488"></a><div class="slot-title">Lessons Learned from Analyzing Dynamic Promotion for User-Level Threading</div><div class="slot-authors">Shintaro Iwasaki (University of Tokyo), Abdelhalim Amer (Argonne National Laboratory), Kenjiro Taura (University of Tokyo), and Pavan Balaji (Argonne National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_608_1539825877_78" onclick="$('#vhsjs_view_608_1539825877_78').hide();
                $('#vhsjs_hide_608_1539825877_78').show();
                $('#607_1539825877_78').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_608_1539825877_78" onclick="$('#607_1539825877_78').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_608_1539825877_78').hide();
                $('#vhsjs_view_608_1539825877_78').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="608_1539825877_78" id="607_1539825877_78" style="display: none"><div class="arrow-slidedown"><blockquote>A performance vs. practicality trade-off exists between user-level threading techniques. The community has settled mostly on a black-and-white perspective; fully fledged threads assume that suspension is imminent and incur overheads when suspension does not take place, and run-to-completion threads are more lightweight but less practical since they cannot suspend. Gray areas exist, however, whereby threads can start with minimal capabilities and then can be dynamically promoted to acquire additional capabilities when needed. This paper investigates the full spectrum of threading techniques from a performance vs. practicality trade-off perspective on modern multicore and many-core systems. Our results indicate that achieving the best trade-off highly depends on the suspension likelihood; dynamic promotion is more appropriate when suspension is unlikely and represents a solid replacement for run to completion, thanks to its lower programming constraints, while fully fledged threads remain the technique of choice when suspension likelihood is high.</blockquote></div></div></div></div><a href="includes/files/pap488s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap484"></a><div class="slot-title">Topology-Aware Space-Shared Co-Analysis of Large-Scale Molecular Dynamics Simulations</div><div class="slot-authors">Preeti Malakar (Indian Institute of Technology Kanpur); Todd Munson, Christopher Knight, and Venkatram Vishwanath (Argonne National Laboratory); and Michael E. Papka (Argonne National Laboratory, Northern Illinois University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_610_1539825877_78" onclick="$('#vhsjs_view_610_1539825877_78').hide();
                $('#vhsjs_hide_610_1539825877_78').show();
                $('#609_1539825877_78').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_610_1539825877_78" onclick="$('#609_1539825877_78').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_610_1539825877_78').hide();
                $('#vhsjs_view_610_1539825877_78').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="610_1539825877_78" id="609_1539825877_78" style="display: none"><div class="arrow-slidedown"><blockquote>Analysis of scientific simulation data can be concurrently executed with simulation either in time- or space-shared mode. This mitigates the I/O bottleneck.  However it results in either stalling the simulation for performing the analysis or transferring data for analysis. In this paper, we improve the throughput of space-shared in situ analysis of large-scale simulations by topology-aware mapping and optimal process decomposition. We propose node interconnect topology-aware process placement for simulation and analysis to reduce the data movement time. We also present an integer linear program for optimal 3D decompositions of simulation and analysis processes. We demonstrate our approach using molecular dynamics simulation on Mira, Cori and Theta supercomputers. Our mapping schemes, combined with optimal 3D process decomposition and code optimizations resulted in up to 30% lower execution times for space-shared in situ analysis than the default approach. Our mappings also reduce MPI collective I/O times by 10-40%.</blockquote></div></div></div></div><a href="includes/files/pap484s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, MPI, Networks, Performance, Programming Systems, State of the Practice, Tech Program Reg Pass</span><br /><div class="session-title">MPI Optimization and Characterization</div><div class="slot-entry"><a name="pap504"></a><div class="slot-title">Cooperative Rendezvous Protocols for Improved Performance and Overlap</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">S. Chakraborty, M. Bayatpour, J. Hashmi, H. Subramoni, and D. K. Panda (Ohio State University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_612_1539825877_78" onclick="$('#vhsjs_view_612_1539825877_78').hide();
                $('#vhsjs_hide_612_1539825877_78').show();
                $('#611_1539825877_78').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_612_1539825877_78" onclick="$('#611_1539825877_78').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_612_1539825877_78').hide();
                $('#vhsjs_view_612_1539825877_78').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="612_1539825877_78" id="611_1539825877_78" style="display: none"><div class="arrow-slidedown"><blockquote>With the emergence of larger multi-/many-core clusters, performance of large message communication is becoming more important. MPI libraries use different Rendezvous protocols to perform large message communication. However, existing Rendezvous protocols do not consider the overall communication pattern and make optimal use of the Sender and the Receiver CPUs. In this work, we propose a cooperative Rendezvous protocol that can provide up to 2x improvement in intra-node bandwidth and latency for large messages. We also propose a scheme to dynamically choose the best Rendezvous protocol for each message based on the communication pattern.  Finally, we show how these improvements can increase the overlap of computation with intra-node and inter-node communication, and lead to application level benefits. We evaluate proposed designs on three different architectures including Intel Xeon, Knights Landing, and OpenPOWER with different HPC applications and obtain benefits up to 19% with Graph500, 16% with CoMD, and 10% with MiniGhost.</blockquote></div></div></div></div><a href="includes/files/pap504s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap463"></a><div class="slot-title">Framework for Scalable Intra-Node Collective Operations Using Shared Memory</div><div class="slot-authors">Surabhi Jain, Rashid Kaleem, Marc Gamell Balmana, Akhil Langer, Dmitry Durnov, Alexander Sannikov, and Maria Garzaran (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_614_1539825877_78" onclick="$('#vhsjs_view_614_1539825877_78').hide();
                $('#vhsjs_hide_614_1539825877_78').show();
                $('#613_1539825877_78').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_614_1539825877_78" onclick="$('#613_1539825877_78').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_614_1539825877_78').hide();
                $('#vhsjs_view_614_1539825877_78').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="614_1539825877_78" id="613_1539825877_78" style="display: none"><div class="arrow-slidedown"><blockquote>Collective operations are used in MPI programs to express common communication patterns, collective computations, or synchronizations. In many collectives, such as barrier or allreduce, the intra-node component of the collective is in the critical path, as the inter-node communication cannot start until the intra-node component has been executed. Thus, with increasing number of core counts in each node, intra-node optimizations that leverage the intra-node shared memory become increasingly important.<br><br>In this paper, we focus on the performance benefit of optimizing intra-node collectives using shared memory. We optimize several collectives using the primitives in broadcast and reduce as building blocks for other collectives. A comparison of our implementation on top of MPICH shows significant performance speedups with respect to the original MPICH implementation, MVAPICH, and OpenMPI, among others.</blockquote></div></div></div></div><a href="includes/files/pap463s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap537"></a><div class="slot-title">Characterization of MPI Usage on a Production Supercomputer</div><div class="slot-authors">Sudheer Chunduri, Scott Parker, Pavan Balaji, Kevin Harms, and Kalyan Kumaran (Argonne National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_616_1539825877_78" onclick="$('#vhsjs_view_616_1539825877_78').hide();
                $('#vhsjs_hide_616_1539825877_78').show();
                $('#615_1539825877_78').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_616_1539825877_78" onclick="$('#615_1539825877_78').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_616_1539825877_78').hide();
                $('#vhsjs_view_616_1539825877_78').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="616_1539825877_78" id="615_1539825877_78" style="display: none"><div class="arrow-slidedown"><blockquote>MPI is the most prominent programming model used in scientific computing today.  Despite it's importance, however, how scientific applications use it in production is not very well understood due to the lack of low overhead profiling tools.   We used a lightweight profiling tool, called autoperf, to log the MPI usage characteristics of production applications on a large supercomputing system (Mira) and its corresponding development system (Cetus).  Autoperf limits the amount of information that it records in order to keep the overhead to a minimum while still storing enough data to derive useful insights.  MPI usage statistics have been collected for over 100K jobs that were run within a 2-year period and are analyzed.  The analysis of this data is intended as a mechanism to provide useful insights for MPI developers and network hardware developers for their next generation of improvements, and for supercomputing center operators for their next system procurements.</blockquote></div></div></div></div><a href="includes/files/pap537s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, Memory, Networks, Parallel Programming Languages, Libraries, and Models, Power, Programming Systems, Scheduling, Tech Program Reg Pass</span><br /><div class="session-title">Task-Based Programming</div><div class="slot-entry"><a name="pap490"></a><div class="slot-title">Dynamic Tracing: Memoization of Task Graphs for Dynamic Task-Based Runtimes</div><div class="slot-authors">Wonchan Lee (Stanford University), Elliott Slaughter (SLAC National Accelerator Laboratory), Michael Bauer and Sean Treichler (Nvidia Corporation), Todd Warszawski (Stanford University), Michael Garland (Nvidia Corporation), and Alex Aiken (Stanford University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_618_1539825877_78" onclick="$('#vhsjs_view_618_1539825877_78').hide();
                $('#vhsjs_hide_618_1539825877_78').show();
                $('#617_1539825877_78').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_618_1539825877_78" onclick="$('#617_1539825877_78').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_618_1539825877_78').hide();
                $('#vhsjs_view_618_1539825877_78').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="618_1539825877_78" id="617_1539825877_78" style="display: none"><div class="arrow-slidedown"><blockquote>Many recent programming systems for both supercomputing and data center workloads generate task graphs to express computations that run on parallel and distributed machines. Due to the overhead associated with constructing these graphs the dependence analysis that generates them is often statically computed and memoized, and the resulting graph executed repeatedly at runtime. However, many applications require a dynamic dependence analysis due to data dependent behavior, but there are new challenges in capturing and re-executing task graphs at runtime. In this work, we introduce dynamic tracing, a technique to capture a dynamic dependence analysis of a trace that generates a task graph, and replay it. We show that an implementation of dynamic tracing improves strong scaling by an average of 4.9X and up to 7.0X on a suite of already optimized benchmarks.</blockquote></div></div></div></div><a href="includes/files/pap490s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap338"></a><div class="slot-title">Runtime-Assisted Cache Coherence Deactivation in Task Parallel Programs</div><div class="slot-authors">Paul Caheny (Barcelona Supercomputing Center, Polytechnic University of Catalonia); Lluc Alvarez (Barcelona Supercomputing Center); Mateo Valero and Miquel Moretó (Barcelona Supercomputing Center, Polytechnic University of Catalonia); and Marc Casas (Barcelona Supercomputing Center)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_620_1539825877_86" onclick="$('#vhsjs_view_620_1539825877_86').hide();
                $('#vhsjs_hide_620_1539825877_86').show();
                $('#619_1539825877_86').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_620_1539825877_86" onclick="$('#619_1539825877_86').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_620_1539825877_86').hide();
                $('#vhsjs_view_620_1539825877_86').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="620_1539825877_86" id="619_1539825877_86" style="display: none"><div class="arrow-slidedown"><blockquote>With increasing core counts, the scalability of directory-based cache coherence has become a challenging problem. To reduce the area and power needs of the directory, recent proposals reduce its size by classifying data as private or shared, and disable coherence for private data. However, existing classification methods suffer from inaccuracies and require complex hardware support with limited scalability.<br><br>This paper proposes a hardware/software co-designed approach: the runtime system identifies data that is guaranteed by the programming model semantics to not require coherence and notifies the microarchitecture. The microarchitecture deactivates coherence for this private data and powers off unused directory capacity. Our proposal reduces directory accesses to just 26% of the baseline system and supports a 64× smaller directory with only 2.8% performance degradation. By dynamically calibrating the directory size, our proposal saves 86% of dynamic energy consumption in the directory without harming performance.</blockquote></div></div></div></div><a href="includes/files/pap338s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap547"></a><div class="slot-title">A Divide and Conquer Algorithm for DAG Scheduling Under Power Constraints</div><div class="slot-authors">Gökalp Demirci, Ivana Marincic, and Henry Hoffmann (University of Chicago)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_622_1539825877_86" onclick="$('#vhsjs_view_622_1539825877_86').hide();
                $('#vhsjs_hide_622_1539825877_86').show();
                $('#621_1539825877_86').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_622_1539825877_86" onclick="$('#621_1539825877_86').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_622_1539825877_86').hide();
                $('#vhsjs_view_622_1539825877_86').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="622_1539825877_86" id="621_1539825877_86" style="display: none"><div class="arrow-slidedown"><blockquote>We consider the problem of scheduling a parallel computation–represented as a directed acyclic graph (DAG)–on a distributed parallel system with a global resource constraint–specifically a global power budget–and configurable resources, allowing a range of different power/performance tradeoffs. There is a rich body of literature on the independent problems of (1) scheduling DAGs and (2) scheduling independent applications under resource constraints. Very little, however, is known about the combined problem of scheduling DAGs under resource constraints. We present a novel approximation algorithm using a divide-and-conquer method for minimizing application execution time. We prove that the length of the schedule returned by our algorithm is always within O(log n)-factor of the optimum that can be achieved with selection of configurations for the tasks. We implement and test our algorithm on simulations of real application DAGs. We find that our divide-and-conquer method improves performance by up to 75% compared to greedy scheduling algorithms.</blockquote></div></div></div></div><a href="includes/files/pap547s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Architectures, Compiler Analysis and Optimization, Floating Point, Performance, Precision, Programming Systems, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Arithmetic and Optimization</div><div class="slot-entry"><a name="pap431"></a><div class="slot-title">Associative Instruction Reordering to Alleviate Register Pressure</div><div class="slot-authors">Prashant Singh Rawat, Aravind Sukumaran-Rajam, and Atanas Rountev (Ohio State University); Fabrice Rastello (French Institute for Research in Computer Science and Automation (INRIA)); Louis-Noel Pouchet (Colorado State University); and P. Sadayappan (Ohio State University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_624_1539825877_86" onclick="$('#vhsjs_view_624_1539825877_86').hide();
                $('#vhsjs_hide_624_1539825877_86').show();
                $('#623_1539825877_86').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_624_1539825877_86" onclick="$('#623_1539825877_86').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_624_1539825877_86').hide();
                $('#vhsjs_view_624_1539825877_86').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="624_1539825877_86" id="623_1539825877_86" style="display: none"><div class="arrow-slidedown"><blockquote>Register allocation is generally considered a practically solved problem. For most applications, the register allocation strategies in production compilers are very effective in controlling the number of loads/stores and register spills. However, existing register allocation strategies are not effective and result in excessive register spilling for computation patterns with a high degree of many-to-many data reuse, e.g., high-order stencils and tensor contractions.  We develop a source-to-source instruction reordering strategy that exploits the flexibility of reordering associative operations to alleviate register pressure.  The developed transformation module implements an adaptable strategy that can appropriately control the degree of instruction-level parallelism, while relieving register pressure.  The effectiveness of the approach is demonstrated through experimental results using multiple production compilers (GCC, Clang/LLVM) and target platforms (Intel Xeon Phi, and Intel x86 multi-core).</blockquote></div></div></div></div><a href="includes/files/pap431s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap464"></a><div class="slot-title">Harnessing GPU's Tensor Cores Fast FP16 Arithmetic to Speedup Mixed-Precision Iterative Refinement Solvers</div><div class="slot-authors">Azzam Haidar (University of Tennessee, Innovative Computing Laboratory); Stan Tomov and Jack Dongarra (University of Tennessee); and Nicholas Higham (University of Manchester, School of Mathematics)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_626_1539825877_86" onclick="$('#vhsjs_view_626_1539825877_86').hide();
                $('#vhsjs_hide_626_1539825877_86').show();
                $('#625_1539825877_86').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_626_1539825877_86" onclick="$('#625_1539825877_86').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_626_1539825877_86').hide();
                $('#vhsjs_view_626_1539825877_86').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="626_1539825877_86" id="625_1539825877_86" style="display: none"><div class="arrow-slidedown"><blockquote>The use of low-precision arithmetic in computing methods has been a powerful tool to accelerate numerous scientific computing applications including Artificial Intelligence. We present an investigation showing that other HPC applications can harness this power too, and in particular, the general HPC problem of solving Ax = b, where A is a large dense matrix, and the solution is needed in FP64 accuracy. Our approach is based on the mixed-precision (FP16->FP64) iterative refinement technique – we generalize and extend prior advances into a framework, for which we develop architecture-specific algorithms and highly-tuned implementations where we show how the use of FP16-TC (tensor cores) arithmetic can provide up to 4X speedup and improve the energy consumption by a factor of 5 achieving 74 Gflop/Watt. This is due to the performance boost that the FP16 (Tensor Cores) provide and to its better accuracy that outperforms the classical FP16.</blockquote></div></div></div></div><a href="includes/files/pap464s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap503"></a><div class="slot-title">ADAPT: Algorithmic Differentiation Applied to Floating-Point Precision Tuning</div><div class="slot-authors">Harshitha Menon (Lawrence Livermore National Laboratory); Michael O. Lam (James Madison University, Lawrence Livermore National Laboratory); and Daniel Osei-Kuffuor, Markus Schordan, Scott Lloyd, Kathryn Mohror, and Jeffrey Hittinger (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_628_1539825877_86" onclick="$('#vhsjs_view_628_1539825877_86').hide();
                $('#vhsjs_hide_628_1539825877_86').show();
                $('#627_1539825877_86').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_628_1539825877_86" onclick="$('#627_1539825877_86').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_628_1539825877_86').hide();
                $('#vhsjs_view_628_1539825877_86').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="628_1539825877_86" id="627_1539825877_86" style="display: none"><div class="arrow-slidedown"><blockquote>HPC applications extensively use floating point arithmetic operations to solve computational problems in various domains. Mixed precision computing, use of lowest precision data type sufficient to achieve a desired accuracy, have been explored to improve performance, reduce power consumption and data movement. Manually optimizing the program to use mixed precision is challenging. In this work, we present ADAPT, an approach for mixed precision analysis on HPC workloads while providing guarantees about the final output error. Our approach uses algorithmic differentiation to accurately estimate the output error for mixed precision configuration. ADAPT provides floating-point precision sensitivity of programs, which highlights regions of the code that that can potentially be converted to lower precision, is used to make algorithmic choices and develop mixed precision configurations. We evaluate ADAPT on six benchmarks and a proxy application and show that we are able to achieve a speedup of 1.2x on the proxy application, LULESH.</blockquote></div></div></div></div><a href="includes/files/pap503s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Linear Algebra, Memory, MPI, OpenMP, Programming Systems, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Programming Systems Tools</div><div class="slot-entry"><a name="pap179"></a><div class="slot-title">Dynamic Data Race Detection for OpenMP Programs</div><div class="slot-authors">Yizi Gu and John Mellor-Crummey (Rice University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_630_1539825877_86" onclick="$('#vhsjs_view_630_1539825877_86').hide();
                $('#vhsjs_hide_630_1539825877_86').show();
                $('#629_1539825877_86').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_630_1539825877_86" onclick="$('#629_1539825877_86').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_630_1539825877_86').hide();
                $('#vhsjs_view_630_1539825877_86').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="630_1539825877_86" id="629_1539825877_86" style="display: none"><div class="arrow-slidedown"><blockquote>Two concurrent accesses to a shared variable that are unordered by synchronization are said to be a data race if at least one access is a write. Data races cause shared memory parallel programs to behave unpredictably. This paper describes ROMP -- a tool for detecting data races in executions of scalable parallel applications that employ OpenMP for node-level parallelism. The complexity of OpenMP, which includes primitives for managing data environments, SPMD and SIMD parallelism, work sharing, tasking, mutual exclusion, and ordering, presents a formidable challenge for data race detection. ROMP is a hybrid data race detector that tracks accesses, access orderings, and mutual exclusion. Unlike other OpenMP race detectors, ROMP detects races with respect to logical parallelism rather than implementation threads. Experiments show that ROMP yields precise race reports for a broader set of OpenMP constructs than prior state-of-the-art race detectors.</blockquote></div></div></div></div><a href="includes/files/pap179s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap256"></a><div class="slot-title">ParSy: Inspection and Transformation of Sparse Matrix Computations for Parallelism</div><div class="slot-authors">Kazem Cheshmi (University of Toronto), Shoaib Kamil (Adobe Research), Michelle Mills Strout (University of Arizona), and Maryam Mehri Dehnavi (University of Toronto)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_632_1539825877_86" onclick="$('#vhsjs_view_632_1539825877_86').hide();
                $('#vhsjs_hide_632_1539825877_86').show();
                $('#631_1539825877_86').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_632_1539825877_86" onclick="$('#631_1539825877_86').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_632_1539825877_86').hide();
                $('#vhsjs_view_632_1539825877_86').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="632_1539825877_86" id="631_1539825877_86" style="display: none"><div class="arrow-slidedown"><blockquote>In this work, we describe ParSy, a framework that uses a novel inspection strategy along with a simple code transformation to optimize parallel sparse algorithms for shared memory processors. Unlike existing approaches that can suffer from load imbalance and excessive synchronization, ParSy uses a novel task coarsening strategy to create well-balanced tasks that can execute in parallel, while maintaining locality of memory accesses. Code using the ParSy inspector and transformation outperforms existing highly-optimized sparse matrix algorithms such as Cholesky factorization on multi-core processors with speedups of 2.8× and 3.1× over the MKL Pardiso and PaStiX libraries respectively.</blockquote></div></div></div></div><a href="includes/files/pap256s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap382"></a><div class="slot-title">Detecting MPI Usage Anomalies via Partial Program Symbolic Execution</div><div class="slot-authors">Fangke Ye, Jisheng Zhao, and Vivek Sarkar (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_634_1539825877_86" onclick="$('#vhsjs_view_634_1539825877_86').hide();
                $('#vhsjs_hide_634_1539825877_86').show();
                $('#633_1539825877_86').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_634_1539825877_86" onclick="$('#633_1539825877_86').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_634_1539825877_86').hide();
                $('#vhsjs_view_634_1539825877_86').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="634_1539825877_86" id="633_1539825877_86" style="display: none"><div class="arrow-slidedown"><blockquote>MPI is a message passing based programming model for distributed-memory parallelism that has been had been widely used for programming supercomputers for over 25 years. However, debugging and verification of MPI programs is widely recognized to be a deep technical challenge. This challenge is further exacerbated by a recent increase in the use of nonblocking MPI operations that bring new classes of bugs related to data races.<br><br>In this paper, we introduce a new MPI program debugging approach based on partial symbolic execution so as to avoid the false alarms inherent in the static analysis based methodology. Compared with the dynamic approach, our approach can be applied to incomplete programs and explore multiple execution paths, thereby bringing more flexibility and precision. By comparing with well known static/dynamic tools on real-world MPI applications, our approach shows same precision as the dynamic tool and avoids false positive produced by the static tool.</blockquote></div></div></div></div><a href="includes/files/pap382s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Applications, Cosmology, Data Analytics, Deep Learning, Machine Learning, Programming Systems, Storage, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Deep Learning</div><div class="slot-entry"><a name="pap425"></a><div class="slot-title">Exploring Flexible Communications for Streamlining DNN Ensemble Training Pipelines</div><div class="slot-authors">Randall Pittman, Hui Guan, and Xipeng Shen (North Carolina State University) and Seung-Hwan Lim and Robert M. Patton (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_636_1539825877_86" onclick="$('#vhsjs_view_636_1539825877_86').hide();
                $('#vhsjs_hide_636_1539825877_86').show();
                $('#635_1539825877_86').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_636_1539825877_86" onclick="$('#635_1539825877_86').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_636_1539825877_86').hide();
                $('#vhsjs_view_636_1539825877_86').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="636_1539825877_86" id="635_1539825877_86" style="display: none"><div class="arrow-slidedown"><blockquote>Parallel training of a Deep Neural Network (DNN) ensemble on a cluster of nodes is a common practice to train multiple models in order to construct a model with a higher prediction accuracy. Existing ensemble training pipelines can perform a great deal of redundant operations, resulting in unnecessary CPU usage, or even poor pipeline performance.  In order to remove these redundancies, we need pipelines with more communication flexibility than existing DNN frameworks provide.<br><br>This project investigates a series of designs to improve pipeline flexibility and adaptivity, while also increasing performance. We implement our designs using Tensorflow with Horovod, and test it using several large DNNs. Our results show that the CPU time spent during training is reduced by 2-11X. Furthermore, our implementation can achieve up to 10X speedups when CPU core limits are imposed. Our best pipeline also reduces the average power draw of the ensemble training process by 5-16%.</blockquote></div></div></div></div><a href="includes/files/pap425s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap429"></a><div class="slot-title">CosmoFlow: Using Deep Learning to Learn the Universe at Scale</div><div class="slot-authors">Amrita Mathuriya (Intel Corporation); Deborah Bard (National Energy Research Scientific Computing Center (NERSC), Lawrence Berkeley National Laboratory); Pete Mendygral (Cray Inc); Lawrence Meadows (Intel Corporation); James Arnemann (University of California, Berkeley); Lei Shao (Intel Corporation); Siyu He (Carnegie Mellon University); Tuomas Karna (Intel Corporation); Diana Moise (Cray Inc); Simon J. Pennycook (Intel Corporation); Kristyn Maschhoff (Cray Inc); Jason Sewall and Nalini Kumar (Intel Corporation); Shirley Ho (Lawrence Berkeley National Laboratory, Carnegie Mellon University); Michael F. Ringenburg (Cray Inc); Mr Prabhat (Lawrence Berkeley National Laboratory, National Energy Research Scientific Computing Center (NERSC)); and Victor Lee (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_638_1539825877_87" onclick="$('#vhsjs_view_638_1539825877_87').hide();
                $('#vhsjs_hide_638_1539825877_87').show();
                $('#637_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_638_1539825877_87" onclick="$('#637_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_638_1539825877_87').hide();
                $('#vhsjs_view_638_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="638_1539825877_87" id="637_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>Deep learning is a promising tool to determine the physical model that describes our universe.   To handle the considerable computational cost of this problem, we present CosmoFlow: a highly scalable deep learning application built on top of the TensorFlow framework.<br><br>CosmoFlow uses efficient implementations of 3D convolution and pooling primitives, together with improvements in threading for many element-wise operations, to improve training performance on Intel Xeon Phi processors.  We also utilize the Cray PE Machine Learning Plugin for efficient scaling to multiple nodes. We demonstrate fully synchronous data-parallel training on 8192 nodes of Cori with 77% parallel efficiency, achieving 3.5 Pflop/s sustained performance. <br><br>To our knowledge, this is the first large-scale science application of the TensorFlow framework at supercomputer scale with fully-synchronous training. These enhancements enable us to process large 3D dark matter distribution and predict the cosmological parameters Omega_M, sigma_8 and N_s with unprecedented accuracy.</blockquote></div></div></div></div><a href="includes/files/pap429s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap322"></a><div class="slot-title">Anatomy of High-Performance Deep Learning Convolutions on SIMD Architectures</div><div class="slot-authors">Evangelos Georganas, Sasikanth Avancha, Kunal Banerjee, Dhiraj Kalamkar, Greg Henry, Hans Pabst, and Alexander Heinecke (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_640_1539825877_87" onclick="$('#vhsjs_view_640_1539825877_87').hide();
                $('#vhsjs_hide_640_1539825877_87').show();
                $('#639_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_640_1539825877_87" onclick="$('#639_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_640_1539825877_87').hide();
                $('#vhsjs_view_640_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="640_1539825877_87" id="639_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>Convolution layers are prevalent in many classes of deep neural networks, including Convolutional Neural Networks (CNNs) which provide state-of-the-art results for tasks like image recognition, neural machine translation, and speech recognition. The computationally expensive nature of a convolution operation has led to the proliferation of implementations including matrix-matrix multiplication formulation, and direct convolution primarily targeting GPUs. In this paper, we introduce direct convolution kernels for x86 architectures, in particular for Xeon and Xeon Phi systems, which are implemented via a dynamic compilation approach. Our JIT-based implementation shows close to theoretical peak performance, depending on the setting and the CPU architecture at hand. We additionally demonstrate how these JIT-optimized kernels can be integrated into a light-weight multi-node graph execution model. This illustrates that single- and multi-node runs yield high efficiencies and high image-throughputs  when executing state of the art image recognition tasks on CPUs.</blockquote></div></div></div></div><a href="includes/files/pap322s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack151"></a><div class="section-title">Resiliency</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">GPUs, Resiliency, State of the Practice, System Software, Tech Program Reg Pass</span><br /><div class="session-title">Resilience</div><div class="slot-entry"><a name="pap262"></a><div class="slot-title">GPU Age-Aware Scheduling to Improve the Reliability of Leadership Jobs on Titan</div><div class="slot-authors">Christopher Zimmer, Don Maxwell, Stephen McNally, Scott Atchley, and Sudharshan S. Vazhkudai (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_642_1539825877_87" onclick="$('#vhsjs_view_642_1539825877_87').hide();
                $('#vhsjs_hide_642_1539825877_87').show();
                $('#641_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_642_1539825877_87" onclick="$('#641_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_642_1539825877_87').hide();
                $('#vhsjs_view_642_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="642_1539825877_87" id="641_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>The increasing rate of failures on the Oak Ridge Leadership Computing Facility's (OLCF) Titan supercomputer, resulted in the replacement of 50% of its GPUs between 2015 and 2017. The largest jobs, also known as "leadership jobs'', continued to experience increased application failures. These jobs contained significant amounts of low-failure rate and high-failure rate GPUs. The impacts of these failures were felt more by leadership jobs due to longer wait times, runtimes, and higher charge rates. In this work, we have designed techniques to increase the use of low-failure GPUs in leadership jobs through targeted resource allocation. This employed two complementary techniques, updating both the system ordering and the allocation mechanisms. In simulation, the application of these techniques resulted in a 33% increase in low-failure GPU hours being assigned to leadership jobs. Our GPU Age-Aware Scheduling has been used in production on Titan since July of 2017.</blockquote></div></div></div></div><a href="includes/files/pap262s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap109"></a><div class="slot-title">FlipTracker: Understanding Natural Error Resilience in HPC Applications</div><div class="slot-authors">Luanzheng Guo and Dong Li (University of California, Merced); Ignacio Laguna (Lawrence Livermore National Laboratory); and Martin Schulz (Technical University Munich)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_644_1539825877_87" onclick="$('#vhsjs_view_644_1539825877_87').hide();
                $('#vhsjs_hide_644_1539825877_87').show();
                $('#643_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_644_1539825877_87" onclick="$('#643_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_644_1539825877_87').hide();
                $('#vhsjs_view_644_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="644_1539825877_87" id="643_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>As high-performance computing systems scale in size and computational power, the danger of silent errors, i.e., errors that can bypass hardware detection mechanisms and impact application state, grows dramatically. Consequently, applications running on HPC systems need to exhibit resilience to such errors. Previous work has found that, for certain codes, this resilience can come for free, i.e., some applications are naturally resilient, but few works have shown the code patterns—combinations or sequences of computations—that make an application naturally resilient. In this paper, we present FlipTracker, a framework designed to extract these patterns using fine-grained tracking of error propagation and resilience properties, and we use it to present a set of computation patterns that are responsible for making representative HPC applications naturally resilient to errors. This not only enables a deeper understanding of resilience properties of these codes, but also can guide future application designs toward patterns with natural resilience.</blockquote></div></div></div></div><a href="includes/files/pap109s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap111"></a><div class="slot-title">Doomsday: Predicting Which Node Will Fail When on Supercomputers</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Anwesha Das and Frank Mueller (North Carolina State University) and Paul Hargrove, Eric Roman, and Scott Baden (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_646_1539825877_87" onclick="$('#vhsjs_view_646_1539825877_87').hide();
                $('#vhsjs_hide_646_1539825877_87').show();
                $('#645_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_646_1539825877_87" onclick="$('#645_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_646_1539825877_87').hide();
                $('#vhsjs_view_646_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="646_1539825877_87" id="645_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>Predicting which node will fail and how soon remains a challenge for HPC resilience, yet may pave the way to exploiting proactive remedies before jobs fail. Not only for increasing scalability up to exascale systems, but even for contemporary supercomputer architectures does it require substantial efforts to distill anomalous events from noisy raw logs. To this end, we propose a novel phrase extraction mechanism called TBP (time-based phrases) to pin-point node failures, which is unprecedented.  Our study, based on real system data and statistical machine learning, demonstrates the feasibility to predict which specific node will fail in Cray systems. TBP achieves no less than 83% recall rates with lead times as high as 2 minutes. This opens up the door for enhancing prediction lead times for supercomputing systems in general, thereby facilitating efficient usage of both computing capacity and power in large scale production systems.</blockquote></div></div></div></div><a href="includes/files/pap111s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Performance, Resiliency, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Resilience II</div><div class="slot-entry"><a name="pap392"></a><div class="slot-title">Lessons Learned from Memory Errors Observed Over the Lifetime of Cielo</div><div class="slot-authors">Scott Levy and Kurt B. Ferreira (Sandia National Laboratories), Nathan DeBardeleben (Los Alamos National Laboratory), Taniya Siddiqua and Vilas Sridharan (Advanced Micro Devices Inc), and Elisabeth Baseman (Los Alamos National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_648_1539825877_87" onclick="$('#vhsjs_view_648_1539825877_87').hide();
                $('#vhsjs_hide_648_1539825877_87').show();
                $('#647_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_648_1539825877_87" onclick="$('#647_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_648_1539825877_87').hide();
                $('#vhsjs_view_648_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="648_1539825877_87" id="647_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>Maintaining the performance of high-performance computing (HPC) applications as failures increase is a major challenge for next-generation extreme-scale systems. Recent research demonstrates that hardware failures are expected to become more common due to increased component counts, reduced device-feature sizes, and tightly-constrained power budgets. Few existing studies, however, have examined failures in the context of the entire lifetime of a single platform. In this paper, we analyze failure data collected over the entire lifetime of Cielo, a leadership-class HPC system. Our analysis reveals several key findings, including: (i) Cielo’s memory (DRAM and SRAM) exhibited no discernible aging effects; (ii) correctable memory faults are not predictive of future uncorrectable memory faults; (iii) developing more comprehensive logging facilities will improve failure analysis on future machines; (iv) continued advances will be required to ensure current failure mitigation techniques remain a viable option for future platforms.</blockquote></div></div></div></div><a href="includes/files/pap392s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap381"></a><div class="slot-title">Partial Redundancy in HPC Systems with Non-Uniform Node Reliabilities</div><div class="slot-authors">Zaeem Hussain, Taieb Znati, and Rami Melhem (University of Pittsburgh)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_650_1539825877_87" onclick="$('#vhsjs_view_650_1539825877_87').hide();
                $('#vhsjs_hide_650_1539825877_87').show();
                $('#649_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_650_1539825877_87" onclick="$('#649_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_650_1539825877_87').hide();
                $('#vhsjs_view_650_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="650_1539825877_87" id="649_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>We study the usefulness of partial redundancy in HPC message passing systems where individual node failure distributions are not identical. Prior research works on fault tolerance have generally assumed identical failure distributions for the nodes of the system. In such settings, partial replication has never been shown to outperform the two extremes (full and no-replication) for any significant range of node counts. We argue that partial redundancy may provide the best performance under the more realistic assumption of non-identical node failure distributions. We provide theoretical results on arranging nodes with different reliability values among replicas such that system reliability is maximized. Moreover, using system reliability to compute MTTI (mean-time-to-interrupt) and expected completion time of a partially replicated system, we numerically determine the optimal partial replication degree. Our results indicate that partial replication can be a more efficient alternative to full replication at system scales where Checkpoint/Restart alone is not sufficient.</blockquote></div></div></div></div><a href="includes/files/pap381s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap386"></a><div class="slot-title">Evaluating and Accelerating High-Fidelity Error Injection for HPC</div><div class="slot-authors">Chun-Kai Chang, Sangkug Lym, and Nicholas Kelly (University of Texas); Michael B. Sullivan (Nvidia Corporation); and Mattan Erez (University of Texas)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_652_1539825877_87" onclick="$('#vhsjs_view_652_1539825877_87').hide();
                $('#vhsjs_hide_652_1539825877_87').show();
                $('#651_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_652_1539825877_87" onclick="$('#651_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_652_1539825877_87').hide();
                $('#vhsjs_view_652_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="652_1539825877_87" id="651_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>We address two important concerns in the analysis of the behavior of applications in the presence of hardware errors: (1) when is it important to model how hardware faults lead to erroneous values (instruction-level errors) with high fidelity, as opposed to using simple bit-flipping models, and (2) how to enable fast high-fidelity error injection campaigns, in particular when error detectors are employed. We present and verify a new nested Monte Carlo methodology for evaluating high-fidelity gate-level fault models and error-detector coverage, which is orders of magnitude faster than current approaches. We use that methodology to demonstrate that, without detectors, simple error models suffice for evaluating errors in 9 HPC benchmarks.</blockquote></div></div></div></div><a href="includes/files/pap386s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, GPUs, Linear Algebra, Networks, Resiliency, Tech Program Reg Pass</span><br /><div class="session-title">Resilience III: GPUs</div><div class="slot-entry"><a name="pap247"></a><div class="slot-title">Optimizing Software-Directed Instruction Replication for GPU Error Detection</div><div class="slot-authors">Abdulrahman Mahmoud (University of Illinois) and Siva Kumar Sastry Hari, Michael B. Sullivan, Timothy Tsai, and Stephen W. Keckler (Nvidia Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_654_1539825877_87" onclick="$('#vhsjs_view_654_1539825877_87').hide();
                $('#vhsjs_hide_654_1539825877_87').show();
                $('#653_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_654_1539825877_87" onclick="$('#653_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_654_1539825877_87').hide();
                $('#vhsjs_view_654_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="654_1539825877_87" id="653_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>Application execution on safety-critical and high-performance computer systems must be resilient to transient errors. As GPUs become more pervasive in such systems, they must supplement ECC/parity for major storage structures with reliability techniques that cover more of the GPU hardware logic.  Instruction duplication has been explored for CPU resilience; however, it has never been studied in the context of GPUs, and it is unclear whether the performance and design choices it presents makes it a feasible GPU solution. This paper describes a practical methodology to employ instruction duplication for GPUs and identifies implementation challenges that can incur high overheads (69% on average). It explores GPU-specific software optimizations that trade fine-grained recoverability for performance. It also proposes simple ISA extensions with limited hardware changes and area costs to further improve performance, cutting the runtime overheads by more than half to an average of 30%.</blockquote></div></div></div></div><a href="includes/files/pap247s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap244"></a><div class="slot-title">Fault Tolerant One-Sided Matrix Decompositions on Heterogeneous Systems with GPUs</div><div class="slot-authors">Jieyang Chen, Hongbo Li, Sihuan Li, and Xin Liang (University of California, Riverside); Panruo Wu (University of Houston); Dingwen Tao (University of Alabama); Kaiming Ouyang, Yuanlai Liu, and Kai Zhao (University of California, Riverside); Qiang Guan (Kent State University); and Zizhong Chen (University of California, Riverside)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_656_1539825877_87" onclick="$('#vhsjs_view_656_1539825877_87').hide();
                $('#vhsjs_hide_656_1539825877_87').show();
                $('#655_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_656_1539825877_87" onclick="$('#655_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_656_1539825877_87').hide();
                $('#vhsjs_view_656_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="656_1539825877_87" id="655_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>Current algorithm-based fault tolerance (ABFT) approach for one-sided matrix decomposition on heterogeneous systems with GPUs have following limitations: (1) they do not provide sufficient protection as most of them only maintain checksum in one dimension; (2) their checking scheme is not efficient due to redundant checksum verifications; (3) they fail to protect PCIe communication; (4) the checksum calculation based on a special type of matrix multiplication is far from efficient. By overcoming the above limitations, we design an efficient ABFT approach providing stronger protection for one-sided matrix decomposition methods on heterogeneous systems. First, we provide full matrix protection by using checksums in two dimensions. Second, our checking scheme is more efficient by prioritizing the checksum verification according to the sensitivity of matrix operations to soft errors. Third, we protect PCIe communication by reordering checksum verifications and decomposition steps. Fourth, we accelerate the checksum calculation by 1.7x via better utilizing GPUs.</blockquote></div></div></div></div><a href="includes/files/pap244s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap430"></a><div class="slot-title">PRISM: Predicting Resilience of GPU Applications Using Statistical Methods</div><div class="slot-authors">Charu Kalra, Fritz Previlon, and Xiangyu Li (Northeastern University); Norman Rubin (Nvidia Corporation); and David Kaeli (Northeastern University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_658_1539825877_87" onclick="$('#vhsjs_view_658_1539825877_87').hide();
                $('#vhsjs_hide_658_1539825877_87').show();
                $('#657_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_658_1539825877_87" onclick="$('#657_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_658_1539825877_87').hide();
                $('#vhsjs_view_658_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="658_1539825877_87" id="657_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>As Graphics Processing Units (GPUs) become more pervasive in HPC and safety-critical domains, ensuring that GPU applications can be protected from data corruption grows in importance. Despite prior efforts to mitigate errors, we still lack a clear understanding of how resilient these applications are in the presence of transient faults.  Due to the random nature of these faults, predicting whether they will alter the program output is a challenging problem. In this paper, we build a framework named PRISM, which uses a systematic approach to predict failures in GPU programs. PRISM extracts micro-architecture agnostic features to characterize program resiliency, which serve as predictors in our statistical model. PRISM enables us to predict failures in applications without running exhaustive fault-injection campaigns on a GPU, thereby reducing the error estimation effort. PRISM can also be used to gain insight into potential architectural support required to improve the reliability of GPU applications.</blockquote></div></div></div></div><a href="includes/files/pap430s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack185"></a><div class="section-title">Resource Management</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Networks, Resource Management, Scheduling, State of the Practice, System Software, Tech Program Reg Pass</span><br /><div class="session-title">Resource Management and Interference</div><div class="slot-entry"><a name="pap360"></a><div class="slot-title">RM-Replay: A High-Fidelity Tuning, Optimization and Exploration Tool for Resource Management</div><div class="slot-authors">Maxime Martinasso, Miguel Gila, Mauro Bianco, Sadaf R. Alam, Colin McMurtrie, and Thomas C. Schulthess (Swiss National Supercomputing Centre)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_660_1539825877_87" onclick="$('#vhsjs_view_660_1539825877_87').hide();
                $('#vhsjs_hide_660_1539825877_87').show();
                $('#659_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_660_1539825877_87" onclick="$('#659_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_660_1539825877_87').hide();
                $('#vhsjs_view_660_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="660_1539825877_87" id="659_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>Leading hybrid and heterogeneous supercomputing systems process hundreds of thousands of jobs using complex scheduling algorithms and parameters. The centers operating these systems aim to achieve higher levels of resource utilization while being restricted by compliance with policy constraints. There is a critical need for a high-fidelity, high-performance tool with familiar interfaces that allows not only tuning and optimization of the operational job scheduler but also enables exploration of new resource management algorithms. We propose a new methodology and a tool called RM-Replay which is not a simulator but instead a fast replay engine for production workloads. Slurm is used as a platform to demonstrate the capabilities of our replay engine.<br><br>The tool accuracy is discussed and our investigation shows that, by providing better job runtime estimation or using topology-aware allocation, scheduling metric values vary. The presented methodology to create fast replay engines can be extended to other complex systems.</blockquote></div></div></div></div><a href="includes/files/pap360s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap541"></a><div class="slot-title">Evaluation of an Interference-Free Node Allocation Policy on Fat-Tree Clusters</div><div class="slot-authors">Samuel D. Pollard (University of Oregon) and Nikhil Jain, Stephen Herbein, and Abhinav Bhatele (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_662_1539825877_87" onclick="$('#vhsjs_view_662_1539825877_87').hide();
                $('#vhsjs_hide_662_1539825877_87').show();
                $('#661_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_662_1539825877_87" onclick="$('#661_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_662_1539825877_87').hide();
                $('#vhsjs_view_662_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="662_1539825877_87" id="661_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>Interference between jobs competing for network bandwidth on a fat-tree cluster can cause significant variability and degradation in performance. These performance issues can be mitigated or completely eliminated if the resource allocation policy takes the network topology into account when allocating nodes to jobs. We implement a fat-tree network topology aware node allocation policy that allocates isolated partitions to jobs in order to eliminate inter-job interference. We compare the impact of this node allocation policy to a topology-oblivious policy with respect to the execution time of individual jobs with different communication patterns. We also evaluate the cluster's quality of service using metrics such as system utilization, schedule makespan, and job wait time for both policies. The results obtained for production workloads indicate that a topology-aware node allocation can provide interference-free execution without negatively impacting the cluster's quality of service.</blockquote></div></div></div></div><a href="includes/files/pap541s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap311"></a><div class="slot-title">Mitigating Inter-Job Interference Using Adaptive Flow-Aware Routing</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Staci A. Smith, Clara E. Cromey, and David K. Lowenthal (University of Arizona); Jens Domke (Tokyo Institute of Technology); and Nikhil Jain, Jayaraman J. Thiagarajan, and Abhinav Bhatele (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_664_1539825877_87" onclick="$('#vhsjs_view_664_1539825877_87').hide();
                $('#vhsjs_hide_664_1539825877_87').show();
                $('#663_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_664_1539825877_87" onclick="$('#663_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_664_1539825877_87').hide();
                $('#vhsjs_view_664_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="664_1539825877_87" id="663_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>On most high performance computing platforms, applications share network resources with other jobs running concurrently on the system.  Inter-job network interference can have a significant impact on the performance of communication-intensive applications, and no satisfactory solutions yet exist for mitigating this degradation.<br><br>In this paper, we analyze network congestion caused by multi-job workloads on two production systems that use popular network topologies---fat-tree and dragonfly. For each system, we establish a regression model to relate network hotspots to application performance degradation, showing that current routing strategies are insufficient to load-balance network traffic and mitigate interference on production systems.  We then propose an alternative type of adaptive routing strategy, which we call adaptive flow-aware routing.  We implement a prototype of our strategy, and tests on the fat-tree system show up to a 46% improvement in job run time when compared to the default routing.</blockquote></div></div></div></div><a href="includes/files/pap311s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Clouds and Distributed Computing, Resource Management, Scheduling, Tech Program Reg Pass</span><br /><div class="session-title">Clouds and Distributed Computing</div><div class="slot-entry"><a name="pap229"></a><div class="slot-title">A Reference Architecture for Datacenter Scheduling: Design, Validation, and Experiments</div><div class="slot-authors">Georgios Andreadis (Delft University of Technology, Vrije University Amsterdam); Laurens Versluis (Vrije University Amsterdam); Fabian Mastenbroek (Delft University of Technology); and Alexandru Iosup (Vrije University Amsterdam, Delft University of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_666_1539825877_87" onclick="$('#vhsjs_view_666_1539825877_87').hide();
                $('#vhsjs_hide_666_1539825877_87').show();
                $('#665_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_666_1539825877_87" onclick="$('#665_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_666_1539825877_87').hide();
                $('#vhsjs_view_666_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="666_1539825877_87" id="665_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>Datacenters act as cloud-infrastructure to stakeholders across industry, government, and academia. To meet growing demand yet operate efficiently, datacenter operators employ increasingly more sophisticated scheduling systems, mechanisms, and policies. Although many scheduling techniques already exist, relatively little research has gone into the abstraction of the scheduling process itself, hampering design, tuning, and comparison of existing techniques. In this work, we propose a reference architecture for datacenter schedulers. The architecture follows five design principles: components with clearly distinct responsibilities, grouping of related components where possible, separation of mechanism from policy, scheduling as complex workflow, and hierarchical multi-scheduler structure. To demonstrate the validity of the reference architecture, we map to it state-of-the-art datacenter schedulers. We find scheduler-stages are commonly underspecified in peer-reviewed publications. Through trace-based simulation and real-world experiments, we show underspecification of scheduler-stages can lead to significant variations in performance.</blockquote></div></div></div></div><a href="includes/files/pap229s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap356"></a><div class="slot-title">Dynamically Negotiating Capacity Between On-Demand and Batch Clusters</div><div class="slot-authors">Feng Liu (University of Minnesota), Kate Keahey (Argonne National Laboratory), Pierre Riteau (University of Chicago), and Jon Weissman (University of Minnesota)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_668_1539825877_87" onclick="$('#vhsjs_view_668_1539825877_87').hide();
                $('#vhsjs_hide_668_1539825877_87').show();
                $('#667_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_668_1539825877_87" onclick="$('#667_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_668_1539825877_87').hide();
                $('#vhsjs_view_668_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="668_1539825877_87" id="667_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>In the era of rapid experimental expansion data analysis needs are rapidly outpacing the capabilities of small institutional clusters and looking to integrate HPC resources into their workflow. We propose one way of reconciling on-demand needs of experimental analytics with the batch managed HPC resources within a system that dynamically moves nodes between an on-demand cluster configured with cloud technology (OpenStack) and a traditional HPC cluster managed by a batch scheduler (Torque). We evaluate this system experimentally both in the context of real-life traces representing two years of a specific institutional need, and via experiments in the context of synthetic traces that capture generalized characteristics of potential batch and on-demand workloads. Our results for the real-life scenario show that our approach could reduce the current investment in on-demand infrastructure by 82% while at the same time improving the mean batch wait time almost by an order of magnitude (8x).</blockquote></div></div></div></div><a href="includes/files/pap356s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap365"></a><div class="slot-title">A Lightweight Model for Right-Sizing Master-Worker Applications</div><div class="slot-authors">Nathaniel Kremer-Herman, Benjamin Tovar, and Douglas Thain (University of Notre Dame)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_670_1539825877_87" onclick="$('#vhsjs_view_670_1539825877_87').hide();
                $('#vhsjs_hide_670_1539825877_87').show();
                $('#669_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_670_1539825877_87" onclick="$('#669_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_670_1539825877_87').hide();
                $('#vhsjs_view_670_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="670_1539825877_87" id="669_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>When running a parallel application at scale, a resource provisioning policy should minimize over-commitment (idle resources) and under-commitment (resource contention). However, users seldom know the quantity of resources to appropriately execute their application. Even with such knowledge, over- and under-commitment of resources may still occur because the application does not run in isolation. It shares resources  such as network and filesystems.<br><br>We formally define the capacity of a parallel application as the quantity of resources that may effectively be provisioned for the best  execution time in an environment.  We present a model to compute an estimate of the capacity of master-worker applications as they run based on execution and data-transfer times. We demonstrate this model with two bioinformatics workflows, a machine learning application, and one synthetic application.  Our results show the model correctly tracks the known value of capacity in scaling,  dynamic task behavior, and with improvements in task throughput.</blockquote></div></div></div></div><a href="includes/files/pap365s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack172"></a><div class="section-title">Scheduling</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Networks, Resource Management, Scheduling, State of the Practice, System Software, Tech Program Reg Pass</span><br /><div class="session-title">Resource Management and Interference</div><div class="slot-entry"><a name="pap360"></a><div class="slot-title">RM-Replay: A High-Fidelity Tuning, Optimization and Exploration Tool for Resource Management</div><div class="slot-authors">Maxime Martinasso, Miguel Gila, Mauro Bianco, Sadaf R. Alam, Colin McMurtrie, and Thomas C. Schulthess (Swiss National Supercomputing Centre)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_672_1539825877_87" onclick="$('#vhsjs_view_672_1539825877_87').hide();
                $('#vhsjs_hide_672_1539825877_87').show();
                $('#671_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_672_1539825877_87" onclick="$('#671_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_672_1539825877_87').hide();
                $('#vhsjs_view_672_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="672_1539825877_87" id="671_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>Leading hybrid and heterogeneous supercomputing systems process hundreds of thousands of jobs using complex scheduling algorithms and parameters. The centers operating these systems aim to achieve higher levels of resource utilization while being restricted by compliance with policy constraints. There is a critical need for a high-fidelity, high-performance tool with familiar interfaces that allows not only tuning and optimization of the operational job scheduler but also enables exploration of new resource management algorithms. We propose a new methodology and a tool called RM-Replay which is not a simulator but instead a fast replay engine for production workloads. Slurm is used as a platform to demonstrate the capabilities of our replay engine.<br><br>The tool accuracy is discussed and our investigation shows that, by providing better job runtime estimation or using topology-aware allocation, scheduling metric values vary. The presented methodology to create fast replay engines can be extended to other complex systems.</blockquote></div></div></div></div><a href="includes/files/pap360s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap541"></a><div class="slot-title">Evaluation of an Interference-Free Node Allocation Policy on Fat-Tree Clusters</div><div class="slot-authors">Samuel D. Pollard (University of Oregon) and Nikhil Jain, Stephen Herbein, and Abhinav Bhatele (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_674_1539825877_87" onclick="$('#vhsjs_view_674_1539825877_87').hide();
                $('#vhsjs_hide_674_1539825877_87').show();
                $('#673_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_674_1539825877_87" onclick="$('#673_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_674_1539825877_87').hide();
                $('#vhsjs_view_674_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="674_1539825877_87" id="673_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>Interference between jobs competing for network bandwidth on a fat-tree cluster can cause significant variability and degradation in performance. These performance issues can be mitigated or completely eliminated if the resource allocation policy takes the network topology into account when allocating nodes to jobs. We implement a fat-tree network topology aware node allocation policy that allocates isolated partitions to jobs in order to eliminate inter-job interference. We compare the impact of this node allocation policy to a topology-oblivious policy with respect to the execution time of individual jobs with different communication patterns. We also evaluate the cluster's quality of service using metrics such as system utilization, schedule makespan, and job wait time for both policies. The results obtained for production workloads indicate that a topology-aware node allocation can provide interference-free execution without negatively impacting the cluster's quality of service.</blockquote></div></div></div></div><a href="includes/files/pap541s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap311"></a><div class="slot-title">Mitigating Inter-Job Interference Using Adaptive Flow-Aware Routing</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Staci A. Smith, Clara E. Cromey, and David K. Lowenthal (University of Arizona); Jens Domke (Tokyo Institute of Technology); and Nikhil Jain, Jayaraman J. Thiagarajan, and Abhinav Bhatele (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_676_1539825877_87" onclick="$('#vhsjs_view_676_1539825877_87').hide();
                $('#vhsjs_hide_676_1539825877_87').show();
                $('#675_1539825877_87').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_676_1539825877_87" onclick="$('#675_1539825877_87').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_676_1539825877_87').hide();
                $('#vhsjs_view_676_1539825877_87').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="676_1539825877_87" id="675_1539825877_87" style="display: none"><div class="arrow-slidedown"><blockquote>On most high performance computing platforms, applications share network resources with other jobs running concurrently on the system.  Inter-job network interference can have a significant impact on the performance of communication-intensive applications, and no satisfactory solutions yet exist for mitigating this degradation.<br><br>In this paper, we analyze network congestion caused by multi-job workloads on two production systems that use popular network topologies---fat-tree and dragonfly. For each system, we establish a regression model to relate network hotspots to application performance degradation, showing that current routing strategies are insufficient to load-balance network traffic and mitigate interference on production systems.  We then propose an alternative type of adaptive routing strategy, which we call adaptive flow-aware routing.  We implement a prototype of our strategy, and tests on the fat-tree system show up to a 46% improvement in job run time when compared to the default routing.</blockquote></div></div></div></div><a href="includes/files/pap311s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, Memory, Networks, Parallel Programming Languages, Libraries, and Models, Power, Programming Systems, Scheduling, Tech Program Reg Pass</span><br /><div class="session-title">Task-Based Programming</div><div class="slot-entry"><a name="pap490"></a><div class="slot-title">Dynamic Tracing: Memoization of Task Graphs for Dynamic Task-Based Runtimes</div><div class="slot-authors">Wonchan Lee (Stanford University), Elliott Slaughter (SLAC National Accelerator Laboratory), Michael Bauer and Sean Treichler (Nvidia Corporation), Todd Warszawski (Stanford University), Michael Garland (Nvidia Corporation), and Alex Aiken (Stanford University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_678_1539825877_88" onclick="$('#vhsjs_view_678_1539825877_88').hide();
                $('#vhsjs_hide_678_1539825877_88').show();
                $('#677_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_678_1539825877_88" onclick="$('#677_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_678_1539825877_88').hide();
                $('#vhsjs_view_678_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="678_1539825877_88" id="677_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>Many recent programming systems for both supercomputing and data center workloads generate task graphs to express computations that run on parallel and distributed machines. Due to the overhead associated with constructing these graphs the dependence analysis that generates them is often statically computed and memoized, and the resulting graph executed repeatedly at runtime. However, many applications require a dynamic dependence analysis due to data dependent behavior, but there are new challenges in capturing and re-executing task graphs at runtime. In this work, we introduce dynamic tracing, a technique to capture a dynamic dependence analysis of a trace that generates a task graph, and replay it. We show that an implementation of dynamic tracing improves strong scaling by an average of 4.9X and up to 7.0X on a suite of already optimized benchmarks.</blockquote></div></div></div></div><a href="includes/files/pap490s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap338"></a><div class="slot-title">Runtime-Assisted Cache Coherence Deactivation in Task Parallel Programs</div><div class="slot-authors">Paul Caheny (Barcelona Supercomputing Center, Polytechnic University of Catalonia); Lluc Alvarez (Barcelona Supercomputing Center); Mateo Valero and Miquel Moretó (Barcelona Supercomputing Center, Polytechnic University of Catalonia); and Marc Casas (Barcelona Supercomputing Center)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_680_1539825877_88" onclick="$('#vhsjs_view_680_1539825877_88').hide();
                $('#vhsjs_hide_680_1539825877_88').show();
                $('#679_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_680_1539825877_88" onclick="$('#679_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_680_1539825877_88').hide();
                $('#vhsjs_view_680_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="680_1539825877_88" id="679_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>With increasing core counts, the scalability of directory-based cache coherence has become a challenging problem. To reduce the area and power needs of the directory, recent proposals reduce its size by classifying data as private or shared, and disable coherence for private data. However, existing classification methods suffer from inaccuracies and require complex hardware support with limited scalability.<br><br>This paper proposes a hardware/software co-designed approach: the runtime system identifies data that is guaranteed by the programming model semantics to not require coherence and notifies the microarchitecture. The microarchitecture deactivates coherence for this private data and powers off unused directory capacity. Our proposal reduces directory accesses to just 26% of the baseline system and supports a 64× smaller directory with only 2.8% performance degradation. By dynamically calibrating the directory size, our proposal saves 86% of dynamic energy consumption in the directory without harming performance.</blockquote></div></div></div></div><a href="includes/files/pap338s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap547"></a><div class="slot-title">A Divide and Conquer Algorithm for DAG Scheduling Under Power Constraints</div><div class="slot-authors">Gökalp Demirci, Ivana Marincic, and Henry Hoffmann (University of Chicago)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_682_1539825877_88" onclick="$('#vhsjs_view_682_1539825877_88').hide();
                $('#vhsjs_hide_682_1539825877_88').show();
                $('#681_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_682_1539825877_88" onclick="$('#681_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_682_1539825877_88').hide();
                $('#vhsjs_view_682_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="682_1539825877_88" id="681_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>We consider the problem of scheduling a parallel computation–represented as a directed acyclic graph (DAG)–on a distributed parallel system with a global resource constraint–specifically a global power budget–and configurable resources, allowing a range of different power/performance tradeoffs. There is a rich body of literature on the independent problems of (1) scheduling DAGs and (2) scheduling independent applications under resource constraints. Very little, however, is known about the combined problem of scheduling DAGs under resource constraints. We present a novel approximation algorithm using a divide-and-conquer method for minimizing application execution time. We prove that the length of the schedule returned by our algorithm is always within O(log n)-factor of the optimum that can be achieved with selection of configurations for the tasks. We implement and test our algorithm on simulations of real application DAGs. We find that our divide-and-conquer method improves performance by up to 75% compared to greedy scheduling algorithms.</blockquote></div></div></div></div><a href="includes/files/pap547s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Clouds and Distributed Computing, Resource Management, Scheduling, Tech Program Reg Pass</span><br /><div class="session-title">Clouds and Distributed Computing</div><div class="slot-entry"><a name="pap229"></a><div class="slot-title">A Reference Architecture for Datacenter Scheduling: Design, Validation, and Experiments</div><div class="slot-authors">Georgios Andreadis (Delft University of Technology, Vrije University Amsterdam); Laurens Versluis (Vrije University Amsterdam); Fabian Mastenbroek (Delft University of Technology); and Alexandru Iosup (Vrije University Amsterdam, Delft University of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_684_1539825877_88" onclick="$('#vhsjs_view_684_1539825877_88').hide();
                $('#vhsjs_hide_684_1539825877_88').show();
                $('#683_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_684_1539825877_88" onclick="$('#683_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_684_1539825877_88').hide();
                $('#vhsjs_view_684_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="684_1539825877_88" id="683_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>Datacenters act as cloud-infrastructure to stakeholders across industry, government, and academia. To meet growing demand yet operate efficiently, datacenter operators employ increasingly more sophisticated scheduling systems, mechanisms, and policies. Although many scheduling techniques already exist, relatively little research has gone into the abstraction of the scheduling process itself, hampering design, tuning, and comparison of existing techniques. In this work, we propose a reference architecture for datacenter schedulers. The architecture follows five design principles: components with clearly distinct responsibilities, grouping of related components where possible, separation of mechanism from policy, scheduling as complex workflow, and hierarchical multi-scheduler structure. To demonstrate the validity of the reference architecture, we map to it state-of-the-art datacenter schedulers. We find scheduler-stages are commonly underspecified in peer-reviewed publications. Through trace-based simulation and real-world experiments, we show underspecification of scheduler-stages can lead to significant variations in performance.</blockquote></div></div></div></div><a href="includes/files/pap229s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap356"></a><div class="slot-title">Dynamically Negotiating Capacity Between On-Demand and Batch Clusters</div><div class="slot-authors">Feng Liu (University of Minnesota), Kate Keahey (Argonne National Laboratory), Pierre Riteau (University of Chicago), and Jon Weissman (University of Minnesota)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_686_1539825877_88" onclick="$('#vhsjs_view_686_1539825877_88').hide();
                $('#vhsjs_hide_686_1539825877_88').show();
                $('#685_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_686_1539825877_88" onclick="$('#685_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_686_1539825877_88').hide();
                $('#vhsjs_view_686_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="686_1539825877_88" id="685_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>In the era of rapid experimental expansion data analysis needs are rapidly outpacing the capabilities of small institutional clusters and looking to integrate HPC resources into their workflow. We propose one way of reconciling on-demand needs of experimental analytics with the batch managed HPC resources within a system that dynamically moves nodes between an on-demand cluster configured with cloud technology (OpenStack) and a traditional HPC cluster managed by a batch scheduler (Torque). We evaluate this system experimentally both in the context of real-life traces representing two years of a specific institutional need, and via experiments in the context of synthetic traces that capture generalized characteristics of potential batch and on-demand workloads. Our results for the real-life scenario show that our approach could reduce the current investment in on-demand infrastructure by 82% while at the same time improving the mean batch wait time almost by an order of magnitude (8x).</blockquote></div></div></div></div><a href="includes/files/pap356s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap365"></a><div class="slot-title">A Lightweight Model for Right-Sizing Master-Worker Applications</div><div class="slot-authors">Nathaniel Kremer-Herman, Benjamin Tovar, and Douglas Thain (University of Notre Dame)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_688_1539825877_88" onclick="$('#vhsjs_view_688_1539825877_88').hide();
                $('#vhsjs_hide_688_1539825877_88').show();
                $('#687_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_688_1539825877_88" onclick="$('#687_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_688_1539825877_88').hide();
                $('#vhsjs_view_688_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="688_1539825877_88" id="687_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>When running a parallel application at scale, a resource provisioning policy should minimize over-commitment (idle resources) and under-commitment (resource contention). However, users seldom know the quantity of resources to appropriately execute their application. Even with such knowledge, over- and under-commitment of resources may still occur because the application does not run in isolation. It shares resources  such as network and filesystems.<br><br>We formally define the capacity of a parallel application as the quantity of resources that may effectively be provisioned for the best  execution time in an environment.  We present a model to compute an estimate of the capacity of master-worker applications as they run based on execution and data-transfer times. We demonstrate this model with two bioinformatics workflows, a machine learning application, and one synthetic application.  Our results show the model correctly tracks the known value of capacity in scaling,  dynamic task behavior, and with improvements in task throughput.</blockquote></div></div></div></div><a href="includes/files/pap365s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack155"></a><div class="section-title">Scientific Computing</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Computational Biology, Scientific Computing, Tech Program Reg Pass</span><br /><div class="session-title">Biology Applications</div><div class="slot-entry"><a name="pap410"></a><div class="slot-title">Extreme Scale De Novo Metagenome Assembly</div><div><span class="BP award">Best Paper Finalists</span></div><div class="slot-authors">Evangelos Georganas (Intel Corporation) and Rob Egan, Steven Hofmeyr, Eugene Goltsman, Bill Arndt, Andrew Tritt, Aydin Buluc, Leonid Oliker, and Katherine Yelick (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_690_1539825877_88" onclick="$('#vhsjs_view_690_1539825877_88').hide();
                $('#vhsjs_hide_690_1539825877_88').show();
                $('#689_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_690_1539825877_88" onclick="$('#689_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_690_1539825877_88').hide();
                $('#vhsjs_view_690_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="690_1539825877_88" id="689_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>Metagenome assembly is the process of transforming a set of short, overlapping, and potentially erroneous DNA segments from environmental samples into the accurate representation of the underlying microbiomes's genomes. State-of-the-art tools require large shared memory machines and cannot handle contemporary metagenome datasets that exceed terabytes in size. In this paper, we introduce the metaHipMer pipeline, a high-quality and high-performance metagenome assembler that employs an iterative de Bruijn graph approach. MetaHipMer leverages a specialized scaffolding algorithm that produces long scaffolds and accommodates the idiosyncrasies of metagenomes. MetaHipMer is end-to-end parallelized using the Unified Parallel C language and therefore can run seamlessly on shared and distributed-memory systems. Experimental results show that metaHipMer matches or outperforms the state-of-the-art tools in terms of accuracy. Moreover, metaHipMer scales efficiently to large concurrencies and is able to assemble previously intractable grand challenge metagenomes.</blockquote></div></div></div></div><a href="includes/files/pap410s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap551"></a><div class="slot-title">Optimizing High Performance Distributed Memory Parallel Hash Tables for DNA k-mer Counting</div><div class="slot-authors">Tony C. Pan (Georgia Institute of Technology, School of Computational Science and Engineering); Sanchit Misra (Intel Corporation, Parallel Computing Lab); and Srinivas Aluru (Georgia Institute of Technology, School of Computational Science and Engineering)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_692_1539825877_88" onclick="$('#vhsjs_view_692_1539825877_88').hide();
                $('#vhsjs_hide_692_1539825877_88').show();
                $('#691_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_692_1539825877_88" onclick="$('#691_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_692_1539825877_88').hide();
                $('#vhsjs_view_692_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="692_1539825877_88" id="691_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>High-throughput DNA sequencing is the mainstay of modern genomics research. A common operation used in bioinformatic analysis for many applications of high-throughput sequencing is the counting and indexing of fixed length substrings of DNA sequences called k-mers. Counting k-mers is often accomplished via hashing, and distributed memory k-mer counting algorithms for large data sets are memory access and network communication bound. In this work, we present two optimized distributed parallel hash table techniques that utilize cache friendly algorithms for local hashing, overlapped communication and computation to hide communication costs, and vectorized hash functions that are specialized for k-mer and other short key indices. On 4096 cores of the NERSC Cori supercomputer, our implementation completed index construction and query on an approximately 1 TB human genome dataset in just 11.8 seconds and 5.8 seconds, demonstrating speedups of 2.06x and 3.7x, respectively, over the previous state-of-the-art distributed memory k-mer counter.</blockquote></div></div></div></div><a href="includes/files/pap551s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap120"></a><div class="slot-title">Redesigning LAMMPS for Petascale and Hundred-Billion-Atom Simulation on Sunway TaihuLight</div><div class="slot-authors">Xiaohui Duan, Ping Gao, Tingjian Zhang, Meng Zhang, and Weiguo Liu (Shandong University); Wusheng Zhang, Wei Xue, Haohuan Fu, Lin Gan, and Dexun Chen (Tsinghua University); Xiangxu Meng (Shandong University); and Guangwen Yang (Tsinghua University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_694_1539825877_88" onclick="$('#vhsjs_view_694_1539825877_88').hide();
                $('#vhsjs_hide_694_1539825877_88').show();
                $('#693_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_694_1539825877_88" onclick="$('#693_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_694_1539825877_88').hide();
                $('#vhsjs_view_694_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="694_1539825877_88" id="693_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>Large-scale molecular dynamics (MD) simulations on supercomputers play an increasingly important role in many research areas. In this paper, we present our efforts on redesigning the widely used LAMMPS MD simulator for Sunway TaihuLight supercomputer and its ShenWei many-core architecture (SW26010). The memory constraints of SW26010 bring a number of new challenges for achieving efficient MD implementation on it. In order to overcome these constraints, we employ four levels of optimization: (1) a hybrid memory update strategy; (2) a software cache strategy; (3) customized transcendental math functions; and (4) a full pipeline acceleration. Furthermore, we redesign the code to enable all possible vectorization. Experiments show that our redesigned software on a single SW26010 processor can outperform over 100 E5-2650 cores for running the latest stable release (11Aug17) of LAMMPS. We also achieve a performance of over 2.43 PFlops for a Tersoff simulation when using 16,384 nodes on Sunway TaihuLight.</blockquote></div></div></div></div><a href="includes/files/pap120s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, Data Analytics, Deep Learning, Networks, Scientific Computing, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Large-Scale Algorithms</div><div class="slot-entry"><a name="pap171"></a><div class="slot-title">Large-Scale Hierarchical K-Means for Heterogeneous Many-Core Supercomputers</div><div class="slot-authors">Liandeng Li (Tsinghua University; National Supercomputing Center, Wuxi); Teng Yu (University of St Andrews); Wenlai Zhao and Haohuan Fu (Tsinghua University; National Supercomputing Center, Wuxi); Chenyu Wang (University of St Andrews; National Supercomputing Center, Wuxi); Li Tan (Beijing Technology and Business University); Guangwen Yang (Tsinghua University; National Supercomputing Center, Wuxi); and John Thomson (University of St Andrews)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_696_1539825877_88" onclick="$('#vhsjs_view_696_1539825877_88').hide();
                $('#vhsjs_hide_696_1539825877_88').show();
                $('#695_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_696_1539825877_88" onclick="$('#695_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_696_1539825877_88').hide();
                $('#vhsjs_view_696_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="696_1539825877_88" id="695_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>This paper presents a novel design and implementation of k-means clustering algorithm targeting the Sunway TaihuLight supercomputer. We introduce a multi-level parallel partition approach that not only partitions by dataflow and centroid, but also by dimension. Our multi-level (nkd) approach unlocks the potential of the hierarchical parallelism in the SW26010 heterogeneous many-core processor and the system architecture of the supercomputer. <br><br>Our design is able to process large-scale clustering problems with up to 196,608 dimensions and over 160,000 targeting centroids, while maintaining high performance and high scalability, significantly improving the capability of k-means over previous approaches. The evaluation shows our implementation achieves performance of less than 18 seconds per iteration for a large-scale clustering case with 196,608 data dimensions and 2,000 centroids by applying 4,096 nodes (1,064,496 cores) in parallel, making k-means a more feasible solution for complex scenarios.</blockquote></div></div></div></div><a href="includes/files/pap171s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap140"></a><div class="slot-title">TriCore: Parallel Triangle Counting on GPUs</div><div class="slot-authors">Yang Hu (George Washington University); Hang Liu (University of Massachusetts, Lowell); and H. Howie Huang (George Washington University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_698_1539825877_88" onclick="$('#vhsjs_view_698_1539825877_88').hide();
                $('#vhsjs_hide_698_1539825877_88').show();
                $('#697_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_698_1539825877_88" onclick="$('#697_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_698_1539825877_88').hide();
                $('#vhsjs_view_698_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="698_1539825877_88" id="697_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>Triangle counting algorithm enumerates the triangles in a graph by identifying the common neighbors between two vertices of every edge. In this work, we present TriCore, a new GPU-based high-performance and scalable triangle counting system that consists of three main techniques. First, we design a binary search based counting algorithm that tremendously increases both thread parallelism and memory performance. Second, TriCore exploits a 2-D partition method to distribute the CSR representation across multiple GPUs, combined with a new streaming buffer to load the edge list from outside of GPUs. Third, we develop a dynamic workload management technique to balance the workload across multiple GPUs. Our evaluation demonstrates TriCore is 22× faster than the state-of-the-art parallel triangle counting projects. In addition, TriCore can not only process big graphs that are significant larger than the memory size of one GPU but also achieve 24× speedup when scaling to 32 GPUs.</blockquote></div></div></div></div><a href="includes/files/pap140s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap141"></a><div class="slot-title">Distributed-Memory Hierarchical Compression of Dense SPD Matrices</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Chenhan D. Yu (University of Texas), Severin Reiz (Technical University Munich), and George Biros (University of Texas)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_700_1539825877_88" onclick="$('#vhsjs_view_700_1539825877_88').hide();
                $('#vhsjs_hide_700_1539825877_88').show();
                $('#699_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_700_1539825877_88" onclick="$('#699_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_700_1539825877_88').hide();
                $('#vhsjs_view_700_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="700_1539825877_88" id="699_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>We present a distributed-memory algorithm for the hierarchical compression of SPD matrices. Our method is based on GOFMM, an algorithm that appeared in doi:10.1145/3126908.3126921.<br><br>For many SPD matrices, GOFMM enables compression and approximate matrix-vector multiplication in NlogN time---as opposed to quadratic work required for a dense matrix. But GOFMM supports only shared memory parallelism. In this paper, we use the message passing interface, extending the ideas of GOFMM to the distributed memory setting. We also introduce an asynchronous algorithm for faster multiplication. We present different usage scenarios of SPD matrices that are related to graphs, neural-networks, and covariance operators. We also compare with STRUMPACK, which, to our knowledge, is the only other parallel software that can compress arbitrary SPD matrices. In our largest run, we were able to compress a 67M-by-67M matrix within three minutes and perform a multiplication with 512 vectors within 5 seconds on 6,144 Intel Skylake cores.</blockquote></div></div></div></div><a href="includes/files/pap141s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Computational Physics, Scientific Computing, Tech Program Reg Pass</span><br /><div class="session-title">Physics and Tensor Applications</div><div class="slot-entry"><a name="pap502"></a><div class="slot-title">Simulating the Wenchuan Earthquake with Accurate Surface Topography on Sunway TaihuLight</div><div class="slot-authors">Bingwei Chen, Haohuan Fu, Yanwen Wei, and Conghui He (Tsinghua University; National Supercomputing Center, Wuxi); Wenqiang Zhang (University of Science and Technology of China); Yuxuan Li (Tsinghua University; National Supercomputing Center, Wuxi); Wubin Wan and Wei Zhang (National Supercomputing Center, Wuxi); Lin Gan (Tsinghua University; National Supercomputing Center, Wuxi); Wei Zhang and Zhenguo Zhang (Southern University of Science and Technology, China); Guangwen Yang (Tsinghua University; National Supercomputing Center, Wuxi); and Xiaofei Chen (Southern University of Science and Technology, China)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_702_1539825877_88" onclick="$('#vhsjs_view_702_1539825877_88').hide();
                $('#vhsjs_hide_702_1539825877_88').show();
                $('#701_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_702_1539825877_88" onclick="$('#701_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_702_1539825877_88').hide();
                $('#vhsjs_view_702_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="702_1539825877_88" id="701_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>This paper reports our efforts on performing 50-m resolution earthquake simulation of the Wenchuan Earthquake (Ms 8.0, China) on Sunway TaihuLight. To accurately capture the surface topography, we adopt a curvilinear grid finite-difference method with a traction image free surface implementation and redesign the algorithm to reduce memory access costs for heterogeneous many-core architectures. We then derive a performance model of our algorithm to guide and drive the further optimization and tuning of various parameters using a genetic algorithm. A data layout transformation is also proposed to improve the direct memory access (DMA) efficiency further. Our efforts improve the simulation efficiency from 0.05% to 7.6%, with a sustained performance of 9.07 Pflops using the entire machine of the Sunway TaihuLight (over 10 million cores), and a large-scale simulation of the Wenchuan earthquake with accurate surface topography and improved coda wave effects.</blockquote></div></div></div></div><a href="includes/files/pap502s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap506"></a><div class="slot-title">Accelerating Quantum Chemistry with Vectorized and Batched Integrals</div><div class="slot-authors">Hua Huang and Edmond Chow (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_704_1539825877_88" onclick="$('#vhsjs_view_704_1539825877_88').hide();
                $('#vhsjs_hide_704_1539825877_88').show();
                $('#703_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_704_1539825877_88" onclick="$('#703_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_704_1539825877_88').hide();
                $('#vhsjs_view_704_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="704_1539825877_88" id="703_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>This paper presents the first quantum chemistry calculations using a recently developed vectorized library for computing electron repulsion integrals. To lengthen the SIMD loop and thus improve SIMD utilization, the approach used in this paper is to batch together the computation of multiple integrals that have the same code path. The standard approach is to compute integrals one at a time, and thus a batching procedure had to be developed. This paper shows proof-of-concept and demonstrates the performance gains possible when the batched approach is used. Batching also enables certain optimizations when the integrals are used to compute the Fock matrix. We further describe several other optimizations that were needed to obtain up to a 270% speedup over the no batching version of the code, making a compelling case for adopting the presented techniques in quantum chemistry software.</blockquote></div></div></div></div><a href="includes/files/pap506s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap133"></a><div class="slot-title">High-Performance Dense Tucker Decomposition on GPU Clusters</div><div class="slot-authors">Jee Choi (IBM), Xing Liu (Intel Corporation), and Venkatesan Chakaravarthy (IBM)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_706_1539825877_88" onclick="$('#vhsjs_view_706_1539825877_88').hide();
                $('#vhsjs_hide_706_1539825877_88').show();
                $('#705_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_706_1539825877_88" onclick="$('#705_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_706_1539825877_88').hide();
                $('#vhsjs_view_706_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="706_1539825877_88" id="705_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>The Tucker decomposition method is one of the most popular algorithms for analyzing and compressing data with multi-way relationship. Its execution time is typically dominated by dense matrix multiplication, which makes it well-suited for GPU acceleration. State-of-the-art distributed dense Tucker implementations for CPU clusters adopt multi-dimensional partitioning that optimizes for storage and communication. This, however, leads to smaller matrix dimensions that result in under-utilizing the GPU. <br><br>In this paper, we present our optimized implementation and performance analysis of dense Tucker decomposition on a multi-GPU cluster. We propose three optimizations: a new partitioning strategy that improves GPU performance, a new tensor matricization layout that halves the number of communication/matricization steps, and a variation of the randomized SVD algorithm to overcome the eigenvalue bottleneck that arises from the high speedups gained from GPU acceleration.  Our GPU implementation employing all three optimizations achieves up to 11.8x speedup on 64 nodes over state-of-the-art TuckerMPI.</blockquote></div></div></div></div><a href="includes/files/pap133s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Networks, Performance, Scientific Computing, State of the Practice, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Large Scale System Deployments</div><div class="slot-entry"><a name="pap277"></a><div class="slot-title">The Design, Deployment, and Evaluation of the CORAL Pre-Exascale Systems</div><div class="slot-authors">Sudharshan S. Vazhkudai (Oak Ridge National Laboratory); Bronis R. de Supinski (Lawrence Livermore National Laboratory); Arthur S. Bland and Al Geist (Oak Ridge National Laboratory); James Sexton and Jim Kahle (IBM); Christopher J. Zimmer, Scott Atchley, Sarp H. Oral, Don E. Maxwell, and Veronica G. Vergara Larrea (Oak Ridge National Laboratory); Adam Bertsch and Robin Goldstone (Lawrence Livermore National Laboratory); Wayne Joubert (Oak Ridge National Laboratory); Chris Chambreau (Lawrence Livermore National Laboratory); David Appelhans and Robert Blackmore (IBM); Ben Casses (Lawrence Livermore National Laboratory); George Chochia and Gene Davison (IBM); Matthew A. Ezell (Oak Ridge National Laboratory); Tom Gooding (IBM); Elsa Gonsiorowski (Lawrence Livermore National Laboratory); Leopold Grinberg, Bill Hanson, and Bill Hartner (IBM); Ian Karlin and Matthew L. Leininger (Lawrence Livermore National Laboratory); Dustin Leverman (Oak Ridge National Laboratory); Chris Marroquin (IBM); Adam Moody (Lawrence Livermore National Laboratory); Martin Ohmacht (IBM); Ramesh Pankajakshan (Lawrence Livermore National Laboratory); Fernando Pizzano (IBM); James H. Rogers (Oak Ridge National Laboratory); Bryan Rosenburg (IBM); Drew Schmidt, Mallikarjun Shankar, and Feiyi Wang (Oak Ridge National Laboratory); Py Watson (Lawrence Livermore National Laboratory); Bob Walkup (IBM); Lance D. Weems (Lawrence Livermore National Laboratory); and Junqi Yin (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_708_1539825877_88" onclick="$('#vhsjs_view_708_1539825877_88').hide();
                $('#vhsjs_hide_708_1539825877_88').show();
                $('#707_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_708_1539825877_88" onclick="$('#707_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_708_1539825877_88').hide();
                $('#vhsjs_view_708_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="708_1539825877_88" id="707_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>CORAL, the Collaboration of Oak Ridge, Argonne and Livermore, is fielding two similar IBM systems, Summit and Sierra, with NVIDIA GPUs that will replace the existing Titan and Sequoia systems. Summit and Sierra are currently ranked No. 1 and No. 3, respectively, on the Top500 list. We discuss the design and key differences of the systems. Our evaluation of the systems highlights the following. Applications that fit in HBM see the most benefit and may prefer more GPUs; however, for some applications, the CPU-GPU bandwidth is more important than the number of GPUs. The node-local burst buffer scales linearly, and can achieve a 4X improvement over the parallel file system for large jobs; smaller jobs, however, may benefit from writing directly to the PFS. Finally, several CPU, network and memory bound analytics and GPU-bound deep learning codes achieve up to a 11X and 79X speedup/node, respectively over Titan.</blockquote></div></div></div></div><a href="includes/files/pap277s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap449"></a><div class="slot-title">Best Practices and Lessons from Deploying and Operating a Sustained-Petascale System: The Blue Waters Experience</div><div class="slot-authors">Gregory H. Bauer, Brett Bode, Jeremy Enos, William T. Kramer, Scott Lathrop, Celso L. Mendes, and Roberto R. Sisneros (University of Illinois, National Center for Supercomputing Applications)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_710_1539825877_88" onclick="$('#vhsjs_view_710_1539825877_88').hide();
                $('#vhsjs_hide_710_1539825877_88').show();
                $('#709_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_710_1539825877_88" onclick="$('#709_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_710_1539825877_88').hide();
                $('#vhsjs_view_710_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="710_1539825877_88" id="709_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>Building and operating versatile extreme-scale computing systems that work productively for a range of frontier research domains present many challenges and opportunities. Solutions created, experiences acquired, and lessons learned, while rarely published, could drive the development of new methods and practices and raise the bar for all organizations supporting research, scholarship, and education. This paper describes the methods and procedures developed for deploying, supporting, and continuously improving the Blue Waters system and its services during the last five years. Being the first US sustained-petascale computing platform available to the open-science community, the Blue Waters project pioneered various unique practices that we are sharing to be adopted and further improved by the community. We present our support and service methodologies, and the leadership practices employed for ensuring that the system stays highly efficient and productive. We also provide the return on investment summaries related to deploying and operating the system.</blockquote></div></div></div></div><a href="includes/files/pap449s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap346"></a><div class="slot-title">Performance Evaluation of a Vector Supercomputer SX-Aurora TSUBASA</div><div class="slot-authors">Kazuhiko Komatsu (Tohoku University); Shintaro Momose, Yoko Isobe, Osamu Watanabe, and Akihiro Musa (Tohoku University, NEC Corporation); Mitsuo Yokokawa (Kobe University, NEC Corporation); Toshikazu Aoyama (NEC Corporation); and Masayuki Sato and Hiroaki Kobayashi (Tohoku University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_712_1539825877_88" onclick="$('#vhsjs_view_712_1539825877_88').hide();
                $('#vhsjs_hide_712_1539825877_88').show();
                $('#711_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_712_1539825877_88" onclick="$('#711_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_712_1539825877_88').hide();
                $('#vhsjs_view_712_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="712_1539825877_88" id="711_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>A new SX-Aurora TSUBASA vector supercomputer has been released with a new system architecture and a new execution model to achieve high sustained performance, especially for memory-intensive applications. In SX-Aurora TSUBASA, the vector host (VH) of a standard x86 Linux node is attached to the vector engine (VE) of a newly developed vector processor.  An application is executed on the VE, and only system calls are offloaded to the VH. This new execution model can avoid redundant data transfers between a VH and a VE that can easily become a bottleneck in the conventional execution model. This paper examines the potential of SX-Aurora TSUBASA. First, the basic performance of SX-Aurora TSUBASA is clarified by evaluating benchmark programs. Then, the effectiveness of the new execution model is examined by using a microbenchmark.  Finally, the high potential of SX-Aurora TSUBASA is clarified through evaluations of practical applications.</blockquote></div></div></div></div><a href="includes/files/pap346s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Computational Physics, Scientific Computing, Tech Program Reg Pass</span><br /><div class="session-title">Astrophysics Applications</div><div class="slot-entry"><a name="pap239"></a><div class="slot-title">Phase Asynchronous AMR Execution for Productive and Performant Astrophysical Flows</div><div class="slot-authors">Muhammad Nufail Farooqi (Koc University); Tan Nguyen, Weiqun Zhang, Ann S. Almgren, and John Shalf (Lawrence Berkeley National Laboratory); and Didem Unat (Koc University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_714_1539825877_88" onclick="$('#vhsjs_view_714_1539825877_88').hide();
                $('#vhsjs_hide_714_1539825877_88').show();
                $('#713_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_714_1539825877_88" onclick="$('#713_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_714_1539825877_88').hide();
                $('#vhsjs_view_714_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="714_1539825877_88" id="713_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>Adaptive Mesh Refinement (AMR) is an approach to solving PDEs that reduces the computational and memory requirements at the expense of increased communication. Although adopting asynchronous execution can overcome communication issues, manually restructuring an AMR application to realize asynchrony is extremely complicated and hinders readability and long-term maintainability. To balance performance against productivity, we design a user-friendly API and adopt phase asynchronous execution model where all subgrids at an AMR level can be computed asynchronously. <br><br>We apply the phase asynchrony to transform a real-world AMR application, CASTRO, which solves multicomponent compressible hydrodynamic equations for astrophysical flows. We evaluate the performance and programming effort required to use our carefully designed API and execution model for transitioning large legacy codes from synchronous to asynchronous execution up to 278,528 Intel-KNL cores. CASTRO is about 100K lines of code but less than 0.2% code changes are required to achieve significant performance improvement.</blockquote></div></div></div></div><a href="includes/files/pap239s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap294"></a><div class="slot-title">Computing Planetary Interior Normal Modes with a Highly Parallel Polynomial Filtering Eigensolver</div><div class="slot-authors">Jia Shi (Rice University), Ruipeng Li (Lawrence Livermore National Laboratory), Yuanzhe Xi and Yousef Saad (University of Minnesota), and Maarten V. de Hoop (Rice University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_716_1539825877_88" onclick="$('#vhsjs_view_716_1539825877_88').hide();
                $('#vhsjs_hide_716_1539825877_88').show();
                $('#715_1539825877_88').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_716_1539825877_88" onclick="$('#715_1539825877_88').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_716_1539825877_88').hide();
                $('#vhsjs_view_716_1539825877_88').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="716_1539825877_88" id="715_1539825877_88" style="display: none"><div class="arrow-slidedown"><blockquote>A highly parallel algorithm has been developed and exploited to compute the planetary normal modes of the elastic-gravitational system, which is approximated via the mixed finite element method on unstructured tetrahedral meshes. The eigenmodes of the relevant generalized eigenvalue problem were extracted by a Lanczos approach combined with polynomial filtering. In contrast with the standard shift-and-invert and the full-mode coupling algorithms, the polynomial filtering technique is ideally suited for solving large-scale 3-D interior eigenvalue problems since it significantly enhances the memory and computational efficiency without loss of accuracy.  The parallel efficiency and scalability of this approach are demonstrated on Stampede2 at the Texas Advanced Computing Center. To our knowledge, this is the first time that the direct calculation of the normal modes of 3-D strongly heterogeneous planets, in particular, Earth and Mars, is made feasible via a combination of multiple matrix-free methods and a separation of the essential spectra.</blockquote></div></div></div></div><a href="includes/files/pap294s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack156"></a><div class="section-title">Security</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Applications, Graph Algorithms, Security, Tech Program Reg Pass</span><br /><div class="session-title">Graph Algorithms and Systems</div><div class="slot-entry"><a name="pap115"></a><div class="slot-title">iSpan: Parallel Identification of Strongly Connected Components with Spanning Trees</div><div class="slot-authors">Yuede Ji (George Washington University); Hang Liu (University of Massachusetts, Lowell); and H. Howie Huang (George Washington University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_718_1539825877_89" onclick="$('#vhsjs_view_718_1539825877_89').hide();
                $('#vhsjs_hide_718_1539825877_89').show();
                $('#717_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_718_1539825877_89" onclick="$('#717_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_718_1539825877_89').hide();
                $('#vhsjs_view_718_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="718_1539825877_89" id="717_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>Detecting strongly connected components (SCCs) in a directed graph is crucial for understanding the structure of graphs. Most real-world graphs have one large SCC that contains the majority of the vertices, and many small SCCs whose sizes are reversely proportional to the frequency of their occurrence. For both types of SCCs, current approaches that rely on depth or breadth first search (DFS or BFS) face the challenges of strict synchronization requirement and high computation cost. In this paper, we advocate a new paradigm of identifying SCCs with simple spanning trees, since SCC detection requires only the knowledge of connectivity among the vertices. We have developed a prototype called iSpan which consists of parallel, relaxed synchronization construction of spanning trees for detecting the large and small SCCs. The evaluations show that iSpan is able to significantly outperform current state-of-the-art DFS and BFS- based methods by average 18× and 4×, respectively.</blockquote></div></div></div></div><a href="includes/files/pap115s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap468"></a><div class="slot-title">Adaptive Anonymization of Data with b-Edge Covers</div><div class="slot-authors">Arif Khan (Pacific Northwest National Laboratory), Krzysztof Choromanski (Google LLC), Alex Pothen and S M Ferdous (Purdue University), and Mahantesh Halappanavar and Antonino Tumeo (Pacific Northwest National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_720_1539825877_89" onclick="$('#vhsjs_view_720_1539825877_89').hide();
                $('#vhsjs_hide_720_1539825877_89').show();
                $('#719_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_720_1539825877_89" onclick="$('#719_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_720_1539825877_89').hide();
                $('#vhsjs_view_720_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="720_1539825877_89" id="719_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>We explore the problem of sharing data that pertains to individuals with anonymity guarantees, where each user requires a desired level of privacy.  We propose the first shared-memory as well as distributed memory parallel algorithms for the adaptive anonymity problem that achieves this goal, and produces high quality anonymized datasets.  <br><br>The new algorithm is based on an optimization procedure that iteratively computes weights on the edges of a dissimilarity matrix, and at each iteration computes a minimum weighted b-Edge cover in the graph. We are able to solve adaptive anonymity problems with hundreds of thousands of instances and hundreds of features on a leadership-class supercomputer in under five minutes. Our algorithm scales up to 4K cores on a distributed memory supercomputer, while also providing good speedups on shared memory multiprocessors. On smaller problems, where an algorithm based on Belief Propagation is feasible, our algorithm is two orders of magnitude faster.</blockquote></div></div></div></div><a href="includes/files/pap468s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap167"></a><div class="slot-title">faimGraph: High Performance Management of Fully-Dynamic Graphs Under Tight Memory Constraints on the GPU</div><div class="slot-authors">Martin Winter and Daniel Mlakar (Graz University of Technology); Rhaleb Zayer and Hans-Peter Seidel (Max Planck Institute for Informatics); and Markus Steinberger (Graz University of Technology, Max Planck Institute for Informatics)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_722_1539825877_89" onclick="$('#vhsjs_view_722_1539825877_89').hide();
                $('#vhsjs_hide_722_1539825877_89').show();
                $('#721_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_722_1539825877_89" onclick="$('#721_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_722_1539825877_89').hide();
                $('#vhsjs_view_722_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="722_1539825877_89" id="721_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>In this paper, we present a fully-dynamic graph data structure for the Graphics Processing Unit (GPU). It delivers high update rates while keeping a low memory footprint using autonomous memory management directly on the GPU. The data structure is fully-dynamic, allowing not only for edge but also vertex updates. Performing the memory management on the GPU allows for fast initialization times and efficient update procedures without additional intervention or reallocation procedures from the host.  faimGraph is the first GPU graph framework that fully reclaims unused memory, permitting long time application with highly changing graph structures. Performance evaluations show that our approach outperforms that previous state-of-the-art in for all types of graph updates. Furthermore, evaluate algorithmic performance using a PageRank and a Static Triangle Counting (STC) implementation, demonstrating the suitability of the framework even for memory access intensive algorithms.</blockquote></div></div></div></div><a href="includes/files/pap167s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack158"></a><div class="section-title">Sparse Computation</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Graph Algorithms, Linear Algebra, Machine Learning, Sparse Computation, Tech Program Reg Pass</span><br /><div class="session-title">Algorithms on Sparse Data</div><div class="slot-entry"><a name="pap511"></a><div class="slot-title">HiCOO: Hierarchical Storage of Sparse Tensors</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Jiajia Li, Jimeng Sun, and Richard Vuduc (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_724_1539825877_89" onclick="$('#vhsjs_view_724_1539825877_89').hide();
                $('#vhsjs_hide_724_1539825877_89').show();
                $('#723_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_724_1539825877_89" onclick="$('#723_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_724_1539825877_89').hide();
                $('#vhsjs_view_724_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="724_1539825877_89" id="723_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>This paper proposes a new storage format for sparse tensors, called Hierarchical COOrdinate (HiCOO; pronounced: “haiku”). It derives from coordinate (COO) format, arguably the de facto standard for general sparse tensor storage. HiCOO improves upon COO by compressing the indices in units of sparse tensor blocks, with the goals of preserving the “mode-agnostic” simplicity of COO while reducing the bytes needed to represent the tensor and promoting data locality. We evaluate HiCOO by implementing a single-node, multicore-parallel version of the matricized tensor-times-Khatri-Rao product (MTTKRP) operation, which is the most expensive computational core in the widely used CANDECOMP/PARAFAC decomposition(CPD) algorithm. This MTTKRP implementation achieves up to 23.0× (6.8× on average) speedup over COO format and up to 15.6× (3.1× on average) speedup over another state-of-the-art format, compressed sparse fiber (CSF), by using less or comparable storage of them. When used within CPD, we also observe speedups against COO- and CSF-based implementations.</blockquote></div></div></div></div><a href="includes/files/pap511s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap273"></a><div class="slot-title">Distributed Memory Sparse Inverse Covariance Matrix Estimation on High-Performance Computing Architectures</div><div class="slot-authors">Aryan Eftekhari (University of Lugano), Matthias Bollhöfer (Braunschweig University of Technology), and Olaf Schenk (University of Lugano)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_726_1539825877_89" onclick="$('#vhsjs_view_726_1539825877_89').hide();
                $('#vhsjs_hide_726_1539825877_89').show();
                $('#725_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_726_1539825877_89" onclick="$('#725_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_726_1539825877_89').hide();
                $('#vhsjs_view_726_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="726_1539825877_89" id="725_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>We consider the problem of estimating sparse inverse covariance matrices for high-dimensional datasets using the l1-regularized Gaussian maximum likelihood method. This task is particularly challenging as the required computational resources increase superlinearly with the dimensionality of the dataset. We introduce a performant and scalable algorithm which builds on the current advancements of second-order, maximum likelihood methods. The routine leverages the intrinsic parallelism in the linear algebra operations and exploits the underlying sparsity of the problem. The computational bottlenecks are identified and the respective subroutines are parallelized using an MPI-OpenMP approach. Experiments conducted on a Cray XC50 system at the Swiss National Supercomputing Center show that, in comparison to the state-of-the-art algorithms, the proposed routine provides significant strong scaling speedup with ideal scalability up to 128 nodes. The developed framework is used to estimate the sparse inverse covariance matrix of both synthetic and real-world datasets with up to 10 million dimensions.</blockquote></div></div></div></div><a href="includes/files/pap273s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap466"></a><div class="slot-title">PruneJuice:  Pruning Trillion-Edge Graphs to a Precise Pattern-Matching Solution</div><div class="slot-authors">Tahsin Reza, Matei Ripeanu, and Nicolas Tripoul (University of British Columbia) and Geoffrey Sanders and Roger Pearce (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_728_1539825877_89" onclick="$('#vhsjs_view_728_1539825877_89').hide();
                $('#vhsjs_hide_728_1539825877_89').show();
                $('#727_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_728_1539825877_89" onclick="$('#727_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_728_1539825877_89').hide();
                $('#vhsjs_view_728_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="728_1539825877_89" id="727_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>Pattern matching is a powerful graph analysis tool. Unfortunately, existing solutions have limited scalability, support only a limited set of search patterns, and/or focus on only a subset of the real-world problems associated with pattern matching. This paper presents a new algorithmic pipeline that: (i) enables highly scalable pattern matching on labeled graphs, (ii) supports arbitrary patterns, (iii) enables trade-offs between precision and time-to-solution (while always selecting all vertices and edges that participate in matches, thus offering 100% recall), and (iv) supports a set of popular data analytics scenarios. We implement our approach on top of HavoqGT and demonstrate its advantages through strong and weak scaling experiments on massive-scale real-world (up to 257 billion edges) and synthetic (up to 4.4 trillion edges) graphs, respectively, and at scales (1,024 nodes / 36,864 cores) orders of magnitude larger than used in the past for similar problems.</blockquote></div></div></div></div><a href="includes/files/pap466s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack159"></a><div class="section-title">State of the Practice</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">GPUs, Resiliency, State of the Practice, System Software, Tech Program Reg Pass</span><br /><div class="session-title">Resilience</div><div class="slot-entry"><a name="pap262"></a><div class="slot-title">GPU Age-Aware Scheduling to Improve the Reliability of Leadership Jobs on Titan</div><div class="slot-authors">Christopher Zimmer, Don Maxwell, Stephen McNally, Scott Atchley, and Sudharshan S. Vazhkudai (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_730_1539825877_89" onclick="$('#vhsjs_view_730_1539825877_89').hide();
                $('#vhsjs_hide_730_1539825877_89').show();
                $('#729_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_730_1539825877_89" onclick="$('#729_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_730_1539825877_89').hide();
                $('#vhsjs_view_730_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="730_1539825877_89" id="729_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>The increasing rate of failures on the Oak Ridge Leadership Computing Facility's (OLCF) Titan supercomputer, resulted in the replacement of 50% of its GPUs between 2015 and 2017. The largest jobs, also known as "leadership jobs'', continued to experience increased application failures. These jobs contained significant amounts of low-failure rate and high-failure rate GPUs. The impacts of these failures were felt more by leadership jobs due to longer wait times, runtimes, and higher charge rates. In this work, we have designed techniques to increase the use of low-failure GPUs in leadership jobs through targeted resource allocation. This employed two complementary techniques, updating both the system ordering and the allocation mechanisms. In simulation, the application of these techniques resulted in a 33% increase in low-failure GPU hours being assigned to leadership jobs. Our GPU Age-Aware Scheduling has been used in production on Titan since July of 2017.</blockquote></div></div></div></div><a href="includes/files/pap262s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap109"></a><div class="slot-title">FlipTracker: Understanding Natural Error Resilience in HPC Applications</div><div class="slot-authors">Luanzheng Guo and Dong Li (University of California, Merced); Ignacio Laguna (Lawrence Livermore National Laboratory); and Martin Schulz (Technical University Munich)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_732_1539825877_89" onclick="$('#vhsjs_view_732_1539825877_89').hide();
                $('#vhsjs_hide_732_1539825877_89').show();
                $('#731_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_732_1539825877_89" onclick="$('#731_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_732_1539825877_89').hide();
                $('#vhsjs_view_732_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="732_1539825877_89" id="731_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>As high-performance computing systems scale in size and computational power, the danger of silent errors, i.e., errors that can bypass hardware detection mechanisms and impact application state, grows dramatically. Consequently, applications running on HPC systems need to exhibit resilience to such errors. Previous work has found that, for certain codes, this resilience can come for free, i.e., some applications are naturally resilient, but few works have shown the code patterns—combinations or sequences of computations—that make an application naturally resilient. In this paper, we present FlipTracker, a framework designed to extract these patterns using fine-grained tracking of error propagation and resilience properties, and we use it to present a set of computation patterns that are responsible for making representative HPC applications naturally resilient to errors. This not only enables a deeper understanding of resilience properties of these codes, but also can guide future application designs toward patterns with natural resilience.</blockquote></div></div></div></div><a href="includes/files/pap109s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap111"></a><div class="slot-title">Doomsday: Predicting Which Node Will Fail When on Supercomputers</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Anwesha Das and Frank Mueller (North Carolina State University) and Paul Hargrove, Eric Roman, and Scott Baden (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_734_1539825877_89" onclick="$('#vhsjs_view_734_1539825877_89').hide();
                $('#vhsjs_hide_734_1539825877_89').show();
                $('#733_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_734_1539825877_89" onclick="$('#733_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_734_1539825877_89').hide();
                $('#vhsjs_view_734_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="734_1539825877_89" id="733_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>Predicting which node will fail and how soon remains a challenge for HPC resilience, yet may pave the way to exploiting proactive remedies before jobs fail. Not only for increasing scalability up to exascale systems, but even for contemporary supercomputer architectures does it require substantial efforts to distill anomalous events from noisy raw logs. To this end, we propose a novel phrase extraction mechanism called TBP (time-based phrases) to pin-point node failures, which is unprecedented.  Our study, based on real system data and statistical machine learning, demonstrates the feasibility to predict which specific node will fail in Cray systems. TBP achieves no less than 83% recall rates with lead times as high as 2 minutes. This opens up the door for enhancing prediction lead times for supercomputing systems in general, thereby facilitating efficient usage of both computing capacity and power in large scale production systems.</blockquote></div></div></div></div><a href="includes/files/pap111s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Networks, Resource Management, Scheduling, State of the Practice, System Software, Tech Program Reg Pass</span><br /><div class="session-title">Resource Management and Interference</div><div class="slot-entry"><a name="pap360"></a><div class="slot-title">RM-Replay: A High-Fidelity Tuning, Optimization and Exploration Tool for Resource Management</div><div class="slot-authors">Maxime Martinasso, Miguel Gila, Mauro Bianco, Sadaf R. Alam, Colin McMurtrie, and Thomas C. Schulthess (Swiss National Supercomputing Centre)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_736_1539825877_89" onclick="$('#vhsjs_view_736_1539825877_89').hide();
                $('#vhsjs_hide_736_1539825877_89').show();
                $('#735_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_736_1539825877_89" onclick="$('#735_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_736_1539825877_89').hide();
                $('#vhsjs_view_736_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="736_1539825877_89" id="735_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>Leading hybrid and heterogeneous supercomputing systems process hundreds of thousands of jobs using complex scheduling algorithms and parameters. The centers operating these systems aim to achieve higher levels of resource utilization while being restricted by compliance with policy constraints. There is a critical need for a high-fidelity, high-performance tool with familiar interfaces that allows not only tuning and optimization of the operational job scheduler but also enables exploration of new resource management algorithms. We propose a new methodology and a tool called RM-Replay which is not a simulator but instead a fast replay engine for production workloads. Slurm is used as a platform to demonstrate the capabilities of our replay engine.<br><br>The tool accuracy is discussed and our investigation shows that, by providing better job runtime estimation or using topology-aware allocation, scheduling metric values vary. The presented methodology to create fast replay engines can be extended to other complex systems.</blockquote></div></div></div></div><a href="includes/files/pap360s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap541"></a><div class="slot-title">Evaluation of an Interference-Free Node Allocation Policy on Fat-Tree Clusters</div><div class="slot-authors">Samuel D. Pollard (University of Oregon) and Nikhil Jain, Stephen Herbein, and Abhinav Bhatele (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_738_1539825877_89" onclick="$('#vhsjs_view_738_1539825877_89').hide();
                $('#vhsjs_hide_738_1539825877_89').show();
                $('#737_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_738_1539825877_89" onclick="$('#737_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_738_1539825877_89').hide();
                $('#vhsjs_view_738_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="738_1539825877_89" id="737_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>Interference between jobs competing for network bandwidth on a fat-tree cluster can cause significant variability and degradation in performance. These performance issues can be mitigated or completely eliminated if the resource allocation policy takes the network topology into account when allocating nodes to jobs. We implement a fat-tree network topology aware node allocation policy that allocates isolated partitions to jobs in order to eliminate inter-job interference. We compare the impact of this node allocation policy to a topology-oblivious policy with respect to the execution time of individual jobs with different communication patterns. We also evaluate the cluster's quality of service using metrics such as system utilization, schedule makespan, and job wait time for both policies. The results obtained for production workloads indicate that a topology-aware node allocation can provide interference-free execution without negatively impacting the cluster's quality of service.</blockquote></div></div></div></div><a href="includes/files/pap541s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap311"></a><div class="slot-title">Mitigating Inter-Job Interference Using Adaptive Flow-Aware Routing</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Staci A. Smith, Clara E. Cromey, and David K. Lowenthal (University of Arizona); Jens Domke (Tokyo Institute of Technology); and Nikhil Jain, Jayaraman J. Thiagarajan, and Abhinav Bhatele (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_740_1539825877_89" onclick="$('#vhsjs_view_740_1539825877_89').hide();
                $('#vhsjs_hide_740_1539825877_89').show();
                $('#739_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_740_1539825877_89" onclick="$('#739_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_740_1539825877_89').hide();
                $('#vhsjs_view_740_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="740_1539825877_89" id="739_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>On most high performance computing platforms, applications share network resources with other jobs running concurrently on the system.  Inter-job network interference can have a significant impact on the performance of communication-intensive applications, and no satisfactory solutions yet exist for mitigating this degradation.<br><br>In this paper, we analyze network congestion caused by multi-job workloads on two production systems that use popular network topologies---fat-tree and dragonfly. For each system, we establish a regression model to relate network hotspots to application performance degradation, showing that current routing strategies are insufficient to load-balance network traffic and mitigate interference on production systems.  We then propose an alternative type of adaptive routing strategy, which we call adaptive flow-aware routing.  We implement a prototype of our strategy, and tests on the fat-tree system show up to a 46% improvement in job run time when compared to the default routing.</blockquote></div></div></div></div><a href="includes/files/pap311s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, MPI, Networks, Performance, Programming Systems, State of the Practice, Tech Program Reg Pass</span><br /><div class="session-title">MPI Optimization and Characterization</div><div class="slot-entry"><a name="pap504"></a><div class="slot-title">Cooperative Rendezvous Protocols for Improved Performance and Overlap</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">S. Chakraborty, M. Bayatpour, J. Hashmi, H. Subramoni, and D. K. Panda (Ohio State University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_742_1539825877_89" onclick="$('#vhsjs_view_742_1539825877_89').hide();
                $('#vhsjs_hide_742_1539825877_89').show();
                $('#741_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_742_1539825877_89" onclick="$('#741_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_742_1539825877_89').hide();
                $('#vhsjs_view_742_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="742_1539825877_89" id="741_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>With the emergence of larger multi-/many-core clusters, performance of large message communication is becoming more important. MPI libraries use different Rendezvous protocols to perform large message communication. However, existing Rendezvous protocols do not consider the overall communication pattern and make optimal use of the Sender and the Receiver CPUs. In this work, we propose a cooperative Rendezvous protocol that can provide up to 2x improvement in intra-node bandwidth and latency for large messages. We also propose a scheme to dynamically choose the best Rendezvous protocol for each message based on the communication pattern.  Finally, we show how these improvements can increase the overlap of computation with intra-node and inter-node communication, and lead to application level benefits. We evaluate proposed designs on three different architectures including Intel Xeon, Knights Landing, and OpenPOWER with different HPC applications and obtain benefits up to 19% with Graph500, 16% with CoMD, and 10% with MiniGhost.</blockquote></div></div></div></div><a href="includes/files/pap504s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap463"></a><div class="slot-title">Framework for Scalable Intra-Node Collective Operations Using Shared Memory</div><div class="slot-authors">Surabhi Jain, Rashid Kaleem, Marc Gamell Balmana, Akhil Langer, Dmitry Durnov, Alexander Sannikov, and Maria Garzaran (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_744_1539825877_89" onclick="$('#vhsjs_view_744_1539825877_89').hide();
                $('#vhsjs_hide_744_1539825877_89').show();
                $('#743_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_744_1539825877_89" onclick="$('#743_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_744_1539825877_89').hide();
                $('#vhsjs_view_744_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="744_1539825877_89" id="743_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>Collective operations are used in MPI programs to express common communication patterns, collective computations, or synchronizations. In many collectives, such as barrier or allreduce, the intra-node component of the collective is in the critical path, as the inter-node communication cannot start until the intra-node component has been executed. Thus, with increasing number of core counts in each node, intra-node optimizations that leverage the intra-node shared memory become increasingly important.<br><br>In this paper, we focus on the performance benefit of optimizing intra-node collectives using shared memory. We optimize several collectives using the primitives in broadcast and reduce as building blocks for other collectives. A comparison of our implementation on top of MPICH shows significant performance speedups with respect to the original MPICH implementation, MVAPICH, and OpenMPI, among others.</blockquote></div></div></div></div><a href="includes/files/pap463s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap537"></a><div class="slot-title">Characterization of MPI Usage on a Production Supercomputer</div><div class="slot-authors">Sudheer Chunduri, Scott Parker, Pavan Balaji, Kevin Harms, and Kalyan Kumaran (Argonne National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_746_1539825877_89" onclick="$('#vhsjs_view_746_1539825877_89').hide();
                $('#vhsjs_hide_746_1539825877_89').show();
                $('#745_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_746_1539825877_89" onclick="$('#745_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_746_1539825877_89').hide();
                $('#vhsjs_view_746_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="746_1539825877_89" id="745_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>MPI is the most prominent programming model used in scientific computing today.  Despite it's importance, however, how scientific applications use it in production is not very well understood due to the lack of low overhead profiling tools.   We used a lightweight profiling tool, called autoperf, to log the MPI usage characteristics of production applications on a large supercomputing system (Mira) and its corresponding development system (Cetus).  Autoperf limits the amount of information that it records in order to keep the overhead to a minimum while still storing enough data to derive useful insights.  MPI usage statistics have been collected for over 100K jobs that were run within a 2-year period and are analyzed.  The analysis of this data is intended as a mechanism to provide useful insights for MPI developers and network hardware developers for their next generation of improvements, and for supercomputing center operators for their next system procurements.</blockquote></div></div></div></div><a href="includes/files/pap537s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Networks, Performance, Scientific Computing, State of the Practice, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Large Scale System Deployments</div><div class="slot-entry"><a name="pap277"></a><div class="slot-title">The Design, Deployment, and Evaluation of the CORAL Pre-Exascale Systems</div><div class="slot-authors">Sudharshan S. Vazhkudai (Oak Ridge National Laboratory); Bronis R. de Supinski (Lawrence Livermore National Laboratory); Arthur S. Bland and Al Geist (Oak Ridge National Laboratory); James Sexton and Jim Kahle (IBM); Christopher J. Zimmer, Scott Atchley, Sarp H. Oral, Don E. Maxwell, and Veronica G. Vergara Larrea (Oak Ridge National Laboratory); Adam Bertsch and Robin Goldstone (Lawrence Livermore National Laboratory); Wayne Joubert (Oak Ridge National Laboratory); Chris Chambreau (Lawrence Livermore National Laboratory); David Appelhans and Robert Blackmore (IBM); Ben Casses (Lawrence Livermore National Laboratory); George Chochia and Gene Davison (IBM); Matthew A. Ezell (Oak Ridge National Laboratory); Tom Gooding (IBM); Elsa Gonsiorowski (Lawrence Livermore National Laboratory); Leopold Grinberg, Bill Hanson, and Bill Hartner (IBM); Ian Karlin and Matthew L. Leininger (Lawrence Livermore National Laboratory); Dustin Leverman (Oak Ridge National Laboratory); Chris Marroquin (IBM); Adam Moody (Lawrence Livermore National Laboratory); Martin Ohmacht (IBM); Ramesh Pankajakshan (Lawrence Livermore National Laboratory); Fernando Pizzano (IBM); James H. Rogers (Oak Ridge National Laboratory); Bryan Rosenburg (IBM); Drew Schmidt, Mallikarjun Shankar, and Feiyi Wang (Oak Ridge National Laboratory); Py Watson (Lawrence Livermore National Laboratory); Bob Walkup (IBM); Lance D. Weems (Lawrence Livermore National Laboratory); and Junqi Yin (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_748_1539825877_89" onclick="$('#vhsjs_view_748_1539825877_89').hide();
                $('#vhsjs_hide_748_1539825877_89').show();
                $('#747_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_748_1539825877_89" onclick="$('#747_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_748_1539825877_89').hide();
                $('#vhsjs_view_748_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="748_1539825877_89" id="747_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>CORAL, the Collaboration of Oak Ridge, Argonne and Livermore, is fielding two similar IBM systems, Summit and Sierra, with NVIDIA GPUs that will replace the existing Titan and Sequoia systems. Summit and Sierra are currently ranked No. 1 and No. 3, respectively, on the Top500 list. We discuss the design and key differences of the systems. Our evaluation of the systems highlights the following. Applications that fit in HBM see the most benefit and may prefer more GPUs; however, for some applications, the CPU-GPU bandwidth is more important than the number of GPUs. The node-local burst buffer scales linearly, and can achieve a 4X improvement over the parallel file system for large jobs; smaller jobs, however, may benefit from writing directly to the PFS. Finally, several CPU, network and memory bound analytics and GPU-bound deep learning codes achieve up to a 11X and 79X speedup/node, respectively over Titan.</blockquote></div></div></div></div><a href="includes/files/pap277s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap449"></a><div class="slot-title">Best Practices and Lessons from Deploying and Operating a Sustained-Petascale System: The Blue Waters Experience</div><div class="slot-authors">Gregory H. Bauer, Brett Bode, Jeremy Enos, William T. Kramer, Scott Lathrop, Celso L. Mendes, and Roberto R. Sisneros (University of Illinois, National Center for Supercomputing Applications)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_750_1539825877_89" onclick="$('#vhsjs_view_750_1539825877_89').hide();
                $('#vhsjs_hide_750_1539825877_89').show();
                $('#749_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_750_1539825877_89" onclick="$('#749_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_750_1539825877_89').hide();
                $('#vhsjs_view_750_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="750_1539825877_89" id="749_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>Building and operating versatile extreme-scale computing systems that work productively for a range of frontier research domains present many challenges and opportunities. Solutions created, experiences acquired, and lessons learned, while rarely published, could drive the development of new methods and practices and raise the bar for all organizations supporting research, scholarship, and education. This paper describes the methods and procedures developed for deploying, supporting, and continuously improving the Blue Waters system and its services during the last five years. Being the first US sustained-petascale computing platform available to the open-science community, the Blue Waters project pioneered various unique practices that we are sharing to be adopted and further improved by the community. We present our support and service methodologies, and the leadership practices employed for ensuring that the system stays highly efficient and productive. We also provide the return on investment summaries related to deploying and operating the system.</blockquote></div></div></div></div><a href="includes/files/pap449s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap346"></a><div class="slot-title">Performance Evaluation of a Vector Supercomputer SX-Aurora TSUBASA</div><div class="slot-authors">Kazuhiko Komatsu (Tohoku University); Shintaro Momose, Yoko Isobe, Osamu Watanabe, and Akihiro Musa (Tohoku University, NEC Corporation); Mitsuo Yokokawa (Kobe University, NEC Corporation); Toshikazu Aoyama (NEC Corporation); and Masayuki Sato and Hiroaki Kobayashi (Tohoku University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_752_1539825877_89" onclick="$('#vhsjs_view_752_1539825877_89').hide();
                $('#vhsjs_hide_752_1539825877_89').show();
                $('#751_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_752_1539825877_89" onclick="$('#751_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_752_1539825877_89').hide();
                $('#vhsjs_view_752_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="752_1539825877_89" id="751_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>A new SX-Aurora TSUBASA vector supercomputer has been released with a new system architecture and a new execution model to achieve high sustained performance, especially for memory-intensive applications. In SX-Aurora TSUBASA, the vector host (VH) of a standard x86 Linux node is attached to the vector engine (VE) of a newly developed vector processor.  An application is executed on the VE, and only system calls are offloaded to the VH. This new execution model can avoid redundant data transfers between a VH and a VE that can easily become a bottleneck in the conventional execution model. This paper examines the potential of SX-Aurora TSUBASA. First, the basic performance of SX-Aurora TSUBASA is clarified by evaluating benchmark programs. Then, the effectiveness of the new execution model is examined by using a microbenchmark.  Finally, the high potential of SX-Aurora TSUBASA is clarified through evaluations of practical applications.</blockquote></div></div></div></div><a href="includes/files/pap346s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Data Management, File Systems, Networks, State of the Practice, System Software, Workflows, Tech Program Reg Pass</span><br /><div class="session-title">File Systems: Data Movement and Provenance</div><div class="slot-entry"><a name="pap407"></a><div class="slot-title">Dac-Man: Data Change Management for Scientific Datasets on HPC Systems</div><div class="slot-authors">Devarshi Ghoshal, Lavanya Ramakrishnan, and Deborah Agarwal (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_754_1539825877_89" onclick="$('#vhsjs_view_754_1539825877_89').hide();
                $('#vhsjs_hide_754_1539825877_89').show();
                $('#753_1539825877_89').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_754_1539825877_89" onclick="$('#753_1539825877_89').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_754_1539825877_89').hide();
                $('#vhsjs_view_754_1539825877_89').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="754_1539825877_89" id="753_1539825877_89" style="display: none"><div class="arrow-slidedown"><blockquote>Scientific data is growing rapidly and often changes due to instrument configurations, software updates, or quality assessments. These changes in datasets can result in significant waste of compute and storage resources on HPC systems as downstream pipelines are reprocessed. Data changes need to be detected, tracked, and analyzed for understanding the impact of data change, managing data provenance, and making efficient and effective decisions about reprocessing and use of HPC resources. Existing methods for identifying and capturing change are often manual, domain-specific, and error-prone and do not scale to large scientific datasets. In this paper, we describe the design and implementation of Dac-Man framework, which identifies, captures, and manages change in large scientific datasets, and enables plug-in of domain-specific change analysis with minimal user effort. Our evaluations show that it can retrieve file changes from directories containing millions of files and terabytes of data in less than a minute.</blockquote></div></div></div></div><a href="includes/files/pap407s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap521"></a><div class="slot-title">Stacker: An Autonomic Data Movement Engine for Extreme-Scale Data Staging-Based In Situ Workflows</div><div class="slot-authors">Pradeep Subedi, Philip Davis, and Shaohua Duan (Rutgers University); Scott Klasky (Oak Ridge National Laboratory); Hemanth Kolla (Sandia National Laboratories); and Manish Parashar (Rutgers University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_756_1539825877_9" onclick="$('#vhsjs_view_756_1539825877_9').hide();
                $('#vhsjs_hide_756_1539825877_9').show();
                $('#755_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_756_1539825877_9" onclick="$('#755_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_756_1539825877_9').hide();
                $('#vhsjs_view_756_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="756_1539825877_9" id="755_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>Data staging and in situ workflows are being explored extensively as an approach to address data-related costs at very large scales. However, the impact of emerging storage architectures (e.g., deep memory hierarchies and burst buffers) upon data staging solutions remains a challenge. In this paper, we investigate how burst buffers can be effectively used by data staging solutions, for example, as a persistence storage tier of the memory hierarchy. Furthermore, we use machine learning based prefetching techniques to move data between the storage levels in an autonomous manner. We also present Stacker, a prototype of the proposed solutions implemented within the Data\-Spaces data staging service, and experimentally evaluate its performance and scalability using the S3D combustion workflow on current leadership class platforms. Our experiments demonstrate that Stacker achieves low latency, high volume data-staging with low overhead as compared to in-memory staging services for production scientific workflows.</blockquote></div></div></div></div><a href="includes/files/pap521s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap206"></a><div class="slot-title">A Year in the Life of a Parallel File System</div><div class="slot-authors">Glenn K. Lockwood (Lawrence Berkeley National Laboratory), Shane Snyder (Argonne National Laboratory), Teng Wang and Suren Byna (Lawrence Berkeley National Laboratory), Philip Carns (Argonne National Laboratory), and Nicholas J. Wright (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_758_1539825877_9" onclick="$('#vhsjs_view_758_1539825877_9').hide();
                $('#vhsjs_hide_758_1539825877_9').show();
                $('#757_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_758_1539825877_9" onclick="$('#757_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_758_1539825877_9').hide();
                $('#vhsjs_view_758_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="758_1539825877_9" id="757_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>I/O performance is a critical aspect of data-intensive scientific computing.  We seek to advance the state of the practice in understanding and diagnosing I/O performance issues through investigation of a comprehensive I/O performance data set that captures a full year of production storage activity at two leadership-scale computing facilities.  We demonstrate techniques to identify regions of interest, perform focused investigations of both long-term trends and transient anomalies, and uncover the contributing factors that lead to performance fluctuation.<br><br>We find that a year in the life of a parallel file system is comprised of distinct regions of long-term performance variation in addition to short-term performance transients.  We demonstrate how systematic identification of these performance regions, combined with comprehensive analysis, allows us to isolate the factors contributing to different performance maladies at different time scales.  From this, we present specific lessons learned and important considerations for HPC storage practitioners.</blockquote></div></div></div></div><a href="includes/files/pap206s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack160"></a><div class="section-title">Storage</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Clouds and Distributed Computing, File Systems, I/O, Storage, Tech Program Reg Pass</span><br /><div class="session-title">Data and Storage</div><div class="slot-entry"><a name="pap165"></a><div class="slot-title">SP-Cache: Load-Balanced, Redundancy-Free Cluster Caching with Selective Partition</div><div class="slot-authors">Yinghao Yu, Renfei Huang, Wei Wang, Jun Zhang, and Khaled Ben Letaief (Hong Kong University of Science and Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_760_1539825877_9" onclick="$('#vhsjs_view_760_1539825877_9').hide();
                $('#vhsjs_hide_760_1539825877_9').show();
                $('#759_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_760_1539825877_9" onclick="$('#759_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_760_1539825877_9').hide();
                $('#vhsjs_view_760_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="760_1539825877_9" id="759_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>Data-intensive clusters increasingly employ in-memory solutions to improve I/O performance. However, the routinely observed file popularity skew and load imbalance create hotspots, which significantly degrades the benefits of in-memory solutions. Common approaches to tame load imbalance include copying multiple replicas of hot files and creating parity chunks using storage codes. Yet, these techniques either suffer from high memory redundancy or incur non-trivial encoding/decoding overhead. In this paper, we propose a different approach to achieve load balancing without memory redundancy or encoding/decoding overhead. Our solution, termed SP-Cache, selectively partitions files based on their popularity and evenly caches those partitions across the cluster. We develop an efficient algorithm to determine the optimal number of partitions for hot files—too few partitions are incapable of mitigating hotspots, while too many are susceptible to stragglers. EC2 deployment and trace-driven simulations show that, compared with existing solutions, SP-Cache reduces the read latencies by up to 40%.</blockquote></div></div></div></div><a href="includes/files/pap165s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap585"></a><div class="slot-title">BESPOKV: Application Tailored Scale-Out Key-Value Stores</div><div class="slot-authors">Ali Anwar (IBM), Yue Cheng (George Mason University), Hai Huang (IBM), Jingoo Han (Virginia Tech), Hyogi Sim (Oak Ridge National Laboratory), Dongyoon Lee (Virginia Tech), Fred Douglis (Perspecta Labs), and Ali R. Butt (Virginia Tech)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_762_1539825877_9" onclick="$('#vhsjs_view_762_1539825877_9').hide();
                $('#vhsjs_hide_762_1539825877_9').show();
                $('#761_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_762_1539825877_9" onclick="$('#761_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_762_1539825877_9').hide();
                $('#vhsjs_view_762_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="762_1539825877_9" id="761_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>Enterprise KV stores are not well suited for HPC applications, and entail customization and cumbersome end-to-end KV design to extract the HPC application needs. In this paper we present BESPOKV, an adaptive, extensible, and scale-out KV store framework. BESPOKV decouples the KV store design into the control plane for distributed management and the data plane for local data store. BESPOKV takes as input a single-server KV store, called a datalet, and transparently enables a scalable and fault-tolerant distributed KV store service. The resulting distributed stores are also adaptive to consistency or topology requirement changes and can be easily extended for new types of services. Experiments show that BESPOKV-enabled distributed KV stores scale horizontally to a large number of nodes, and performs comparably and sometimes better than the state-of-the-art systems.</blockquote></div></div></div></div><a href="includes/files/pap585s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap450"></a><div class="slot-title">Scaling Embedded In Situ Indexing with DeltaFS</div><div class="slot-authors">Qing Zheng, Charles D. Cranor, Danhao Guo, Gregory R. Ganger, George Amvrosiadis, and Garth A. Gibson (Carnegie Mellon University) and Bradley W. Settlemyer, Gary Grider, and Fan Guo (Los Alamos National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_764_1539825877_9" onclick="$('#vhsjs_view_764_1539825877_9').hide();
                $('#vhsjs_hide_764_1539825877_9').show();
                $('#763_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_764_1539825877_9" onclick="$('#763_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_764_1539825877_9').hide();
                $('#vhsjs_view_764_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="764_1539825877_9" id="763_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>Analysis of large-scale simulation output is a core element of scientific inquiry, but analysis queries may experience significant I/O overhead when the data is not structured for efficient retrieval. While in-situ processing allows for improved time-to-insight for many applications, scaling in-situ frameworks to hundreds of thousands of cores can be difficult in practice. The DeltaFS in-situ indexing is a new approach for in-situ processing of massive amounts of data to achieve efficient point and small-range queries. This paper describes the challenges and lessons learned when scaling this in-situ processing function to hundreds of thousands of cores. We propose techniques for scalable all-to-all communication that is memory and bandwidth efficient, concurrent indexing, and specialized LSM-Tree formats. Combining these techniques allows DeltaFS to control the cost of in-situ processing while maintaining 3 orders of magnitude query speedup when scaling alongside the popular VPIC particle-in-cell code to 131,072 cores.</blockquote></div></div></div></div><a href="includes/files/pap450s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Data Analytics, Performance, Programming Systems, Storage, Tools, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Performance Optimization Studies</div><div class="slot-entry"><a name="pap335"></a><div class="slot-title">Many-Core Graph Workload Analysis</div><div class="slot-authors">Stijn Eyerman, Wim Heirman, Kristof Du Bois, Joshua B. Fryman, and Ibrahim Hur (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_766_1539825877_9" onclick="$('#vhsjs_view_766_1539825877_9').hide();
                $('#vhsjs_hide_766_1539825877_9').show();
                $('#765_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_766_1539825877_9" onclick="$('#765_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_766_1539825877_9').hide();
                $('#vhsjs_view_766_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="766_1539825877_9" id="765_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>Graph applications have specific characteristics that are not common in other application domains. In this paper, we analyze multiple graph applications on current multi- and many-core processors and provide conclusions and recommendations for future designs. We provide new insights on executing graph applications on many-core processors.<br><br>Our main novel observations are (i) some memory streams do show locality, while others show no locality, (ii) thread imbalance becomes a major problem with many threads, and (iii) many threads are required to saturate high-bandwidth memories. We recommend a selective memory access policy, where accesses with locality are cached and prefetched, while accesses without locality can remain uncached to save cache capacity. Additionally, more threads are needed, but they are not used efficiently due to thread imbalance. Our recommendation is to revise the graph analysis algorithms to provide more parallelism, and to provide a few high-performance cores that speedup sections with low parallelism.</blockquote></div></div></div></div><a href="includes/files/pap335s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap488"></a><div class="slot-title">Lessons Learned from Analyzing Dynamic Promotion for User-Level Threading</div><div class="slot-authors">Shintaro Iwasaki (University of Tokyo), Abdelhalim Amer (Argonne National Laboratory), Kenjiro Taura (University of Tokyo), and Pavan Balaji (Argonne National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_768_1539825877_9" onclick="$('#vhsjs_view_768_1539825877_9').hide();
                $('#vhsjs_hide_768_1539825877_9').show();
                $('#767_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_768_1539825877_9" onclick="$('#767_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_768_1539825877_9').hide();
                $('#vhsjs_view_768_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="768_1539825877_9" id="767_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>A performance vs. practicality trade-off exists between user-level threading techniques. The community has settled mostly on a black-and-white perspective; fully fledged threads assume that suspension is imminent and incur overheads when suspension does not take place, and run-to-completion threads are more lightweight but less practical since they cannot suspend. Gray areas exist, however, whereby threads can start with minimal capabilities and then can be dynamically promoted to acquire additional capabilities when needed. This paper investigates the full spectrum of threading techniques from a performance vs. practicality trade-off perspective on modern multicore and many-core systems. Our results indicate that achieving the best trade-off highly depends on the suspension likelihood; dynamic promotion is more appropriate when suspension is unlikely and represents a solid replacement for run to completion, thanks to its lower programming constraints, while fully fledged threads remain the technique of choice when suspension likelihood is high.</blockquote></div></div></div></div><a href="includes/files/pap488s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap484"></a><div class="slot-title">Topology-Aware Space-Shared Co-Analysis of Large-Scale Molecular Dynamics Simulations</div><div class="slot-authors">Preeti Malakar (Indian Institute of Technology Kanpur); Todd Munson, Christopher Knight, and Venkatram Vishwanath (Argonne National Laboratory); and Michael E. Papka (Argonne National Laboratory, Northern Illinois University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_770_1539825877_9" onclick="$('#vhsjs_view_770_1539825877_9').hide();
                $('#vhsjs_hide_770_1539825877_9').show();
                $('#769_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_770_1539825877_9" onclick="$('#769_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_770_1539825877_9').hide();
                $('#vhsjs_view_770_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="770_1539825877_9" id="769_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>Analysis of scientific simulation data can be concurrently executed with simulation either in time- or space-shared mode. This mitigates the I/O bottleneck.  However it results in either stalling the simulation for performing the analysis or transferring data for analysis. In this paper, we improve the throughput of space-shared in situ analysis of large-scale simulations by topology-aware mapping and optimal process decomposition. We propose node interconnect topology-aware process placement for simulation and analysis to reduce the data movement time. We also present an integer linear program for optimal 3D decompositions of simulation and analysis processes. We demonstrate our approach using molecular dynamics simulation on Mira, Cori and Theta supercomputers. Our mapping schemes, combined with optimal 3D process decomposition and code optimizations resulted in up to 30% lower execution times for space-shared in situ analysis than the default approach. Our mappings also reduce MPI collective I/O times by 10-40%.</blockquote></div></div></div></div><a href="includes/files/pap484s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Applications, Cosmology, Data Analytics, Deep Learning, Machine Learning, Programming Systems, Storage, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Deep Learning</div><div class="slot-entry"><a name="pap425"></a><div class="slot-title">Exploring Flexible Communications for Streamlining DNN Ensemble Training Pipelines</div><div class="slot-authors">Randall Pittman, Hui Guan, and Xipeng Shen (North Carolina State University) and Seung-Hwan Lim and Robert M. Patton (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_772_1539825877_9" onclick="$('#vhsjs_view_772_1539825877_9').hide();
                $('#vhsjs_hide_772_1539825877_9').show();
                $('#771_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_772_1539825877_9" onclick="$('#771_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_772_1539825877_9').hide();
                $('#vhsjs_view_772_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="772_1539825877_9" id="771_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>Parallel training of a Deep Neural Network (DNN) ensemble on a cluster of nodes is a common practice to train multiple models in order to construct a model with a higher prediction accuracy. Existing ensemble training pipelines can perform a great deal of redundant operations, resulting in unnecessary CPU usage, or even poor pipeline performance.  In order to remove these redundancies, we need pipelines with more communication flexibility than existing DNN frameworks provide.<br><br>This project investigates a series of designs to improve pipeline flexibility and adaptivity, while also increasing performance. We implement our designs using Tensorflow with Horovod, and test it using several large DNNs. Our results show that the CPU time spent during training is reduced by 2-11X. Furthermore, our implementation can achieve up to 10X speedups when CPU core limits are imposed. Our best pipeline also reduces the average power draw of the ensemble training process by 5-16%.</blockquote></div></div></div></div><a href="includes/files/pap425s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap429"></a><div class="slot-title">CosmoFlow: Using Deep Learning to Learn the Universe at Scale</div><div class="slot-authors">Amrita Mathuriya (Intel Corporation); Deborah Bard (National Energy Research Scientific Computing Center (NERSC), Lawrence Berkeley National Laboratory); Pete Mendygral (Cray Inc); Lawrence Meadows (Intel Corporation); James Arnemann (University of California, Berkeley); Lei Shao (Intel Corporation); Siyu He (Carnegie Mellon University); Tuomas Karna (Intel Corporation); Diana Moise (Cray Inc); Simon J. Pennycook (Intel Corporation); Kristyn Maschhoff (Cray Inc); Jason Sewall and Nalini Kumar (Intel Corporation); Shirley Ho (Lawrence Berkeley National Laboratory, Carnegie Mellon University); Michael F. Ringenburg (Cray Inc); Mr Prabhat (Lawrence Berkeley National Laboratory, National Energy Research Scientific Computing Center (NERSC)); and Victor Lee (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_774_1539825877_9" onclick="$('#vhsjs_view_774_1539825877_9').hide();
                $('#vhsjs_hide_774_1539825877_9').show();
                $('#773_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_774_1539825877_9" onclick="$('#773_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_774_1539825877_9').hide();
                $('#vhsjs_view_774_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="774_1539825877_9" id="773_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>Deep learning is a promising tool to determine the physical model that describes our universe.   To handle the considerable computational cost of this problem, we present CosmoFlow: a highly scalable deep learning application built on top of the TensorFlow framework.<br><br>CosmoFlow uses efficient implementations of 3D convolution and pooling primitives, together with improvements in threading for many element-wise operations, to improve training performance on Intel Xeon Phi processors.  We also utilize the Cray PE Machine Learning Plugin for efficient scaling to multiple nodes. We demonstrate fully synchronous data-parallel training on 8192 nodes of Cori with 77% parallel efficiency, achieving 3.5 Pflop/s sustained performance. <br><br>To our knowledge, this is the first large-scale science application of the TensorFlow framework at supercomputer scale with fully-synchronous training. These enhancements enable us to process large 3D dark matter distribution and predict the cosmological parameters Omega_M, sigma_8 and N_s with unprecedented accuracy.</blockquote></div></div></div></div><a href="includes/files/pap429s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap322"></a><div class="slot-title">Anatomy of High-Performance Deep Learning Convolutions on SIMD Architectures</div><div class="slot-authors">Evangelos Georganas, Sasikanth Avancha, Kunal Banerjee, Dhiraj Kalamkar, Greg Henry, Hans Pabst, and Alexander Heinecke (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_776_1539825877_9" onclick="$('#vhsjs_view_776_1539825877_9').hide();
                $('#vhsjs_hide_776_1539825877_9').show();
                $('#775_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_776_1539825877_9" onclick="$('#775_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_776_1539825877_9').hide();
                $('#vhsjs_view_776_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="776_1539825877_9" id="775_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>Convolution layers are prevalent in many classes of deep neural networks, including Convolutional Neural Networks (CNNs) which provide state-of-the-art results for tasks like image recognition, neural machine translation, and speech recognition. The computationally expensive nature of a convolution operation has led to the proliferation of implementations including matrix-matrix multiplication formulation, and direct convolution primarily targeting GPUs. In this paper, we introduce direct convolution kernels for x86 architectures, in particular for Xeon and Xeon Phi systems, which are implemented via a dynamic compilation approach. Our JIT-based implementation shows close to theoretical peak performance, depending on the setting and the CPU architecture at hand. We additionally demonstrate how these JIT-optimized kernels can be integrated into a light-weight multi-node graph execution model. This illustrates that single- and multi-node runs yield high efficiencies and high image-throughputs  when executing state of the art image recognition tasks on CPUs.</blockquote></div></div></div></div><a href="includes/files/pap322s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack163"></a><div class="section-title">System Software</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">GPUs, Resiliency, State of the Practice, System Software, Tech Program Reg Pass</span><br /><div class="session-title">Resilience</div><div class="slot-entry"><a name="pap262"></a><div class="slot-title">GPU Age-Aware Scheduling to Improve the Reliability of Leadership Jobs on Titan</div><div class="slot-authors">Christopher Zimmer, Don Maxwell, Stephen McNally, Scott Atchley, and Sudharshan S. Vazhkudai (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_778_1539825877_9" onclick="$('#vhsjs_view_778_1539825877_9').hide();
                $('#vhsjs_hide_778_1539825877_9').show();
                $('#777_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_778_1539825877_9" onclick="$('#777_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_778_1539825877_9').hide();
                $('#vhsjs_view_778_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="778_1539825877_9" id="777_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>The increasing rate of failures on the Oak Ridge Leadership Computing Facility's (OLCF) Titan supercomputer, resulted in the replacement of 50% of its GPUs between 2015 and 2017. The largest jobs, also known as "leadership jobs'', continued to experience increased application failures. These jobs contained significant amounts of low-failure rate and high-failure rate GPUs. The impacts of these failures were felt more by leadership jobs due to longer wait times, runtimes, and higher charge rates. In this work, we have designed techniques to increase the use of low-failure GPUs in leadership jobs through targeted resource allocation. This employed two complementary techniques, updating both the system ordering and the allocation mechanisms. In simulation, the application of these techniques resulted in a 33% increase in low-failure GPU hours being assigned to leadership jobs. Our GPU Age-Aware Scheduling has been used in production on Titan since July of 2017.</blockquote></div></div></div></div><a href="includes/files/pap262s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap109"></a><div class="slot-title">FlipTracker: Understanding Natural Error Resilience in HPC Applications</div><div class="slot-authors">Luanzheng Guo and Dong Li (University of California, Merced); Ignacio Laguna (Lawrence Livermore National Laboratory); and Martin Schulz (Technical University Munich)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_780_1539825877_9" onclick="$('#vhsjs_view_780_1539825877_9').hide();
                $('#vhsjs_hide_780_1539825877_9').show();
                $('#779_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_780_1539825877_9" onclick="$('#779_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_780_1539825877_9').hide();
                $('#vhsjs_view_780_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="780_1539825877_9" id="779_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>As high-performance computing systems scale in size and computational power, the danger of silent errors, i.e., errors that can bypass hardware detection mechanisms and impact application state, grows dramatically. Consequently, applications running on HPC systems need to exhibit resilience to such errors. Previous work has found that, for certain codes, this resilience can come for free, i.e., some applications are naturally resilient, but few works have shown the code patterns—combinations or sequences of computations—that make an application naturally resilient. In this paper, we present FlipTracker, a framework designed to extract these patterns using fine-grained tracking of error propagation and resilience properties, and we use it to present a set of computation patterns that are responsible for making representative HPC applications naturally resilient to errors. This not only enables a deeper understanding of resilience properties of these codes, but also can guide future application designs toward patterns with natural resilience.</blockquote></div></div></div></div><a href="includes/files/pap109s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap111"></a><div class="slot-title">Doomsday: Predicting Which Node Will Fail When on Supercomputers</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Anwesha Das and Frank Mueller (North Carolina State University) and Paul Hargrove, Eric Roman, and Scott Baden (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_782_1539825877_9" onclick="$('#vhsjs_view_782_1539825877_9').hide();
                $('#vhsjs_hide_782_1539825877_9').show();
                $('#781_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_782_1539825877_9" onclick="$('#781_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_782_1539825877_9').hide();
                $('#vhsjs_view_782_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="782_1539825877_9" id="781_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>Predicting which node will fail and how soon remains a challenge for HPC resilience, yet may pave the way to exploiting proactive remedies before jobs fail. Not only for increasing scalability up to exascale systems, but even for contemporary supercomputer architectures does it require substantial efforts to distill anomalous events from noisy raw logs. To this end, we propose a novel phrase extraction mechanism called TBP (time-based phrases) to pin-point node failures, which is unprecedented.  Our study, based on real system data and statistical machine learning, demonstrates the feasibility to predict which specific node will fail in Cray systems. TBP achieves no less than 83% recall rates with lead times as high as 2 minutes. This opens up the door for enhancing prediction lead times for supercomputing systems in general, thereby facilitating efficient usage of both computing capacity and power in large scale production systems.</blockquote></div></div></div></div><a href="includes/files/pap111s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Networks, Resource Management, Scheduling, State of the Practice, System Software, Tech Program Reg Pass</span><br /><div class="session-title">Resource Management and Interference</div><div class="slot-entry"><a name="pap360"></a><div class="slot-title">RM-Replay: A High-Fidelity Tuning, Optimization and Exploration Tool for Resource Management</div><div class="slot-authors">Maxime Martinasso, Miguel Gila, Mauro Bianco, Sadaf R. Alam, Colin McMurtrie, and Thomas C. Schulthess (Swiss National Supercomputing Centre)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_784_1539825877_9" onclick="$('#vhsjs_view_784_1539825877_9').hide();
                $('#vhsjs_hide_784_1539825877_9').show();
                $('#783_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_784_1539825877_9" onclick="$('#783_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_784_1539825877_9').hide();
                $('#vhsjs_view_784_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="784_1539825877_9" id="783_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>Leading hybrid and heterogeneous supercomputing systems process hundreds of thousands of jobs using complex scheduling algorithms and parameters. The centers operating these systems aim to achieve higher levels of resource utilization while being restricted by compliance with policy constraints. There is a critical need for a high-fidelity, high-performance tool with familiar interfaces that allows not only tuning and optimization of the operational job scheduler but also enables exploration of new resource management algorithms. We propose a new methodology and a tool called RM-Replay which is not a simulator but instead a fast replay engine for production workloads. Slurm is used as a platform to demonstrate the capabilities of our replay engine.<br><br>The tool accuracy is discussed and our investigation shows that, by providing better job runtime estimation or using topology-aware allocation, scheduling metric values vary. The presented methodology to create fast replay engines can be extended to other complex systems.</blockquote></div></div></div></div><a href="includes/files/pap360s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap541"></a><div class="slot-title">Evaluation of an Interference-Free Node Allocation Policy on Fat-Tree Clusters</div><div class="slot-authors">Samuel D. Pollard (University of Oregon) and Nikhil Jain, Stephen Herbein, and Abhinav Bhatele (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_786_1539825877_9" onclick="$('#vhsjs_view_786_1539825877_9').hide();
                $('#vhsjs_hide_786_1539825877_9').show();
                $('#785_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_786_1539825877_9" onclick="$('#785_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_786_1539825877_9').hide();
                $('#vhsjs_view_786_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="786_1539825877_9" id="785_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>Interference between jobs competing for network bandwidth on a fat-tree cluster can cause significant variability and degradation in performance. These performance issues can be mitigated or completely eliminated if the resource allocation policy takes the network topology into account when allocating nodes to jobs. We implement a fat-tree network topology aware node allocation policy that allocates isolated partitions to jobs in order to eliminate inter-job interference. We compare the impact of this node allocation policy to a topology-oblivious policy with respect to the execution time of individual jobs with different communication patterns. We also evaluate the cluster's quality of service using metrics such as system utilization, schedule makespan, and job wait time for both policies. The results obtained for production workloads indicate that a topology-aware node allocation can provide interference-free execution without negatively impacting the cluster's quality of service.</blockquote></div></div></div></div><a href="includes/files/pap541s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap311"></a><div class="slot-title">Mitigating Inter-Job Interference Using Adaptive Flow-Aware Routing</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Staci A. Smith, Clara E. Cromey, and David K. Lowenthal (University of Arizona); Jens Domke (Tokyo Institute of Technology); and Nikhil Jain, Jayaraman J. Thiagarajan, and Abhinav Bhatele (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_788_1539825877_9" onclick="$('#vhsjs_view_788_1539825877_9').hide();
                $('#vhsjs_hide_788_1539825877_9').show();
                $('#787_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_788_1539825877_9" onclick="$('#787_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_788_1539825877_9').hide();
                $('#vhsjs_view_788_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="788_1539825877_9" id="787_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>On most high performance computing platforms, applications share network resources with other jobs running concurrently on the system.  Inter-job network interference can have a significant impact on the performance of communication-intensive applications, and no satisfactory solutions yet exist for mitigating this degradation.<br><br>In this paper, we analyze network congestion caused by multi-job workloads on two production systems that use popular network topologies---fat-tree and dragonfly. For each system, we establish a regression model to relate network hotspots to application performance degradation, showing that current routing strategies are insufficient to load-balance network traffic and mitigate interference on production systems.  We then propose an alternative type of adaptive routing strategy, which we call adaptive flow-aware routing.  We implement a prototype of our strategy, and tests on the fat-tree system show up to a 46% improvement in job run time when compared to the default routing.</blockquote></div></div></div></div><a href="includes/files/pap311s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">GPUs, Memory, NVRAM, Performance, System Software, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Non-Volatile Memory</div><div class="slot-entry"><a name="pap203"></a><div class="slot-title">Runtime Data Management on Non-Volatile Memory-Based Heterogeneous Memory for Task-Parallel Programs</div><div class="slot-authors">Kai Wu, Jie Ren, and Dong Li (University of California, Merced)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_790_1539825877_9" onclick="$('#vhsjs_view_790_1539825877_9').hide();
                $('#vhsjs_hide_790_1539825877_9').show();
                $('#789_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_790_1539825877_9" onclick="$('#789_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_790_1539825877_9').hide();
                $('#vhsjs_view_790_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="790_1539825877_9" id="789_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>Non-volatile memory (NVM) provides a scalable solution to replace DRAM as main memory. Because of relatively high latency and low bandwidth of NVM (comparing with DRAM), NVM often pairs with DRAM to build a  heterogeneous main memory system (HMS). Deciding data placement on NVM-based HMS is critical to enable future NVM-based HPC. In this paper, we study task-parallel programs and introduce a runtime system to address the data placement problem on NVM-based HMS. Leveraging semantics and execution mode of task-parallel programs, we efficiently characterize memory access patterns of tasks and reduce data movement overhead. We also introduce a performance model to predict performance for tasks with various data placements on HMS. Evaluating with a set of HPC benchmarks, we show that our runtime system achieves higher performance than a conventional HMS-oblivious runtime (24% improvement on average) and two state-of-the-art HMS-aware solutions (16% and 11% improvement on average, respectively).</blockquote></div></div></div></div><a href="includes/files/pap203s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap194"></a><div class="slot-title">DRAGON: Breaking GPU Memory Capacity Limits with Direct NVM Access</div><div class="slot-authors">Pak Markthub (Tokyo Institute of Technology); Mehmet E. Belviranli, Seyong Lee, and Jeffrey S. Vetter (Oak Ridge National Laboratory); and Satoshi Matsuoka (RIKEN, Tokyo Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_792_1539825877_9" onclick="$('#vhsjs_view_792_1539825877_9').hide();
                $('#vhsjs_hide_792_1539825877_9').show();
                $('#791_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_792_1539825877_9" onclick="$('#791_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_792_1539825877_9').hide();
                $('#vhsjs_view_792_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="792_1539825877_9" id="791_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>Heterogeneous computing with accelerators is growing in importance in high performance computing (HPC). Recently, application datasets have expanded beyond the memory capacity of these accelerators, and often beyond the capacity of their hosts. Meanwhile, nonvolatile memory (NVM) storage has emerged as a pervasive component in HPC systems because NVM provides massive amounts of memory capacity at affordable cost. Currently, for accelerator applications to use NVM, they must manually orchestrate data movement across multiple memories and this approach only performs well for applications with simple access behaviors. To address this issue, we developed DRAGON, a solution that enables all classes of GP-GPU applications to transparently compute on terabyte datasets residing in NVM. DRAGON leverages the page-faulting mechanism on the recent NVIDIA GPUs by extending capabilities of CUDA Unified Memory (UM). Our experimental results show that DRAGON transparently expands memory capacity and obtain additional speedups via automated I/O and data transfer overlapping.</blockquote></div></div></div></div><a href="includes/files/pap194s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap355"></a><div class="slot-title">Siena: Exploring the Design Space of Heterogeneous Memory Systems</div><div class="slot-authors">Ivy B. Peng and Jeffrey S. Vetter (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_794_1539825877_9" onclick="$('#vhsjs_view_794_1539825877_9').hide();
                $('#vhsjs_hide_794_1539825877_9').show();
                $('#793_1539825877_9').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_794_1539825877_9" onclick="$('#793_1539825877_9').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_794_1539825877_9').hide();
                $('#vhsjs_view_794_1539825877_9').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="794_1539825877_9" id="793_1539825877_9" style="display: none"><div class="arrow-slidedown"><blockquote>Memory systems are crucial to the performance, power, and cost of high-performance computing systems. Recently, multiple factors are driving the need for more complex, deep memory hierarchies. However, architects and customers are struggling to design memory systems that effectively balance multiple, often competing, factors in this large, multidimensional, and fast-moving design space. In this paper, we systematically explore the organization of heterogeneous memory systems on a framework, called Siena. Siena facilitates quick exploration of memory architectures with flexible configurations of memory systems and realistic memory workloads. We perform a design space exploration on 22 proposed memory systems using eight relevant workloads. Our results show that horizontal organizations of memories can achieve higher performance than that of vertical organizations when the distribution of memory traffic balances the performance gap between memories. However, the coupling effects through shared resources and application behaviors could negate the advantage of high-performance memory in horizontal organizations.</blockquote></div></div></div></div><a href="includes/files/pap355s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Data Management, File Systems, Networks, State of the Practice, System Software, Workflows, Tech Program Reg Pass</span><br /><div class="session-title">File Systems: Data Movement and Provenance</div><div class="slot-entry"><a name="pap407"></a><div class="slot-title">Dac-Man: Data Change Management for Scientific Datasets on HPC Systems</div><div class="slot-authors">Devarshi Ghoshal, Lavanya Ramakrishnan, and Deborah Agarwal (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_796_1539825877_91" onclick="$('#vhsjs_view_796_1539825877_91').hide();
                $('#vhsjs_hide_796_1539825877_91').show();
                $('#795_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_796_1539825877_91" onclick="$('#795_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_796_1539825877_91').hide();
                $('#vhsjs_view_796_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="796_1539825877_91" id="795_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>Scientific data is growing rapidly and often changes due to instrument configurations, software updates, or quality assessments. These changes in datasets can result in significant waste of compute and storage resources on HPC systems as downstream pipelines are reprocessed. Data changes need to be detected, tracked, and analyzed for understanding the impact of data change, managing data provenance, and making efficient and effective decisions about reprocessing and use of HPC resources. Existing methods for identifying and capturing change are often manual, domain-specific, and error-prone and do not scale to large scientific datasets. In this paper, we describe the design and implementation of Dac-Man framework, which identifies, captures, and manages change in large scientific datasets, and enables plug-in of domain-specific change analysis with minimal user effort. Our evaluations show that it can retrieve file changes from directories containing millions of files and terabytes of data in less than a minute.</blockquote></div></div></div></div><a href="includes/files/pap407s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap521"></a><div class="slot-title">Stacker: An Autonomic Data Movement Engine for Extreme-Scale Data Staging-Based In Situ Workflows</div><div class="slot-authors">Pradeep Subedi, Philip Davis, and Shaohua Duan (Rutgers University); Scott Klasky (Oak Ridge National Laboratory); Hemanth Kolla (Sandia National Laboratories); and Manish Parashar (Rutgers University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_798_1539825877_91" onclick="$('#vhsjs_view_798_1539825877_91').hide();
                $('#vhsjs_hide_798_1539825877_91').show();
                $('#797_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_798_1539825877_91" onclick="$('#797_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_798_1539825877_91').hide();
                $('#vhsjs_view_798_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="798_1539825877_91" id="797_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>Data staging and in situ workflows are being explored extensively as an approach to address data-related costs at very large scales. However, the impact of emerging storage architectures (e.g., deep memory hierarchies and burst buffers) upon data staging solutions remains a challenge. In this paper, we investigate how burst buffers can be effectively used by data staging solutions, for example, as a persistence storage tier of the memory hierarchy. Furthermore, we use machine learning based prefetching techniques to move data between the storage levels in an autonomous manner. We also present Stacker, a prototype of the proposed solutions implemented within the Data\-Spaces data staging service, and experimentally evaluate its performance and scalability using the S3D combustion workflow on current leadership class platforms. Our experiments demonstrate that Stacker achieves low latency, high volume data-staging with low overhead as compared to in-memory staging services for production scientific workflows.</blockquote></div></div></div></div><a href="includes/files/pap521s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap206"></a><div class="slot-title">A Year in the Life of a Parallel File System</div><div class="slot-authors">Glenn K. Lockwood (Lawrence Berkeley National Laboratory), Shane Snyder (Argonne National Laboratory), Teng Wang and Suren Byna (Lawrence Berkeley National Laboratory), Philip Carns (Argonne National Laboratory), and Nicholas J. Wright (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_800_1539825877_91" onclick="$('#vhsjs_view_800_1539825877_91').hide();
                $('#vhsjs_hide_800_1539825877_91').show();
                $('#799_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_800_1539825877_91" onclick="$('#799_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_800_1539825877_91').hide();
                $('#vhsjs_view_800_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="800_1539825877_91" id="799_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>I/O performance is a critical aspect of data-intensive scientific computing.  We seek to advance the state of the practice in understanding and diagnosing I/O performance issues through investigation of a comprehensive I/O performance data set that captures a full year of production storage activity at two leadership-scale computing facilities.  We demonstrate techniques to identify regions of interest, perform focused investigations of both long-term trends and transient anomalies, and uncover the contributing factors that lead to performance fluctuation.<br><br>We find that a year in the life of a parallel file system is comprised of distinct regions of long-term performance variation in addition to short-term performance transients.  We demonstrate how systematic identification of these performance regions, combined with comprehensive analysis, allows us to isolate the factors contributing to different performance maladies at different time scales.  From this, we present specific lessons learned and important considerations for HPC storage practitioners.</blockquote></div></div></div></div><a href="includes/files/pap206s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack170"></a><div class="section-title">Tools</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">OpenMP, Performance, Power, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Performance and Energy Analysis</div><div class="slot-entry"><a name="pap175"></a><div class="slot-title">A Parallelism Profiler with What-If Analyses for OpenMP Programs</div><div class="slot-authors">Nader Boushehrinejadmoradi, Adarsh Yoga, and Santosh Nagarakatte (Rutgers University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_802_1539825877_91" onclick="$('#vhsjs_view_802_1539825877_91').hide();
                $('#vhsjs_hide_802_1539825877_91').show();
                $('#801_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_802_1539825877_91" onclick="$('#801_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_802_1539825877_91').hide();
                $('#vhsjs_view_802_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="802_1539825877_91" id="801_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>This paper proposes OMP-WHIP, a profiler that measures inherent parallelism in the program for a given input and provides what-if analyses to estimate improvements in parallelism. We propose a novel OpenMP series parallel graph representation (OSPG) that precisely captures series-parallel relations induced by various directives between different fragments of dynamic execution. OMP-WHIP constructs the OSPG and measures the computation performed by each dynamic fragment using hardware performance counters. This series-parallel representation along with the fine-grained measurement of computation is a performance model of the program for a given input, which enables computation of inherent parallelism. This novel performance model also enables what-if analyses where a programmer can estimate improvements in parallelism when bottlenecks are parallelized. We have used OMP-WHIP to identify parallelism bottlenecks in more than forty applications and then designed strategies to improve the speedup in seven applications.</blockquote></div></div></div></div><a href="includes/files/pap175s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap186"></a><div class="slot-title">Energy Efficiency Modeling of Parallel Applications</div><div class="slot-authors">Mark Endrei, Chao Jin, Minh Ngoc Dinh, and David Abramson (University of Queensland); Heidi Poxon and Luiz DeRose (Cray Inc); and Bronis R. de Supinski (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_804_1539825877_91" onclick="$('#vhsjs_view_804_1539825877_91').hide();
                $('#vhsjs_hide_804_1539825877_91').show();
                $('#803_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_804_1539825877_91" onclick="$('#803_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_804_1539825877_91').hide();
                $('#vhsjs_view_804_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="804_1539825877_91" id="803_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>Energy efficiency has become increasingly important in high performance computing (HPC), as power constraints and costs escalate. Workload and system characteristics form a complex optimization search space in which optimal settings for energy efficiency and performance often diverge. Thus, we must identify trade-off options to find the desired balance. We present an innovative statistical model that accurately predicts the Pareto optimal trade-off options using only user-controllable parameters. Our approach can also tolerate both measurement and model errors. We study model training and validation using several HPC kernels, then with more complex workloads, including AMG and LAMMPS. We can calibrate an accurate model from as few as 12 runs, with prediction error of less than 10%. Our results identify trade-off options allowing up to 40% energy efficiency improvement at the cost of under 20% performance loss. For AMG, we reduce the required sample measurement time from 13 hours to 74 minutes.</blockquote></div></div></div></div><a href="includes/files/pap186s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap421"></a><div class="slot-title">HPL and DGEMM Performance Variability on the Xeon Platinum 8160 Processor</div><div class="slot-authors">John D. McCalpin (University of Texas, Texas Advanced Computing Center)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_806_1539825877_91" onclick="$('#vhsjs_view_806_1539825877_91').hide();
                $('#vhsjs_hide_806_1539825877_91').show();
                $('#805_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_806_1539825877_91" onclick="$('#805_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_806_1539825877_91').hide();
                $('#vhsjs_view_806_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="806_1539825877_91" id="805_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>During initial testing of a large cluster equipped with Xeon Platinum 8160 processors, we observed infrequent, but significant, performance drops in HPL benchmark results. The variability was seen in both single node and multi-node runs, with approximately 0.4% of results more than 10% slower than the median. We were able to reproduce this behavior with a single-socket (24-core) DGEMM benchmark. Performance counter analysis of several thousand DGEMM runs showed that increased DRAM read traffic is the primary driver of increased execution time. Increased DRAM traffic in this benchmark is primarily generated by dramatically elevated snoop filter evictions, which arise due to the interaction of high-order (physical) address bits with the hash used to map addresses across the 24 coherence agents on the processor. These conflicts (and the associated performance variability) were effectively eliminated (for both DGEMM and HPL) by using 1 GiB large pages.</blockquote></div></div></div></div><a href="includes/files/pap421s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Data Analytics, Performance, Programming Systems, Storage, Tools, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Performance Optimization Studies</div><div class="slot-entry"><a name="pap335"></a><div class="slot-title">Many-Core Graph Workload Analysis</div><div class="slot-authors">Stijn Eyerman, Wim Heirman, Kristof Du Bois, Joshua B. Fryman, and Ibrahim Hur (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_808_1539825877_91" onclick="$('#vhsjs_view_808_1539825877_91').hide();
                $('#vhsjs_hide_808_1539825877_91').show();
                $('#807_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_808_1539825877_91" onclick="$('#807_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_808_1539825877_91').hide();
                $('#vhsjs_view_808_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="808_1539825877_91" id="807_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>Graph applications have specific characteristics that are not common in other application domains. In this paper, we analyze multiple graph applications on current multi- and many-core processors and provide conclusions and recommendations for future designs. We provide new insights on executing graph applications on many-core processors.<br><br>Our main novel observations are (i) some memory streams do show locality, while others show no locality, (ii) thread imbalance becomes a major problem with many threads, and (iii) many threads are required to saturate high-bandwidth memories. We recommend a selective memory access policy, where accesses with locality are cached and prefetched, while accesses without locality can remain uncached to save cache capacity. Additionally, more threads are needed, but they are not used efficiently due to thread imbalance. Our recommendation is to revise the graph analysis algorithms to provide more parallelism, and to provide a few high-performance cores that speedup sections with low parallelism.</blockquote></div></div></div></div><a href="includes/files/pap335s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap488"></a><div class="slot-title">Lessons Learned from Analyzing Dynamic Promotion for User-Level Threading</div><div class="slot-authors">Shintaro Iwasaki (University of Tokyo), Abdelhalim Amer (Argonne National Laboratory), Kenjiro Taura (University of Tokyo), and Pavan Balaji (Argonne National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_810_1539825877_91" onclick="$('#vhsjs_view_810_1539825877_91').hide();
                $('#vhsjs_hide_810_1539825877_91').show();
                $('#809_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_810_1539825877_91" onclick="$('#809_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_810_1539825877_91').hide();
                $('#vhsjs_view_810_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="810_1539825877_91" id="809_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>A performance vs. practicality trade-off exists between user-level threading techniques. The community has settled mostly on a black-and-white perspective; fully fledged threads assume that suspension is imminent and incur overheads when suspension does not take place, and run-to-completion threads are more lightweight but less practical since they cannot suspend. Gray areas exist, however, whereby threads can start with minimal capabilities and then can be dynamically promoted to acquire additional capabilities when needed. This paper investigates the full spectrum of threading techniques from a performance vs. practicality trade-off perspective on modern multicore and many-core systems. Our results indicate that achieving the best trade-off highly depends on the suspension likelihood; dynamic promotion is more appropriate when suspension is unlikely and represents a solid replacement for run to completion, thanks to its lower programming constraints, while fully fledged threads remain the technique of choice when suspension likelihood is high.</blockquote></div></div></div></div><a href="includes/files/pap488s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap484"></a><div class="slot-title">Topology-Aware Space-Shared Co-Analysis of Large-Scale Molecular Dynamics Simulations</div><div class="slot-authors">Preeti Malakar (Indian Institute of Technology Kanpur); Todd Munson, Christopher Knight, and Venkatram Vishwanath (Argonne National Laboratory); and Michael E. Papka (Argonne National Laboratory, Northern Illinois University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_812_1539825877_91" onclick="$('#vhsjs_view_812_1539825877_91').hide();
                $('#vhsjs_hide_812_1539825877_91').show();
                $('#811_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_812_1539825877_91" onclick="$('#811_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_812_1539825877_91').hide();
                $('#vhsjs_view_812_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="812_1539825877_91" id="811_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>Analysis of scientific simulation data can be concurrently executed with simulation either in time- or space-shared mode. This mitigates the I/O bottleneck.  However it results in either stalling the simulation for performing the analysis or transferring data for analysis. In this paper, we improve the throughput of space-shared in situ analysis of large-scale simulations by topology-aware mapping and optimal process decomposition. We propose node interconnect topology-aware process placement for simulation and analysis to reduce the data movement time. We also present an integer linear program for optimal 3D decompositions of simulation and analysis processes. We demonstrate our approach using molecular dynamics simulation on Mira, Cori and Theta supercomputers. Our mapping schemes, combined with optimal 3D process decomposition and code optimizations resulted in up to 30% lower execution times for space-shared in situ analysis than the default approach. Our mappings also reduce MPI collective I/O times by 10-40%.</blockquote></div></div></div></div><a href="includes/files/pap484s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">GPUs, Memory, NVRAM, Performance, System Software, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Non-Volatile Memory</div><div class="slot-entry"><a name="pap203"></a><div class="slot-title">Runtime Data Management on Non-Volatile Memory-Based Heterogeneous Memory for Task-Parallel Programs</div><div class="slot-authors">Kai Wu, Jie Ren, and Dong Li (University of California, Merced)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_814_1539825877_91" onclick="$('#vhsjs_view_814_1539825877_91').hide();
                $('#vhsjs_hide_814_1539825877_91').show();
                $('#813_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_814_1539825877_91" onclick="$('#813_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_814_1539825877_91').hide();
                $('#vhsjs_view_814_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="814_1539825877_91" id="813_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>Non-volatile memory (NVM) provides a scalable solution to replace DRAM as main memory. Because of relatively high latency and low bandwidth of NVM (comparing with DRAM), NVM often pairs with DRAM to build a  heterogeneous main memory system (HMS). Deciding data placement on NVM-based HMS is critical to enable future NVM-based HPC. In this paper, we study task-parallel programs and introduce a runtime system to address the data placement problem on NVM-based HMS. Leveraging semantics and execution mode of task-parallel programs, we efficiently characterize memory access patterns of tasks and reduce data movement overhead. We also introduce a performance model to predict performance for tasks with various data placements on HMS. Evaluating with a set of HPC benchmarks, we show that our runtime system achieves higher performance than a conventional HMS-oblivious runtime (24% improvement on average) and two state-of-the-art HMS-aware solutions (16% and 11% improvement on average, respectively).</blockquote></div></div></div></div><a href="includes/files/pap203s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap194"></a><div class="slot-title">DRAGON: Breaking GPU Memory Capacity Limits with Direct NVM Access</div><div class="slot-authors">Pak Markthub (Tokyo Institute of Technology); Mehmet E. Belviranli, Seyong Lee, and Jeffrey S. Vetter (Oak Ridge National Laboratory); and Satoshi Matsuoka (RIKEN, Tokyo Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_816_1539825877_91" onclick="$('#vhsjs_view_816_1539825877_91').hide();
                $('#vhsjs_hide_816_1539825877_91').show();
                $('#815_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_816_1539825877_91" onclick="$('#815_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_816_1539825877_91').hide();
                $('#vhsjs_view_816_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="816_1539825877_91" id="815_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>Heterogeneous computing with accelerators is growing in importance in high performance computing (HPC). Recently, application datasets have expanded beyond the memory capacity of these accelerators, and often beyond the capacity of their hosts. Meanwhile, nonvolatile memory (NVM) storage has emerged as a pervasive component in HPC systems because NVM provides massive amounts of memory capacity at affordable cost. Currently, for accelerator applications to use NVM, they must manually orchestrate data movement across multiple memories and this approach only performs well for applications with simple access behaviors. To address this issue, we developed DRAGON, a solution that enables all classes of GP-GPU applications to transparently compute on terabyte datasets residing in NVM. DRAGON leverages the page-faulting mechanism on the recent NVIDIA GPUs by extending capabilities of CUDA Unified Memory (UM). Our experimental results show that DRAGON transparently expands memory capacity and obtain additional speedups via automated I/O and data transfer overlapping.</blockquote></div></div></div></div><a href="includes/files/pap194s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap355"></a><div class="slot-title">Siena: Exploring the Design Space of Heterogeneous Memory Systems</div><div class="slot-authors">Ivy B. Peng and Jeffrey S. Vetter (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_818_1539825877_91" onclick="$('#vhsjs_view_818_1539825877_91').hide();
                $('#vhsjs_hide_818_1539825877_91').show();
                $('#817_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_818_1539825877_91" onclick="$('#817_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_818_1539825877_91').hide();
                $('#vhsjs_view_818_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="818_1539825877_91" id="817_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>Memory systems are crucial to the performance, power, and cost of high-performance computing systems. Recently, multiple factors are driving the need for more complex, deep memory hierarchies. However, architects and customers are struggling to design memory systems that effectively balance multiple, often competing, factors in this large, multidimensional, and fast-moving design space. In this paper, we systematically explore the organization of heterogeneous memory systems on a framework, called Siena. Siena facilitates quick exploration of memory architectures with flexible configurations of memory systems and realistic memory workloads. We perform a design space exploration on 22 proposed memory systems using eight relevant workloads. Our results show that horizontal organizations of memories can achieve higher performance than that of vertical organizations when the distribution of memory traffic balances the performance gap between memories. However, the coupling effects through shared resources and application behaviors could negate the advantage of high-performance memory in horizontal organizations.</blockquote></div></div></div></div><a href="includes/files/pap355s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Performance, Resiliency, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Resilience II</div><div class="slot-entry"><a name="pap392"></a><div class="slot-title">Lessons Learned from Memory Errors Observed Over the Lifetime of Cielo</div><div class="slot-authors">Scott Levy and Kurt B. Ferreira (Sandia National Laboratories), Nathan DeBardeleben (Los Alamos National Laboratory), Taniya Siddiqua and Vilas Sridharan (Advanced Micro Devices Inc), and Elisabeth Baseman (Los Alamos National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_820_1539825877_91" onclick="$('#vhsjs_view_820_1539825877_91').hide();
                $('#vhsjs_hide_820_1539825877_91').show();
                $('#819_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_820_1539825877_91" onclick="$('#819_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_820_1539825877_91').hide();
                $('#vhsjs_view_820_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="820_1539825877_91" id="819_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>Maintaining the performance of high-performance computing (HPC) applications as failures increase is a major challenge for next-generation extreme-scale systems. Recent research demonstrates that hardware failures are expected to become more common due to increased component counts, reduced device-feature sizes, and tightly-constrained power budgets. Few existing studies, however, have examined failures in the context of the entire lifetime of a single platform. In this paper, we analyze failure data collected over the entire lifetime of Cielo, a leadership-class HPC system. Our analysis reveals several key findings, including: (i) Cielo’s memory (DRAM and SRAM) exhibited no discernible aging effects; (ii) correctable memory faults are not predictive of future uncorrectable memory faults; (iii) developing more comprehensive logging facilities will improve failure analysis on future machines; (iv) continued advances will be required to ensure current failure mitigation techniques remain a viable option for future platforms.</blockquote></div></div></div></div><a href="includes/files/pap392s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap381"></a><div class="slot-title">Partial Redundancy in HPC Systems with Non-Uniform Node Reliabilities</div><div class="slot-authors">Zaeem Hussain, Taieb Znati, and Rami Melhem (University of Pittsburgh)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_822_1539825877_91" onclick="$('#vhsjs_view_822_1539825877_91').hide();
                $('#vhsjs_hide_822_1539825877_91').show();
                $('#821_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_822_1539825877_91" onclick="$('#821_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_822_1539825877_91').hide();
                $('#vhsjs_view_822_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="822_1539825877_91" id="821_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>We study the usefulness of partial redundancy in HPC message passing systems where individual node failure distributions are not identical. Prior research works on fault tolerance have generally assumed identical failure distributions for the nodes of the system. In such settings, partial replication has never been shown to outperform the two extremes (full and no-replication) for any significant range of node counts. We argue that partial redundancy may provide the best performance under the more realistic assumption of non-identical node failure distributions. We provide theoretical results on arranging nodes with different reliability values among replicas such that system reliability is maximized. Moreover, using system reliability to compute MTTI (mean-time-to-interrupt) and expected completion time of a partially replicated system, we numerically determine the optimal partial replication degree. Our results indicate that partial replication can be a more efficient alternative to full replication at system scales where Checkpoint/Restart alone is not sufficient.</blockquote></div></div></div></div><a href="includes/files/pap381s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap386"></a><div class="slot-title">Evaluating and Accelerating High-Fidelity Error Injection for HPC</div><div class="slot-authors">Chun-Kai Chang, Sangkug Lym, and Nicholas Kelly (University of Texas); Michael B. Sullivan (Nvidia Corporation); and Mattan Erez (University of Texas)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_824_1539825877_91" onclick="$('#vhsjs_view_824_1539825877_91').hide();
                $('#vhsjs_hide_824_1539825877_91').show();
                $('#823_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_824_1539825877_91" onclick="$('#823_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_824_1539825877_91').hide();
                $('#vhsjs_view_824_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="824_1539825877_91" id="823_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>We address two important concerns in the analysis of the behavior of applications in the presence of hardware errors: (1) when is it important to model how hardware faults lead to erroneous values (instruction-level errors) with high fidelity, as opposed to using simple bit-flipping models, and (2) how to enable fast high-fidelity error injection campaigns, in particular when error detectors are employed. We present and verify a new nested Monte Carlo methodology for evaluating high-fidelity gate-level fault models and error-detector coverage, which is orders of magnitude faster than current approaches. We use that methodology to demonstrate that, without detectors, simple error models suffice for evaluating errors in 9 HPC benchmarks.</blockquote></div></div></div></div><a href="includes/files/pap386s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Networks, Performance, Scientific Computing, State of the Practice, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Large Scale System Deployments</div><div class="slot-entry"><a name="pap277"></a><div class="slot-title">The Design, Deployment, and Evaluation of the CORAL Pre-Exascale Systems</div><div class="slot-authors">Sudharshan S. Vazhkudai (Oak Ridge National Laboratory); Bronis R. de Supinski (Lawrence Livermore National Laboratory); Arthur S. Bland and Al Geist (Oak Ridge National Laboratory); James Sexton and Jim Kahle (IBM); Christopher J. Zimmer, Scott Atchley, Sarp H. Oral, Don E. Maxwell, and Veronica G. Vergara Larrea (Oak Ridge National Laboratory); Adam Bertsch and Robin Goldstone (Lawrence Livermore National Laboratory); Wayne Joubert (Oak Ridge National Laboratory); Chris Chambreau (Lawrence Livermore National Laboratory); David Appelhans and Robert Blackmore (IBM); Ben Casses (Lawrence Livermore National Laboratory); George Chochia and Gene Davison (IBM); Matthew A. Ezell (Oak Ridge National Laboratory); Tom Gooding (IBM); Elsa Gonsiorowski (Lawrence Livermore National Laboratory); Leopold Grinberg, Bill Hanson, and Bill Hartner (IBM); Ian Karlin and Matthew L. Leininger (Lawrence Livermore National Laboratory); Dustin Leverman (Oak Ridge National Laboratory); Chris Marroquin (IBM); Adam Moody (Lawrence Livermore National Laboratory); Martin Ohmacht (IBM); Ramesh Pankajakshan (Lawrence Livermore National Laboratory); Fernando Pizzano (IBM); James H. Rogers (Oak Ridge National Laboratory); Bryan Rosenburg (IBM); Drew Schmidt, Mallikarjun Shankar, and Feiyi Wang (Oak Ridge National Laboratory); Py Watson (Lawrence Livermore National Laboratory); Bob Walkup (IBM); Lance D. Weems (Lawrence Livermore National Laboratory); and Junqi Yin (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_826_1539825877_91" onclick="$('#vhsjs_view_826_1539825877_91').hide();
                $('#vhsjs_hide_826_1539825877_91').show();
                $('#825_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_826_1539825877_91" onclick="$('#825_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_826_1539825877_91').hide();
                $('#vhsjs_view_826_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="826_1539825877_91" id="825_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>CORAL, the Collaboration of Oak Ridge, Argonne and Livermore, is fielding two similar IBM systems, Summit and Sierra, with NVIDIA GPUs that will replace the existing Titan and Sequoia systems. Summit and Sierra are currently ranked No. 1 and No. 3, respectively, on the Top500 list. We discuss the design and key differences of the systems. Our evaluation of the systems highlights the following. Applications that fit in HBM see the most benefit and may prefer more GPUs; however, for some applications, the CPU-GPU bandwidth is more important than the number of GPUs. The node-local burst buffer scales linearly, and can achieve a 4X improvement over the parallel file system for large jobs; smaller jobs, however, may benefit from writing directly to the PFS. Finally, several CPU, network and memory bound analytics and GPU-bound deep learning codes achieve up to a 11X and 79X speedup/node, respectively over Titan.</blockquote></div></div></div></div><a href="includes/files/pap277s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap449"></a><div class="slot-title">Best Practices and Lessons from Deploying and Operating a Sustained-Petascale System: The Blue Waters Experience</div><div class="slot-authors">Gregory H. Bauer, Brett Bode, Jeremy Enos, William T. Kramer, Scott Lathrop, Celso L. Mendes, and Roberto R. Sisneros (University of Illinois, National Center for Supercomputing Applications)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_828_1539825877_91" onclick="$('#vhsjs_view_828_1539825877_91').hide();
                $('#vhsjs_hide_828_1539825877_91').show();
                $('#827_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_828_1539825877_91" onclick="$('#827_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_828_1539825877_91').hide();
                $('#vhsjs_view_828_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="828_1539825877_91" id="827_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>Building and operating versatile extreme-scale computing systems that work productively for a range of frontier research domains present many challenges and opportunities. Solutions created, experiences acquired, and lessons learned, while rarely published, could drive the development of new methods and practices and raise the bar for all organizations supporting research, scholarship, and education. This paper describes the methods and procedures developed for deploying, supporting, and continuously improving the Blue Waters system and its services during the last five years. Being the first US sustained-petascale computing platform available to the open-science community, the Blue Waters project pioneered various unique practices that we are sharing to be adopted and further improved by the community. We present our support and service methodologies, and the leadership practices employed for ensuring that the system stays highly efficient and productive. We also provide the return on investment summaries related to deploying and operating the system.</blockquote></div></div></div></div><a href="includes/files/pap449s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap346"></a><div class="slot-title">Performance Evaluation of a Vector Supercomputer SX-Aurora TSUBASA</div><div class="slot-authors">Kazuhiko Komatsu (Tohoku University); Shintaro Momose, Yoko Isobe, Osamu Watanabe, and Akihiro Musa (Tohoku University, NEC Corporation); Mitsuo Yokokawa (Kobe University, NEC Corporation); Toshikazu Aoyama (NEC Corporation); and Masayuki Sato and Hiroaki Kobayashi (Tohoku University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_830_1539825877_91" onclick="$('#vhsjs_view_830_1539825877_91').hide();
                $('#vhsjs_hide_830_1539825877_91').show();
                $('#829_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_830_1539825877_91" onclick="$('#829_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_830_1539825877_91').hide();
                $('#vhsjs_view_830_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="830_1539825877_91" id="829_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>A new SX-Aurora TSUBASA vector supercomputer has been released with a new system architecture and a new execution model to achieve high sustained performance, especially for memory-intensive applications. In SX-Aurora TSUBASA, the vector host (VH) of a standard x86 Linux node is attached to the vector engine (VE) of a newly developed vector processor.  An application is executed on the VE, and only system calls are offloaded to the VH. This new execution model can avoid redundant data transfers between a VH and a VE that can easily become a bottleneck in the conventional execution model. This paper examines the potential of SX-Aurora TSUBASA. First, the basic performance of SX-Aurora TSUBASA is clarified by evaluating benchmark programs. Then, the effectiveness of the new execution model is examined by using a microbenchmark.  Finally, the high potential of SX-Aurora TSUBASA is clarified through evaluations of practical applications.</blockquote></div></div></div></div><a href="includes/files/pap346s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Architectures, Compiler Analysis and Optimization, Floating Point, Performance, Precision, Programming Systems, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Arithmetic and Optimization</div><div class="slot-entry"><a name="pap431"></a><div class="slot-title">Associative Instruction Reordering to Alleviate Register Pressure</div><div class="slot-authors">Prashant Singh Rawat, Aravind Sukumaran-Rajam, and Atanas Rountev (Ohio State University); Fabrice Rastello (French Institute for Research in Computer Science and Automation (INRIA)); Louis-Noel Pouchet (Colorado State University); and P. Sadayappan (Ohio State University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_832_1539825877_91" onclick="$('#vhsjs_view_832_1539825877_91').hide();
                $('#vhsjs_hide_832_1539825877_91').show();
                $('#831_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_832_1539825877_91" onclick="$('#831_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_832_1539825877_91').hide();
                $('#vhsjs_view_832_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="832_1539825877_91" id="831_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>Register allocation is generally considered a practically solved problem. For most applications, the register allocation strategies in production compilers are very effective in controlling the number of loads/stores and register spills. However, existing register allocation strategies are not effective and result in excessive register spilling for computation patterns with a high degree of many-to-many data reuse, e.g., high-order stencils and tensor contractions.  We develop a source-to-source instruction reordering strategy that exploits the flexibility of reordering associative operations to alleviate register pressure.  The developed transformation module implements an adaptable strategy that can appropriately control the degree of instruction-level parallelism, while relieving register pressure.  The effectiveness of the approach is demonstrated through experimental results using multiple production compilers (GCC, Clang/LLVM) and target platforms (Intel Xeon Phi, and Intel x86 multi-core).</blockquote></div></div></div></div><a href="includes/files/pap431s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap464"></a><div class="slot-title">Harnessing GPU's Tensor Cores Fast FP16 Arithmetic to Speedup Mixed-Precision Iterative Refinement Solvers</div><div class="slot-authors">Azzam Haidar (University of Tennessee, Innovative Computing Laboratory); Stan Tomov and Jack Dongarra (University of Tennessee); and Nicholas Higham (University of Manchester, School of Mathematics)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_834_1539825877_91" onclick="$('#vhsjs_view_834_1539825877_91').hide();
                $('#vhsjs_hide_834_1539825877_91').show();
                $('#833_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_834_1539825877_91" onclick="$('#833_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_834_1539825877_91').hide();
                $('#vhsjs_view_834_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="834_1539825877_91" id="833_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>The use of low-precision arithmetic in computing methods has been a powerful tool to accelerate numerous scientific computing applications including Artificial Intelligence. We present an investigation showing that other HPC applications can harness this power too, and in particular, the general HPC problem of solving Ax = b, where A is a large dense matrix, and the solution is needed in FP64 accuracy. Our approach is based on the mixed-precision (FP16->FP64) iterative refinement technique – we generalize and extend prior advances into a framework, for which we develop architecture-specific algorithms and highly-tuned implementations where we show how the use of FP16-TC (tensor cores) arithmetic can provide up to 4X speedup and improve the energy consumption by a factor of 5 achieving 74 Gflop/Watt. This is due to the performance boost that the FP16 (Tensor Cores) provide and to its better accuracy that outperforms the classical FP16.</blockquote></div></div></div></div><a href="includes/files/pap464s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap503"></a><div class="slot-title">ADAPT: Algorithmic Differentiation Applied to Floating-Point Precision Tuning</div><div class="slot-authors">Harshitha Menon (Lawrence Livermore National Laboratory); Michael O. Lam (James Madison University, Lawrence Livermore National Laboratory); and Daniel Osei-Kuffuor, Markus Schordan, Scott Lloyd, Kathryn Mohror, and Jeffrey Hittinger (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_836_1539825877_91" onclick="$('#vhsjs_view_836_1539825877_91').hide();
                $('#vhsjs_hide_836_1539825877_91').show();
                $('#835_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_836_1539825877_91" onclick="$('#835_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_836_1539825877_91').hide();
                $('#vhsjs_view_836_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="836_1539825877_91" id="835_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>HPC applications extensively use floating point arithmetic operations to solve computational problems in various domains. Mixed precision computing, use of lowest precision data type sufficient to achieve a desired accuracy, have been explored to improve performance, reduce power consumption and data movement. Manually optimizing the program to use mixed precision is challenging. In this work, we present ADAPT, an approach for mixed precision analysis on HPC workloads while providing guarantees about the final output error. Our approach uses algorithmic differentiation to accurately estimate the output error for mixed precision configuration. ADAPT provides floating-point precision sensitivity of programs, which highlights regions of the code that that can potentially be converted to lower precision, is used to make algorithmic choices and develop mixed precision configurations. We evaluate ADAPT on six benchmarks and a proxy application and show that we are able to achieve a speedup of 1.2x on the proxy application, LULESH.</blockquote></div></div></div></div><a href="includes/files/pap503s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Linear Algebra, Memory, MPI, OpenMP, Programming Systems, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Programming Systems Tools</div><div class="slot-entry"><a name="pap179"></a><div class="slot-title">Dynamic Data Race Detection for OpenMP Programs</div><div class="slot-authors">Yizi Gu and John Mellor-Crummey (Rice University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_838_1539825877_91" onclick="$('#vhsjs_view_838_1539825877_91').hide();
                $('#vhsjs_hide_838_1539825877_91').show();
                $('#837_1539825877_91').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_838_1539825877_91" onclick="$('#837_1539825877_91').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_838_1539825877_91').hide();
                $('#vhsjs_view_838_1539825877_91').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="838_1539825877_91" id="837_1539825877_91" style="display: none"><div class="arrow-slidedown"><blockquote>Two concurrent accesses to a shared variable that are unordered by synchronization are said to be a data race if at least one access is a write. Data races cause shared memory parallel programs to behave unpredictably. This paper describes ROMP -- a tool for detecting data races in executions of scalable parallel applications that employ OpenMP for node-level parallelism. The complexity of OpenMP, which includes primitives for managing data environments, SPMD and SIMD parallelism, work sharing, tasking, mutual exclusion, and ordering, presents a formidable challenge for data race detection. ROMP is a hybrid data race detector that tracks accesses, access orderings, and mutual exclusion. Unlike other OpenMP race detectors, ROMP detects races with respect to logical parallelism rather than implementation threads. Experiments show that ROMP yields precise race reports for a broader set of OpenMP constructs than prior state-of-the-art race detectors.</blockquote></div></div></div></div><a href="includes/files/pap179s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap256"></a><div class="slot-title">ParSy: Inspection and Transformation of Sparse Matrix Computations for Parallelism</div><div class="slot-authors">Kazem Cheshmi (University of Toronto), Shoaib Kamil (Adobe Research), Michelle Mills Strout (University of Arizona), and Maryam Mehri Dehnavi (University of Toronto)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_840_1539825877_92" onclick="$('#vhsjs_view_840_1539825877_92').hide();
                $('#vhsjs_hide_840_1539825877_92').show();
                $('#839_1539825877_92').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_840_1539825877_92" onclick="$('#839_1539825877_92').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_840_1539825877_92').hide();
                $('#vhsjs_view_840_1539825877_92').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="840_1539825877_92" id="839_1539825877_92" style="display: none"><div class="arrow-slidedown"><blockquote>In this work, we describe ParSy, a framework that uses a novel inspection strategy along with a simple code transformation to optimize parallel sparse algorithms for shared memory processors. Unlike existing approaches that can suffer from load imbalance and excessive synchronization, ParSy uses a novel task coarsening strategy to create well-balanced tasks that can execute in parallel, while maintaining locality of memory accesses. Code using the ParSy inspector and transformation outperforms existing highly-optimized sparse matrix algorithms such as Cholesky factorization on multi-core processors with speedups of 2.8× and 3.1× over the MKL Pardiso and PaStiX libraries respectively.</blockquote></div></div></div></div><a href="includes/files/pap256s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap382"></a><div class="slot-title">Detecting MPI Usage Anomalies via Partial Program Symbolic Execution</div><div class="slot-authors">Fangke Ye, Jisheng Zhao, and Vivek Sarkar (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_842_1539825877_92" onclick="$('#vhsjs_view_842_1539825877_92').hide();
                $('#vhsjs_hide_842_1539825877_92').show();
                $('#841_1539825877_92').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_842_1539825877_92" onclick="$('#841_1539825877_92').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_842_1539825877_92').hide();
                $('#vhsjs_view_842_1539825877_92').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="842_1539825877_92" id="841_1539825877_92" style="display: none"><div class="arrow-slidedown"><blockquote>MPI is a message passing based programming model for distributed-memory parallelism that has been had been widely used for programming supercomputers for over 25 years. However, debugging and verification of MPI programs is widely recognized to be a deep technical challenge. This challenge is further exacerbated by a recent increase in the use of nonblocking MPI operations that bring new classes of bugs related to data races.<br><br>In this paper, we introduce a new MPI program debugging approach based on partial symbolic execution so as to avoid the false alarms inherent in the static analysis based methodology. Compared with the dynamic approach, our approach can be applied to incomplete programs and explore multiple execution paths, thereby bringing more flexibility and precision. By comparing with well known static/dynamic tools on real-world MPI applications, our approach shows same precision as the dynamic tool and avoids false positive produced by the static tool.</blockquote></div></div></div></div><a href="includes/files/pap382s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack166"></a><div class="section-title">Visualization</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, Data Analytics, Deep Learning, Networks, Scientific Computing, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Large-Scale Algorithms</div><div class="slot-entry"><a name="pap171"></a><div class="slot-title">Large-Scale Hierarchical K-Means for Heterogeneous Many-Core Supercomputers</div><div class="slot-authors">Liandeng Li (Tsinghua University; National Supercomputing Center, Wuxi); Teng Yu (University of St Andrews); Wenlai Zhao and Haohuan Fu (Tsinghua University; National Supercomputing Center, Wuxi); Chenyu Wang (University of St Andrews; National Supercomputing Center, Wuxi); Li Tan (Beijing Technology and Business University); Guangwen Yang (Tsinghua University; National Supercomputing Center, Wuxi); and John Thomson (University of St Andrews)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_844_1539825877_92" onclick="$('#vhsjs_view_844_1539825877_92').hide();
                $('#vhsjs_hide_844_1539825877_92').show();
                $('#843_1539825877_92').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_844_1539825877_92" onclick="$('#843_1539825877_92').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_844_1539825877_92').hide();
                $('#vhsjs_view_844_1539825877_92').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="844_1539825877_92" id="843_1539825877_92" style="display: none"><div class="arrow-slidedown"><blockquote>This paper presents a novel design and implementation of k-means clustering algorithm targeting the Sunway TaihuLight supercomputer. We introduce a multi-level parallel partition approach that not only partitions by dataflow and centroid, but also by dimension. Our multi-level (nkd) approach unlocks the potential of the hierarchical parallelism in the SW26010 heterogeneous many-core processor and the system architecture of the supercomputer. <br><br>Our design is able to process large-scale clustering problems with up to 196,608 dimensions and over 160,000 targeting centroids, while maintaining high performance and high scalability, significantly improving the capability of k-means over previous approaches. The evaluation shows our implementation achieves performance of less than 18 seconds per iteration for a large-scale clustering case with 196,608 data dimensions and 2,000 centroids by applying 4,096 nodes (1,064,496 cores) in parallel, making k-means a more feasible solution for complex scenarios.</blockquote></div></div></div></div><a href="includes/files/pap171s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap140"></a><div class="slot-title">TriCore: Parallel Triangle Counting on GPUs</div><div class="slot-authors">Yang Hu (George Washington University); Hang Liu (University of Massachusetts, Lowell); and H. Howie Huang (George Washington University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_846_1539825877_92" onclick="$('#vhsjs_view_846_1539825877_92').hide();
                $('#vhsjs_hide_846_1539825877_92').show();
                $('#845_1539825877_92').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_846_1539825877_92" onclick="$('#845_1539825877_92').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_846_1539825877_92').hide();
                $('#vhsjs_view_846_1539825877_92').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="846_1539825877_92" id="845_1539825877_92" style="display: none"><div class="arrow-slidedown"><blockquote>Triangle counting algorithm enumerates the triangles in a graph by identifying the common neighbors between two vertices of every edge. In this work, we present TriCore, a new GPU-based high-performance and scalable triangle counting system that consists of three main techniques. First, we design a binary search based counting algorithm that tremendously increases both thread parallelism and memory performance. Second, TriCore exploits a 2-D partition method to distribute the CSR representation across multiple GPUs, combined with a new streaming buffer to load the edge list from outside of GPUs. Third, we develop a dynamic workload management technique to balance the workload across multiple GPUs. Our evaluation demonstrates TriCore is 22× faster than the state-of-the-art parallel triangle counting projects. In addition, TriCore can not only process big graphs that are significant larger than the memory size of one GPU but also achieve 24× speedup when scaling to 32 GPUs.</blockquote></div></div></div></div><a href="includes/files/pap140s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap141"></a><div class="slot-title">Distributed-Memory Hierarchical Compression of Dense SPD Matrices</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Chenhan D. Yu (University of Texas), Severin Reiz (Technical University Munich), and George Biros (University of Texas)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_848_1539825877_92" onclick="$('#vhsjs_view_848_1539825877_92').hide();
                $('#vhsjs_hide_848_1539825877_92').show();
                $('#847_1539825877_92').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_848_1539825877_92" onclick="$('#847_1539825877_92').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_848_1539825877_92').hide();
                $('#vhsjs_view_848_1539825877_92').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="848_1539825877_92" id="847_1539825877_92" style="display: none"><div class="arrow-slidedown"><blockquote>We present a distributed-memory algorithm for the hierarchical compression of SPD matrices. Our method is based on GOFMM, an algorithm that appeared in doi:10.1145/3126908.3126921.<br><br>For many SPD matrices, GOFMM enables compression and approximate matrix-vector multiplication in NlogN time---as opposed to quadratic work required for a dense matrix. But GOFMM supports only shared memory parallelism. In this paper, we use the message passing interface, extending the ideas of GOFMM to the distributed memory setting. We also introduce an asynchronous algorithm for faster multiplication. We present different usage scenarios of SPD matrices that are related to graphs, neural-networks, and covariance operators. We also compare with STRUMPACK, which, to our knowledge, is the only other parallel software that can compress arbitrary SPD matrices. In our largest run, we were able to compress a 67M-by-67M matrix within three minutes and perform a multiplication with 512 vectors within 5 seconds on 6,144 Intel Skylake cores.</blockquote></div></div></div></div><a href="includes/files/pap141s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Data Analytics, Performance, Programming Systems, Storage, Tools, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Performance Optimization Studies</div><div class="slot-entry"><a name="pap335"></a><div class="slot-title">Many-Core Graph Workload Analysis</div><div class="slot-authors">Stijn Eyerman, Wim Heirman, Kristof Du Bois, Joshua B. Fryman, and Ibrahim Hur (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_850_1539825877_92" onclick="$('#vhsjs_view_850_1539825877_92').hide();
                $('#vhsjs_hide_850_1539825877_92').show();
                $('#849_1539825877_92').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_850_1539825877_92" onclick="$('#849_1539825877_92').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_850_1539825877_92').hide();
                $('#vhsjs_view_850_1539825877_92').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="850_1539825877_92" id="849_1539825877_92" style="display: none"><div class="arrow-slidedown"><blockquote>Graph applications have specific characteristics that are not common in other application domains. In this paper, we analyze multiple graph applications on current multi- and many-core processors and provide conclusions and recommendations for future designs. We provide new insights on executing graph applications on many-core processors.<br><br>Our main novel observations are (i) some memory streams do show locality, while others show no locality, (ii) thread imbalance becomes a major problem with many threads, and (iii) many threads are required to saturate high-bandwidth memories. We recommend a selective memory access policy, where accesses with locality are cached and prefetched, while accesses without locality can remain uncached to save cache capacity. Additionally, more threads are needed, but they are not used efficiently due to thread imbalance. Our recommendation is to revise the graph analysis algorithms to provide more parallelism, and to provide a few high-performance cores that speedup sections with low parallelism.</blockquote></div></div></div></div><a href="includes/files/pap335s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap488"></a><div class="slot-title">Lessons Learned from Analyzing Dynamic Promotion for User-Level Threading</div><div class="slot-authors">Shintaro Iwasaki (University of Tokyo), Abdelhalim Amer (Argonne National Laboratory), Kenjiro Taura (University of Tokyo), and Pavan Balaji (Argonne National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_852_1539825877_92" onclick="$('#vhsjs_view_852_1539825877_92').hide();
                $('#vhsjs_hide_852_1539825877_92').show();
                $('#851_1539825877_92').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_852_1539825877_92" onclick="$('#851_1539825877_92').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_852_1539825877_92').hide();
                $('#vhsjs_view_852_1539825877_92').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="852_1539825877_92" id="851_1539825877_92" style="display: none"><div class="arrow-slidedown"><blockquote>A performance vs. practicality trade-off exists between user-level threading techniques. The community has settled mostly on a black-and-white perspective; fully fledged threads assume that suspension is imminent and incur overheads when suspension does not take place, and run-to-completion threads are more lightweight but less practical since they cannot suspend. Gray areas exist, however, whereby threads can start with minimal capabilities and then can be dynamically promoted to acquire additional capabilities when needed. This paper investigates the full spectrum of threading techniques from a performance vs. practicality trade-off perspective on modern multicore and many-core systems. Our results indicate that achieving the best trade-off highly depends on the suspension likelihood; dynamic promotion is more appropriate when suspension is unlikely and represents a solid replacement for run to completion, thanks to its lower programming constraints, while fully fledged threads remain the technique of choice when suspension likelihood is high.</blockquote></div></div></div></div><a href="includes/files/pap488s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap484"></a><div class="slot-title">Topology-Aware Space-Shared Co-Analysis of Large-Scale Molecular Dynamics Simulations</div><div class="slot-authors">Preeti Malakar (Indian Institute of Technology Kanpur); Todd Munson, Christopher Knight, and Venkatram Vishwanath (Argonne National Laboratory); and Michael E. Papka (Argonne National Laboratory, Northern Illinois University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_854_1539825877_92" onclick="$('#vhsjs_view_854_1539825877_92').hide();
                $('#vhsjs_hide_854_1539825877_92').show();
                $('#853_1539825877_92').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_854_1539825877_92" onclick="$('#853_1539825877_92').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_854_1539825877_92').hide();
                $('#vhsjs_view_854_1539825877_92').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="854_1539825877_92" id="853_1539825877_92" style="display: none"><div class="arrow-slidedown"><blockquote>Analysis of scientific simulation data can be concurrently executed with simulation either in time- or space-shared mode. This mitigates the I/O bottleneck.  However it results in either stalling the simulation for performing the analysis or transferring data for analysis. In this paper, we improve the throughput of space-shared in situ analysis of large-scale simulations by topology-aware mapping and optimal process decomposition. We propose node interconnect topology-aware process placement for simulation and analysis to reduce the data movement time. We also present an integer linear program for optimal 3D decompositions of simulation and analysis processes. We demonstrate our approach using molecular dynamics simulation on Mira, Cori and Theta supercomputers. Our mapping schemes, combined with optimal 3D process decomposition and code optimizations resulted in up to 30% lower execution times for space-shared in situ analysis than the default approach. Our mappings also reduce MPI collective I/O times by 10-40%.</blockquote></div></div></div></div><a href="includes/files/pap484s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Applications, Cosmology, Data Analytics, Deep Learning, Machine Learning, Programming Systems, Storage, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Deep Learning</div><div class="slot-entry"><a name="pap425"></a><div class="slot-title">Exploring Flexible Communications for Streamlining DNN Ensemble Training Pipelines</div><div class="slot-authors">Randall Pittman, Hui Guan, and Xipeng Shen (North Carolina State University) and Seung-Hwan Lim and Robert M. Patton (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_856_1539825877_92" onclick="$('#vhsjs_view_856_1539825877_92').hide();
                $('#vhsjs_hide_856_1539825877_92').show();
                $('#855_1539825877_92').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_856_1539825877_92" onclick="$('#855_1539825877_92').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_856_1539825877_92').hide();
                $('#vhsjs_view_856_1539825877_92').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="856_1539825877_92" id="855_1539825877_92" style="display: none"><div class="arrow-slidedown"><blockquote>Parallel training of a Deep Neural Network (DNN) ensemble on a cluster of nodes is a common practice to train multiple models in order to construct a model with a higher prediction accuracy. Existing ensemble training pipelines can perform a great deal of redundant operations, resulting in unnecessary CPU usage, or even poor pipeline performance.  In order to remove these redundancies, we need pipelines with more communication flexibility than existing DNN frameworks provide.<br><br>This project investigates a series of designs to improve pipeline flexibility and adaptivity, while also increasing performance. We implement our designs using Tensorflow with Horovod, and test it using several large DNNs. Our results show that the CPU time spent during training is reduced by 2-11X. Furthermore, our implementation can achieve up to 10X speedups when CPU core limits are imposed. Our best pipeline also reduces the average power draw of the ensemble training process by 5-16%.</blockquote></div></div></div></div><a href="includes/files/pap425s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap429"></a><div class="slot-title">CosmoFlow: Using Deep Learning to Learn the Universe at Scale</div><div class="slot-authors">Amrita Mathuriya (Intel Corporation); Deborah Bard (National Energy Research Scientific Computing Center (NERSC), Lawrence Berkeley National Laboratory); Pete Mendygral (Cray Inc); Lawrence Meadows (Intel Corporation); James Arnemann (University of California, Berkeley); Lei Shao (Intel Corporation); Siyu He (Carnegie Mellon University); Tuomas Karna (Intel Corporation); Diana Moise (Cray Inc); Simon J. Pennycook (Intel Corporation); Kristyn Maschhoff (Cray Inc); Jason Sewall and Nalini Kumar (Intel Corporation); Shirley Ho (Lawrence Berkeley National Laboratory, Carnegie Mellon University); Michael F. Ringenburg (Cray Inc); Mr Prabhat (Lawrence Berkeley National Laboratory, National Energy Research Scientific Computing Center (NERSC)); and Victor Lee (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_858_1539825877_92" onclick="$('#vhsjs_view_858_1539825877_92').hide();
                $('#vhsjs_hide_858_1539825877_92').show();
                $('#857_1539825877_92').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_858_1539825877_92" onclick="$('#857_1539825877_92').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_858_1539825877_92').hide();
                $('#vhsjs_view_858_1539825877_92').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="858_1539825877_92" id="857_1539825877_92" style="display: none"><div class="arrow-slidedown"><blockquote>Deep learning is a promising tool to determine the physical model that describes our universe.   To handle the considerable computational cost of this problem, we present CosmoFlow: a highly scalable deep learning application built on top of the TensorFlow framework.<br><br>CosmoFlow uses efficient implementations of 3D convolution and pooling primitives, together with improvements in threading for many element-wise operations, to improve training performance on Intel Xeon Phi processors.  We also utilize the Cray PE Machine Learning Plugin for efficient scaling to multiple nodes. We demonstrate fully synchronous data-parallel training on 8192 nodes of Cori with 77% parallel efficiency, achieving 3.5 Pflop/s sustained performance. <br><br>To our knowledge, this is the first large-scale science application of the TensorFlow framework at supercomputer scale with fully-synchronous training. These enhancements enable us to process large 3D dark matter distribution and predict the cosmological parameters Omega_M, sigma_8 and N_s with unprecedented accuracy.</blockquote></div></div></div></div><a href="includes/files/pap429s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap322"></a><div class="slot-title">Anatomy of High-Performance Deep Learning Convolutions on SIMD Architectures</div><div class="slot-authors">Evangelos Georganas, Sasikanth Avancha, Kunal Banerjee, Dhiraj Kalamkar, Greg Henry, Hans Pabst, and Alexander Heinecke (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_860_1539825877_92" onclick="$('#vhsjs_view_860_1539825877_92').hide();
                $('#vhsjs_hide_860_1539825877_92').show();
                $('#859_1539825877_92').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_860_1539825877_92" onclick="$('#859_1539825877_92').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_860_1539825877_92').hide();
                $('#vhsjs_view_860_1539825877_92').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="860_1539825877_92" id="859_1539825877_92" style="display: none"><div class="arrow-slidedown"><blockquote>Convolution layers are prevalent in many classes of deep neural networks, including Convolutional Neural Networks (CNNs) which provide state-of-the-art results for tasks like image recognition, neural machine translation, and speech recognition. The computationally expensive nature of a convolution operation has led to the proliferation of implementations including matrix-matrix multiplication formulation, and direct convolution primarily targeting GPUs. In this paper, we introduce direct convolution kernels for x86 architectures, in particular for Xeon and Xeon Phi systems, which are implemented via a dynamic compilation approach. Our JIT-based implementation shows close to theoretical peak performance, depending on the setting and the CPU architecture at hand. We additionally demonstrate how these JIT-optimized kernels can be integrated into a light-weight multi-node graph execution model. This illustrates that single- and multi-node runs yield high efficiencies and high image-throughputs  when executing state of the art image recognition tasks on CPUs.</blockquote></div></div></div></div><a href="includes/files/pap322s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack168"></a><div class="section-title">Workflows</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Data Management, File Systems, Networks, State of the Practice, System Software, Workflows, Tech Program Reg Pass</span><br /><div class="session-title">File Systems: Data Movement and Provenance</div><div class="slot-entry"><a name="pap407"></a><div class="slot-title">Dac-Man: Data Change Management for Scientific Datasets on HPC Systems</div><div class="slot-authors">Devarshi Ghoshal, Lavanya Ramakrishnan, and Deborah Agarwal (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_862_1539825877_92" onclick="$('#vhsjs_view_862_1539825877_92').hide();
                $('#vhsjs_hide_862_1539825877_92').show();
                $('#861_1539825877_92').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_862_1539825877_92" onclick="$('#861_1539825877_92').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_862_1539825877_92').hide();
                $('#vhsjs_view_862_1539825877_92').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="862_1539825877_92" id="861_1539825877_92" style="display: none"><div class="arrow-slidedown"><blockquote>Scientific data is growing rapidly and often changes due to instrument configurations, software updates, or quality assessments. These changes in datasets can result in significant waste of compute and storage resources on HPC systems as downstream pipelines are reprocessed. Data changes need to be detected, tracked, and analyzed for understanding the impact of data change, managing data provenance, and making efficient and effective decisions about reprocessing and use of HPC resources. Existing methods for identifying and capturing change are often manual, domain-specific, and error-prone and do not scale to large scientific datasets. In this paper, we describe the design and implementation of Dac-Man framework, which identifies, captures, and manages change in large scientific datasets, and enables plug-in of domain-specific change analysis with minimal user effort. Our evaluations show that it can retrieve file changes from directories containing millions of files and terabytes of data in less than a minute.</blockquote></div></div></div></div><a href="includes/files/pap407s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap521"></a><div class="slot-title">Stacker: An Autonomic Data Movement Engine for Extreme-Scale Data Staging-Based In Situ Workflows</div><div class="slot-authors">Pradeep Subedi, Philip Davis, and Shaohua Duan (Rutgers University); Scott Klasky (Oak Ridge National Laboratory); Hemanth Kolla (Sandia National Laboratories); and Manish Parashar (Rutgers University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_864_1539825877_92" onclick="$('#vhsjs_view_864_1539825877_92').hide();
                $('#vhsjs_hide_864_1539825877_92').show();
                $('#863_1539825877_92').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_864_1539825877_92" onclick="$('#863_1539825877_92').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_864_1539825877_92').hide();
                $('#vhsjs_view_864_1539825877_92').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="864_1539825877_92" id="863_1539825877_92" style="display: none"><div class="arrow-slidedown"><blockquote>Data staging and in situ workflows are being explored extensively as an approach to address data-related costs at very large scales. However, the impact of emerging storage architectures (e.g., deep memory hierarchies and burst buffers) upon data staging solutions remains a challenge. In this paper, we investigate how burst buffers can be effectively used by data staging solutions, for example, as a persistence storage tier of the memory hierarchy. Furthermore, we use machine learning based prefetching techniques to move data between the storage levels in an autonomous manner. We also present Stacker, a prototype of the proposed solutions implemented within the Data\-Spaces data staging service, and experimentally evaluate its performance and scalability using the S3D combustion workflow on current leadership class platforms. Our experiments demonstrate that Stacker achieves low latency, high volume data-staging with low overhead as compared to in-memory staging services for production scientific workflows.</blockquote></div></div></div></div><a href="includes/files/pap521s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap206"></a><div class="slot-title">A Year in the Life of a Parallel File System</div><div class="slot-authors">Glenn K. Lockwood (Lawrence Berkeley National Laboratory), Shane Snyder (Argonne National Laboratory), Teng Wang and Suren Byna (Lawrence Berkeley National Laboratory), Philip Carns (Argonne National Laboratory), and Nicholas J. Wright (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_866_1539825877_92" onclick="$('#vhsjs_view_866_1539825877_92').hide();
                $('#vhsjs_hide_866_1539825877_92').show();
                $('#865_1539825877_92').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_866_1539825877_92" onclick="$('#865_1539825877_92').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_866_1539825877_92').hide();
                $('#vhsjs_view_866_1539825877_92').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="866_1539825877_92" id="865_1539825877_92" style="display: none"><div class="arrow-slidedown"><blockquote>I/O performance is a critical aspect of data-intensive scientific computing.  We seek to advance the state of the practice in understanding and diagnosing I/O performance issues through investigation of a comprehensive I/O performance data set that captures a full year of production storage activity at two leadership-scale computing facilities.  We demonstrate techniques to identify regions of interest, perform focused investigations of both long-term trends and transient anomalies, and uncover the contributing factors that lead to performance fluctuation.<br><br>We find that a year in the life of a parallel file system is comprised of distinct regions of long-term performance variation in addition to short-term performance transients.  We demonstrate how systematic identification of these performance regions, combined with comprehensive analysis, allows us to isolate the factors contributing to different performance maladies at different time scales.  From this, we present specific lessons learned and important considerations for HPC storage practitioners.</blockquote></div></div></div></div><a href="includes/files/pap206s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="ptrack179"></a><div class="section-title">Tech Program Reg Pass</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Data Analytics, Networks, Tech Program Reg Pass</span><br /><div class="session-title">Next-Generation Networking</div><div class="slot-entry"><a name="pap147"></a><div class="slot-title">Exploiting Idle Resources in a High-Radix Switch for Supplemental Storage</div><div><span class="BP award">Best Paper Finalists</span></div><div class="slot-authors">Matthias A. Blumrich, Nan Jiang, and Larry R. Dennison (Nvidia Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_868_1539825877_92" onclick="$('#vhsjs_view_868_1539825877_92').hide();
                $('#vhsjs_hide_868_1539825877_92').show();
                $('#867_1539825877_92').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_868_1539825877_92" onclick="$('#867_1539825877_92').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_868_1539825877_92').hide();
                $('#vhsjs_view_868_1539825877_92').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="868_1539825877_92" id="867_1539825877_92" style="display: none"><div class="arrow-slidedown"><blockquote>A general-purpose switch for a high-performance network is usually designed with symmetric ports providing credit-based flow control and error recovery via link-level retransmission. Because port buffers must be sized for the longest links and modern asymmetric network topologies have a wide range of link lengths, we observe that there can be a significant amount of unused buffer memory, particularly in edge switches. We also observe that the tiled architecture used in many high-radix switches contains an abundance of internal bandwidth. We combine these observations to create a new switch architecture that allows ports to stash packets in unused buffers on other ports, accessible via excess internal bandwidth in the tiled switch. We explore this architecture through two use cases: end-to-end resilience and congestion mitigation. We find that stashing is highly effective and does not negatively impact network performance.</blockquote></div></div></div></div><a href="includes/files/pap147s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap163"></a><div class="slot-title">Fine-Grained, Multi-Domain Network Resource Abstraction as a Fundamental Primitive to Enable High-Performance, Collaborative Data Sciences</div><div class="slot-authors">Qiao Xiang (Yale University); J. Jensen Zhang, X. Tony Wang, and Y. Jace Liu (Tongji University); Chin Guok (Lawrence Berkeley National Laboratory); Franck Le (IBM); John MacAuley (Lawrence Berkeley National Laboratory); Harvey Newman (California Institute of Technology); and Y. Richard Yang (Yale University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_870_1539825877_92" onclick="$('#vhsjs_view_870_1539825877_92').hide();
                $('#vhsjs_hide_870_1539825877_92').show();
                $('#869_1539825877_92').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_870_1539825877_92" onclick="$('#869_1539825877_92').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_870_1539825877_92').hide();
                $('#vhsjs_view_870_1539825877_92').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="870_1539825877_92" id="869_1539825877_92" style="display: none"><div class="arrow-slidedown"><blockquote>Multi-domain network resource reservation systems are being deployed, driven by the demand and substantial benefits of providing predictable network resources. However, a major lack of existing systems is their coarse granularity, due to the participating networks’ concern of revealing sensitive information, which can result in substantial inefficiencies. This paper presents Mercator, a novel multi-domain network resource discovery system to provide fine-grained, global network resource information, for collaborative sciences. The foundation of Mercator is a resource abstraction through algebraic-expression enumeration (i.e., linear inequalities/equations), as a compact representation of the available bandwidth in multi-domain networks. In addition, we develop an obfuscating protocol, to address the privacy concerns by ensuring that no participant can associate the algebraic expressions with the corresponding member networks. We also introduce a superset projection technique to increase Mercator’s scalability. Finally, we implement Mercator and demonstrate both its efficiency and efficacy through extensive experiments using real topologies and traces.</blockquote></div></div></div></div><a href="includes/files/pap163s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap157"></a><div class="slot-title">Light-Weight Protocols for Wire-Speed Ordering</div><div class="slot-authors">Hans Eberle and Larry Dennison (Nvidia Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_872_1539825877_92" onclick="$('#vhsjs_view_872_1539825877_92').hide();
                $('#vhsjs_hide_872_1539825877_92').show();
                $('#871_1539825877_92').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_872_1539825877_92" onclick="$('#871_1539825877_92').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_872_1539825877_92').hide();
                $('#vhsjs_view_872_1539825877_92').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="872_1539825877_92" id="871_1539825877_92" style="display: none"><div class="arrow-slidedown"><blockquote>We describe light-weight protocols for selective packet ordering in out-of-order networks that carry memory traffic. The protocols are designed for heterogeneous high-performance systems, in particular, accelerated systems with endpoints that have few resources available for interfacing the network.<br><br>The protocols preserve the semantics of a relaxed memory ordering model as adopted by highly-threaded many-core processors and accelerators.<br><br>The protocols achieve link-rate performance through the following techniques: (1) speculative connection setup avoids round-trip delays found in protocols with little knowledge about endpoint resources, (2) target-side ordering avoids round-trip delays found in source-side ordering mechanisms, (3) fine-grained ordering removes dependencies unwarranted by program code avoiding cumulative ordering dependencies caused by coarse-grained ordering, (4) ordering relaxations and optimizations for producer/consumer communication patterns.<br><br>We describe two ordering protocols that provide (1) strict sequential ordering and (2) relaxed ordering for multi-packet transfers. The protocols impose no restrictions on routing, including multipath routing.</blockquote></div></div></div></div><a href="includes/files/pap157s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">GPUs, Resiliency, State of the Practice, System Software, Tech Program Reg Pass</span><br /><div class="session-title">Resilience</div><div class="slot-entry"><a name="pap262"></a><div class="slot-title">GPU Age-Aware Scheduling to Improve the Reliability of Leadership Jobs on Titan</div><div class="slot-authors">Christopher Zimmer, Don Maxwell, Stephen McNally, Scott Atchley, and Sudharshan S. Vazhkudai (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_874_1539825877_93" onclick="$('#vhsjs_view_874_1539825877_93').hide();
                $('#vhsjs_hide_874_1539825877_93').show();
                $('#873_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_874_1539825877_93" onclick="$('#873_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_874_1539825877_93').hide();
                $('#vhsjs_view_874_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="874_1539825877_93" id="873_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>The increasing rate of failures on the Oak Ridge Leadership Computing Facility's (OLCF) Titan supercomputer, resulted in the replacement of 50% of its GPUs between 2015 and 2017. The largest jobs, also known as "leadership jobs'', continued to experience increased application failures. These jobs contained significant amounts of low-failure rate and high-failure rate GPUs. The impacts of these failures were felt more by leadership jobs due to longer wait times, runtimes, and higher charge rates. In this work, we have designed techniques to increase the use of low-failure GPUs in leadership jobs through targeted resource allocation. This employed two complementary techniques, updating both the system ordering and the allocation mechanisms. In simulation, the application of these techniques resulted in a 33% increase in low-failure GPU hours being assigned to leadership jobs. Our GPU Age-Aware Scheduling has been used in production on Titan since July of 2017.</blockquote></div></div></div></div><a href="includes/files/pap262s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap109"></a><div class="slot-title">FlipTracker: Understanding Natural Error Resilience in HPC Applications</div><div class="slot-authors">Luanzheng Guo and Dong Li (University of California, Merced); Ignacio Laguna (Lawrence Livermore National Laboratory); and Martin Schulz (Technical University Munich)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_876_1539825877_93" onclick="$('#vhsjs_view_876_1539825877_93').hide();
                $('#vhsjs_hide_876_1539825877_93').show();
                $('#875_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_876_1539825877_93" onclick="$('#875_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_876_1539825877_93').hide();
                $('#vhsjs_view_876_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="876_1539825877_93" id="875_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>As high-performance computing systems scale in size and computational power, the danger of silent errors, i.e., errors that can bypass hardware detection mechanisms and impact application state, grows dramatically. Consequently, applications running on HPC systems need to exhibit resilience to such errors. Previous work has found that, for certain codes, this resilience can come for free, i.e., some applications are naturally resilient, but few works have shown the code patterns—combinations or sequences of computations—that make an application naturally resilient. In this paper, we present FlipTracker, a framework designed to extract these patterns using fine-grained tracking of error propagation and resilience properties, and we use it to present a set of computation patterns that are responsible for making representative HPC applications naturally resilient to errors. This not only enables a deeper understanding of resilience properties of these codes, but also can guide future application designs toward patterns with natural resilience.</blockquote></div></div></div></div><a href="includes/files/pap109s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap111"></a><div class="slot-title">Doomsday: Predicting Which Node Will Fail When on Supercomputers</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Anwesha Das and Frank Mueller (North Carolina State University) and Paul Hargrove, Eric Roman, and Scott Baden (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_878_1539825877_93" onclick="$('#vhsjs_view_878_1539825877_93').hide();
                $('#vhsjs_hide_878_1539825877_93').show();
                $('#877_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_878_1539825877_93" onclick="$('#877_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_878_1539825877_93').hide();
                $('#vhsjs_view_878_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="878_1539825877_93" id="877_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>Predicting which node will fail and how soon remains a challenge for HPC resilience, yet may pave the way to exploiting proactive remedies before jobs fail. Not only for increasing scalability up to exascale systems, but even for contemporary supercomputer architectures does it require substantial efforts to distill anomalous events from noisy raw logs. To this end, we propose a novel phrase extraction mechanism called TBP (time-based phrases) to pin-point node failures, which is unprecedented.  Our study, based on real system data and statistical machine learning, demonstrates the feasibility to predict which specific node will fail in Cray systems. TBP achieves no less than 83% recall rates with lead times as high as 2 minutes. This opens up the door for enhancing prediction lead times for supercomputing systems in general, thereby facilitating efficient usage of both computing capacity and power in large scale production systems.</blockquote></div></div></div></div><a href="includes/files/pap111s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Clouds and Distributed Computing, File Systems, I/O, Storage, Tech Program Reg Pass</span><br /><div class="session-title">Data and Storage</div><div class="slot-entry"><a name="pap165"></a><div class="slot-title">SP-Cache: Load-Balanced, Redundancy-Free Cluster Caching with Selective Partition</div><div class="slot-authors">Yinghao Yu, Renfei Huang, Wei Wang, Jun Zhang, and Khaled Ben Letaief (Hong Kong University of Science and Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_880_1539825877_93" onclick="$('#vhsjs_view_880_1539825877_93').hide();
                $('#vhsjs_hide_880_1539825877_93').show();
                $('#879_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_880_1539825877_93" onclick="$('#879_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_880_1539825877_93').hide();
                $('#vhsjs_view_880_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="880_1539825877_93" id="879_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>Data-intensive clusters increasingly employ in-memory solutions to improve I/O performance. However, the routinely observed file popularity skew and load imbalance create hotspots, which significantly degrades the benefits of in-memory solutions. Common approaches to tame load imbalance include copying multiple replicas of hot files and creating parity chunks using storage codes. Yet, these techniques either suffer from high memory redundancy or incur non-trivial encoding/decoding overhead. In this paper, we propose a different approach to achieve load balancing without memory redundancy or encoding/decoding overhead. Our solution, termed SP-Cache, selectively partitions files based on their popularity and evenly caches those partitions across the cluster. We develop an efficient algorithm to determine the optimal number of partitions for hot files—too few partitions are incapable of mitigating hotspots, while too many are susceptible to stragglers. EC2 deployment and trace-driven simulations show that, compared with existing solutions, SP-Cache reduces the read latencies by up to 40%.</blockquote></div></div></div></div><a href="includes/files/pap165s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap585"></a><div class="slot-title">BESPOKV: Application Tailored Scale-Out Key-Value Stores</div><div class="slot-authors">Ali Anwar (IBM), Yue Cheng (George Mason University), Hai Huang (IBM), Jingoo Han (Virginia Tech), Hyogi Sim (Oak Ridge National Laboratory), Dongyoon Lee (Virginia Tech), Fred Douglis (Perspecta Labs), and Ali R. Butt (Virginia Tech)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_882_1539825877_93" onclick="$('#vhsjs_view_882_1539825877_93').hide();
                $('#vhsjs_hide_882_1539825877_93').show();
                $('#881_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_882_1539825877_93" onclick="$('#881_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_882_1539825877_93').hide();
                $('#vhsjs_view_882_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="882_1539825877_93" id="881_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>Enterprise KV stores are not well suited for HPC applications, and entail customization and cumbersome end-to-end KV design to extract the HPC application needs. In this paper we present BESPOKV, an adaptive, extensible, and scale-out KV store framework. BESPOKV decouples the KV store design into the control plane for distributed management and the data plane for local data store. BESPOKV takes as input a single-server KV store, called a datalet, and transparently enables a scalable and fault-tolerant distributed KV store service. The resulting distributed stores are also adaptive to consistency or topology requirement changes and can be easily extended for new types of services. Experiments show that BESPOKV-enabled distributed KV stores scale horizontally to a large number of nodes, and performs comparably and sometimes better than the state-of-the-art systems.</blockquote></div></div></div></div><a href="includes/files/pap585s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap450"></a><div class="slot-title">Scaling Embedded In Situ Indexing with DeltaFS</div><div class="slot-authors">Qing Zheng, Charles D. Cranor, Danhao Guo, Gregory R. Ganger, George Amvrosiadis, and Garth A. Gibson (Carnegie Mellon University) and Bradley W. Settlemyer, Gary Grider, and Fan Guo (Los Alamos National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_884_1539825877_93" onclick="$('#vhsjs_view_884_1539825877_93').hide();
                $('#vhsjs_hide_884_1539825877_93').show();
                $('#883_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_884_1539825877_93" onclick="$('#883_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_884_1539825877_93').hide();
                $('#vhsjs_view_884_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="884_1539825877_93" id="883_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>Analysis of large-scale simulation output is a core element of scientific inquiry, but analysis queries may experience significant I/O overhead when the data is not structured for efficient retrieval. While in-situ processing allows for improved time-to-insight for many applications, scaling in-situ frameworks to hundreds of thousands of cores can be difficult in practice. The DeltaFS in-situ indexing is a new approach for in-situ processing of massive amounts of data to achieve efficient point and small-range queries. This paper describes the challenges and lessons learned when scaling this in-situ processing function to hundreds of thousands of cores. We propose techniques for scalable all-to-all communication that is memory and bandwidth efficient, concurrent indexing, and specialized LSM-Tree formats. Combining these techniques allows DeltaFS to control the cost of in-situ processing while maintaining 3 orders of magnitude query speedup when scaling alongside the popular VPIC particle-in-cell code to 131,072 cores.</blockquote></div></div></div></div><a href="includes/files/pap450s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Computational Biology, Scientific Computing, Tech Program Reg Pass</span><br /><div class="session-title">Biology Applications</div><div class="slot-entry"><a name="pap410"></a><div class="slot-title">Extreme Scale De Novo Metagenome Assembly</div><div><span class="BP award">Best Paper Finalists</span></div><div class="slot-authors">Evangelos Georganas (Intel Corporation) and Rob Egan, Steven Hofmeyr, Eugene Goltsman, Bill Arndt, Andrew Tritt, Aydin Buluc, Leonid Oliker, and Katherine Yelick (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_886_1539825877_93" onclick="$('#vhsjs_view_886_1539825877_93').hide();
                $('#vhsjs_hide_886_1539825877_93').show();
                $('#885_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_886_1539825877_93" onclick="$('#885_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_886_1539825877_93').hide();
                $('#vhsjs_view_886_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="886_1539825877_93" id="885_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>Metagenome assembly is the process of transforming a set of short, overlapping, and potentially erroneous DNA segments from environmental samples into the accurate representation of the underlying microbiomes's genomes. State-of-the-art tools require large shared memory machines and cannot handle contemporary metagenome datasets that exceed terabytes in size. In this paper, we introduce the metaHipMer pipeline, a high-quality and high-performance metagenome assembler that employs an iterative de Bruijn graph approach. MetaHipMer leverages a specialized scaffolding algorithm that produces long scaffolds and accommodates the idiosyncrasies of metagenomes. MetaHipMer is end-to-end parallelized using the Unified Parallel C language and therefore can run seamlessly on shared and distributed-memory systems. Experimental results show that metaHipMer matches or outperforms the state-of-the-art tools in terms of accuracy. Moreover, metaHipMer scales efficiently to large concurrencies and is able to assemble previously intractable grand challenge metagenomes.</blockquote></div></div></div></div><a href="includes/files/pap410s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap551"></a><div class="slot-title">Optimizing High Performance Distributed Memory Parallel Hash Tables for DNA k-mer Counting</div><div class="slot-authors">Tony C. Pan (Georgia Institute of Technology, School of Computational Science and Engineering); Sanchit Misra (Intel Corporation, Parallel Computing Lab); and Srinivas Aluru (Georgia Institute of Technology, School of Computational Science and Engineering)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_888_1539825877_93" onclick="$('#vhsjs_view_888_1539825877_93').hide();
                $('#vhsjs_hide_888_1539825877_93').show();
                $('#887_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_888_1539825877_93" onclick="$('#887_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_888_1539825877_93').hide();
                $('#vhsjs_view_888_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="888_1539825877_93" id="887_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>High-throughput DNA sequencing is the mainstay of modern genomics research. A common operation used in bioinformatic analysis for many applications of high-throughput sequencing is the counting and indexing of fixed length substrings of DNA sequences called k-mers. Counting k-mers is often accomplished via hashing, and distributed memory k-mer counting algorithms for large data sets are memory access and network communication bound. In this work, we present two optimized distributed parallel hash table techniques that utilize cache friendly algorithms for local hashing, overlapped communication and computation to hide communication costs, and vectorized hash functions that are specialized for k-mer and other short key indices. On 4096 cores of the NERSC Cori supercomputer, our implementation completed index construction and query on an approximately 1 TB human genome dataset in just 11.8 seconds and 5.8 seconds, demonstrating speedups of 2.06x and 3.7x, respectively, over the previous state-of-the-art distributed memory k-mer counter.</blockquote></div></div></div></div><a href="includes/files/pap551s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap120"></a><div class="slot-title">Redesigning LAMMPS for Petascale and Hundred-Billion-Atom Simulation on Sunway TaihuLight</div><div class="slot-authors">Xiaohui Duan, Ping Gao, Tingjian Zhang, Meng Zhang, and Weiguo Liu (Shandong University); Wusheng Zhang, Wei Xue, Haohuan Fu, Lin Gan, and Dexun Chen (Tsinghua University); Xiangxu Meng (Shandong University); and Guangwen Yang (Tsinghua University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_890_1539825877_93" onclick="$('#vhsjs_view_890_1539825877_93').hide();
                $('#vhsjs_hide_890_1539825877_93').show();
                $('#889_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_890_1539825877_93" onclick="$('#889_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_890_1539825877_93').hide();
                $('#vhsjs_view_890_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="890_1539825877_93" id="889_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>Large-scale molecular dynamics (MD) simulations on supercomputers play an increasingly important role in many research areas. In this paper, we present our efforts on redesigning the widely used LAMMPS MD simulator for Sunway TaihuLight supercomputer and its ShenWei many-core architecture (SW26010). The memory constraints of SW26010 bring a number of new challenges for achieving efficient MD implementation on it. In order to overcome these constraints, we employ four levels of optimization: (1) a hybrid memory update strategy; (2) a software cache strategy; (3) customized transcendental math functions; and (4) a full pipeline acceleration. Furthermore, we redesign the code to enable all possible vectorization. Experiments show that our redesigned software on a single SW26010 processor can outperform over 100 E5-2650 cores for running the latest stable release (11Aug17) of LAMMPS. We also achieve a performance of over 2.43 PFlops for a Tersoff simulation when using 16,384 nodes on Sunway TaihuLight.</blockquote></div></div></div></div><a href="includes/files/pap120s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">OpenMP, Performance, Power, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Performance and Energy Analysis</div><div class="slot-entry"><a name="pap175"></a><div class="slot-title">A Parallelism Profiler with What-If Analyses for OpenMP Programs</div><div class="slot-authors">Nader Boushehrinejadmoradi, Adarsh Yoga, and Santosh Nagarakatte (Rutgers University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_892_1539825877_93" onclick="$('#vhsjs_view_892_1539825877_93').hide();
                $('#vhsjs_hide_892_1539825877_93').show();
                $('#891_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_892_1539825877_93" onclick="$('#891_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_892_1539825877_93').hide();
                $('#vhsjs_view_892_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="892_1539825877_93" id="891_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>This paper proposes OMP-WHIP, a profiler that measures inherent parallelism in the program for a given input and provides what-if analyses to estimate improvements in parallelism. We propose a novel OpenMP series parallel graph representation (OSPG) that precisely captures series-parallel relations induced by various directives between different fragments of dynamic execution. OMP-WHIP constructs the OSPG and measures the computation performed by each dynamic fragment using hardware performance counters. This series-parallel representation along with the fine-grained measurement of computation is a performance model of the program for a given input, which enables computation of inherent parallelism. This novel performance model also enables what-if analyses where a programmer can estimate improvements in parallelism when bottlenecks are parallelized. We have used OMP-WHIP to identify parallelism bottlenecks in more than forty applications and then designed strategies to improve the speedup in seven applications.</blockquote></div></div></div></div><a href="includes/files/pap175s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap186"></a><div class="slot-title">Energy Efficiency Modeling of Parallel Applications</div><div class="slot-authors">Mark Endrei, Chao Jin, Minh Ngoc Dinh, and David Abramson (University of Queensland); Heidi Poxon and Luiz DeRose (Cray Inc); and Bronis R. de Supinski (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_894_1539825877_93" onclick="$('#vhsjs_view_894_1539825877_93').hide();
                $('#vhsjs_hide_894_1539825877_93').show();
                $('#893_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_894_1539825877_93" onclick="$('#893_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_894_1539825877_93').hide();
                $('#vhsjs_view_894_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="894_1539825877_93" id="893_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>Energy efficiency has become increasingly important in high performance computing (HPC), as power constraints and costs escalate. Workload and system characteristics form a complex optimization search space in which optimal settings for energy efficiency and performance often diverge. Thus, we must identify trade-off options to find the desired balance. We present an innovative statistical model that accurately predicts the Pareto optimal trade-off options using only user-controllable parameters. Our approach can also tolerate both measurement and model errors. We study model training and validation using several HPC kernels, then with more complex workloads, including AMG and LAMMPS. We can calibrate an accurate model from as few as 12 runs, with prediction error of less than 10%. Our results identify trade-off options allowing up to 40% energy efficiency improvement at the cost of under 20% performance loss. For AMG, we reduce the required sample measurement time from 13 hours to 74 minutes.</blockquote></div></div></div></div><a href="includes/files/pap186s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap421"></a><div class="slot-title">HPL and DGEMM Performance Variability on the Xeon Platinum 8160 Processor</div><div class="slot-authors">John D. McCalpin (University of Texas, Texas Advanced Computing Center)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_896_1539825877_93" onclick="$('#vhsjs_view_896_1539825877_93').hide();
                $('#vhsjs_hide_896_1539825877_93').show();
                $('#895_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_896_1539825877_93" onclick="$('#895_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_896_1539825877_93').hide();
                $('#vhsjs_view_896_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="896_1539825877_93" id="895_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>During initial testing of a large cluster equipped with Xeon Platinum 8160 processors, we observed infrequent, but significant, performance drops in HPL benchmark results. The variability was seen in both single node and multi-node runs, with approximately 0.4% of results more than 10% slower than the median. We were able to reproduce this behavior with a single-socket (24-core) DGEMM benchmark. Performance counter analysis of several thousand DGEMM runs showed that increased DRAM read traffic is the primary driver of increased execution time. Increased DRAM traffic in this benchmark is primarily generated by dramatically elevated snoop filter evictions, which arise due to the interaction of high-order (physical) address bits with the hash used to map addresses across the 24 coherence agents on the processor. These conflicts (and the associated performance variability) were effectively eliminated (for both DGEMM and HPL) by using 1 GiB large pages.</blockquote></div></div></div></div><a href="includes/files/pap421s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, Data Analytics, Deep Learning, Networks, Scientific Computing, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Large-Scale Algorithms</div><div class="slot-entry"><a name="pap171"></a><div class="slot-title">Large-Scale Hierarchical K-Means for Heterogeneous Many-Core Supercomputers</div><div class="slot-authors">Liandeng Li (Tsinghua University; National Supercomputing Center, Wuxi); Teng Yu (University of St Andrews); Wenlai Zhao and Haohuan Fu (Tsinghua University; National Supercomputing Center, Wuxi); Chenyu Wang (University of St Andrews; National Supercomputing Center, Wuxi); Li Tan (Beijing Technology and Business University); Guangwen Yang (Tsinghua University; National Supercomputing Center, Wuxi); and John Thomson (University of St Andrews)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_898_1539825877_93" onclick="$('#vhsjs_view_898_1539825877_93').hide();
                $('#vhsjs_hide_898_1539825877_93').show();
                $('#897_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_898_1539825877_93" onclick="$('#897_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_898_1539825877_93').hide();
                $('#vhsjs_view_898_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="898_1539825877_93" id="897_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>This paper presents a novel design and implementation of k-means clustering algorithm targeting the Sunway TaihuLight supercomputer. We introduce a multi-level parallel partition approach that not only partitions by dataflow and centroid, but also by dimension. Our multi-level (nkd) approach unlocks the potential of the hierarchical parallelism in the SW26010 heterogeneous many-core processor and the system architecture of the supercomputer. <br><br>Our design is able to process large-scale clustering problems with up to 196,608 dimensions and over 160,000 targeting centroids, while maintaining high performance and high scalability, significantly improving the capability of k-means over previous approaches. The evaluation shows our implementation achieves performance of less than 18 seconds per iteration for a large-scale clustering case with 196,608 data dimensions and 2,000 centroids by applying 4,096 nodes (1,064,496 cores) in parallel, making k-means a more feasible solution for complex scenarios.</blockquote></div></div></div></div><a href="includes/files/pap171s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap140"></a><div class="slot-title">TriCore: Parallel Triangle Counting on GPUs</div><div class="slot-authors">Yang Hu (George Washington University); Hang Liu (University of Massachusetts, Lowell); and H. Howie Huang (George Washington University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_900_1539825877_93" onclick="$('#vhsjs_view_900_1539825877_93').hide();
                $('#vhsjs_hide_900_1539825877_93').show();
                $('#899_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_900_1539825877_93" onclick="$('#899_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_900_1539825877_93').hide();
                $('#vhsjs_view_900_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="900_1539825877_93" id="899_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>Triangle counting algorithm enumerates the triangles in a graph by identifying the common neighbors between two vertices of every edge. In this work, we present TriCore, a new GPU-based high-performance and scalable triangle counting system that consists of three main techniques. First, we design a binary search based counting algorithm that tremendously increases both thread parallelism and memory performance. Second, TriCore exploits a 2-D partition method to distribute the CSR representation across multiple GPUs, combined with a new streaming buffer to load the edge list from outside of GPUs. Third, we develop a dynamic workload management technique to balance the workload across multiple GPUs. Our evaluation demonstrates TriCore is 22× faster than the state-of-the-art parallel triangle counting projects. In addition, TriCore can not only process big graphs that are significant larger than the memory size of one GPU but also achieve 24× speedup when scaling to 32 GPUs.</blockquote></div></div></div></div><a href="includes/files/pap140s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap141"></a><div class="slot-title">Distributed-Memory Hierarchical Compression of Dense SPD Matrices</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Chenhan D. Yu (University of Texas), Severin Reiz (Technical University Munich), and George Biros (University of Texas)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_902_1539825877_93" onclick="$('#vhsjs_view_902_1539825877_93').hide();
                $('#vhsjs_hide_902_1539825877_93').show();
                $('#901_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_902_1539825877_93" onclick="$('#901_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_902_1539825877_93').hide();
                $('#vhsjs_view_902_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="902_1539825877_93" id="901_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>We present a distributed-memory algorithm for the hierarchical compression of SPD matrices. Our method is based on GOFMM, an algorithm that appeared in doi:10.1145/3126908.3126921.<br><br>For many SPD matrices, GOFMM enables compression and approximate matrix-vector multiplication in NlogN time---as opposed to quadratic work required for a dense matrix. But GOFMM supports only shared memory parallelism. In this paper, we use the message passing interface, extending the ideas of GOFMM to the distributed memory setting. We also introduce an asynchronous algorithm for faster multiplication. We present different usage scenarios of SPD matrices that are related to graphs, neural-networks, and covariance operators. We also compare with STRUMPACK, which, to our knowledge, is the only other parallel software that can compress arbitrary SPD matrices. In our largest run, we were able to compress a 67M-by-67M matrix within three minutes and perform a multiplication with 512 vectors within 5 seconds on 6,144 Intel Skylake cores.</blockquote></div></div></div></div><a href="includes/files/pap141s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Networks, Resource Management, Scheduling, State of the Practice, System Software, Tech Program Reg Pass</span><br /><div class="session-title">Resource Management and Interference</div><div class="slot-entry"><a name="pap360"></a><div class="slot-title">RM-Replay: A High-Fidelity Tuning, Optimization and Exploration Tool for Resource Management</div><div class="slot-authors">Maxime Martinasso, Miguel Gila, Mauro Bianco, Sadaf R. Alam, Colin McMurtrie, and Thomas C. Schulthess (Swiss National Supercomputing Centre)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_904_1539825877_93" onclick="$('#vhsjs_view_904_1539825877_93').hide();
                $('#vhsjs_hide_904_1539825877_93').show();
                $('#903_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_904_1539825877_93" onclick="$('#903_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_904_1539825877_93').hide();
                $('#vhsjs_view_904_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="904_1539825877_93" id="903_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>Leading hybrid and heterogeneous supercomputing systems process hundreds of thousands of jobs using complex scheduling algorithms and parameters. The centers operating these systems aim to achieve higher levels of resource utilization while being restricted by compliance with policy constraints. There is a critical need for a high-fidelity, high-performance tool with familiar interfaces that allows not only tuning and optimization of the operational job scheduler but also enables exploration of new resource management algorithms. We propose a new methodology and a tool called RM-Replay which is not a simulator but instead a fast replay engine for production workloads. Slurm is used as a platform to demonstrate the capabilities of our replay engine.<br><br>The tool accuracy is discussed and our investigation shows that, by providing better job runtime estimation or using topology-aware allocation, scheduling metric values vary. The presented methodology to create fast replay engines can be extended to other complex systems.</blockquote></div></div></div></div><a href="includes/files/pap360s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap541"></a><div class="slot-title">Evaluation of an Interference-Free Node Allocation Policy on Fat-Tree Clusters</div><div class="slot-authors">Samuel D. Pollard (University of Oregon) and Nikhil Jain, Stephen Herbein, and Abhinav Bhatele (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_906_1539825877_93" onclick="$('#vhsjs_view_906_1539825877_93').hide();
                $('#vhsjs_hide_906_1539825877_93').show();
                $('#905_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_906_1539825877_93" onclick="$('#905_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_906_1539825877_93').hide();
                $('#vhsjs_view_906_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="906_1539825877_93" id="905_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>Interference between jobs competing for network bandwidth on a fat-tree cluster can cause significant variability and degradation in performance. These performance issues can be mitigated or completely eliminated if the resource allocation policy takes the network topology into account when allocating nodes to jobs. We implement a fat-tree network topology aware node allocation policy that allocates isolated partitions to jobs in order to eliminate inter-job interference. We compare the impact of this node allocation policy to a topology-oblivious policy with respect to the execution time of individual jobs with different communication patterns. We also evaluate the cluster's quality of service using metrics such as system utilization, schedule makespan, and job wait time for both policies. The results obtained for production workloads indicate that a topology-aware node allocation can provide interference-free execution without negatively impacting the cluster's quality of service.</blockquote></div></div></div></div><a href="includes/files/pap541s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap311"></a><div class="slot-title">Mitigating Inter-Job Interference Using Adaptive Flow-Aware Routing</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Staci A. Smith, Clara E. Cromey, and David K. Lowenthal (University of Arizona); Jens Domke (Tokyo Institute of Technology); and Nikhil Jain, Jayaraman J. Thiagarajan, and Abhinav Bhatele (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_908_1539825877_93" onclick="$('#vhsjs_view_908_1539825877_93').hide();
                $('#vhsjs_hide_908_1539825877_93').show();
                $('#907_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_908_1539825877_93" onclick="$('#907_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_908_1539825877_93').hide();
                $('#vhsjs_view_908_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="908_1539825877_93" id="907_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>On most high performance computing platforms, applications share network resources with other jobs running concurrently on the system.  Inter-job network interference can have a significant impact on the performance of communication-intensive applications, and no satisfactory solutions yet exist for mitigating this degradation.<br><br>In this paper, we analyze network congestion caused by multi-job workloads on two production systems that use popular network topologies---fat-tree and dragonfly. For each system, we establish a regression model to relate network hotspots to application performance degradation, showing that current routing strategies are insufficient to load-balance network traffic and mitigate interference on production systems.  We then propose an alternative type of adaptive routing strategy, which we call adaptive flow-aware routing.  We implement a prototype of our strategy, and tests on the fat-tree system show up to a 46% improvement in job run time when compared to the default routing.</blockquote></div></div></div></div><a href="includes/files/pap311s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Graph Algorithms, Linear Algebra, Machine Learning, Sparse Computation, Tech Program Reg Pass</span><br /><div class="session-title">Algorithms on Sparse Data</div><div class="slot-entry"><a name="pap511"></a><div class="slot-title">HiCOO: Hierarchical Storage of Sparse Tensors</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">Jiajia Li, Jimeng Sun, and Richard Vuduc (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_910_1539825877_93" onclick="$('#vhsjs_view_910_1539825877_93').hide();
                $('#vhsjs_hide_910_1539825877_93').show();
                $('#909_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_910_1539825877_93" onclick="$('#909_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_910_1539825877_93').hide();
                $('#vhsjs_view_910_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="910_1539825877_93" id="909_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>This paper proposes a new storage format for sparse tensors, called Hierarchical COOrdinate (HiCOO; pronounced: “haiku”). It derives from coordinate (COO) format, arguably the de facto standard for general sparse tensor storage. HiCOO improves upon COO by compressing the indices in units of sparse tensor blocks, with the goals of preserving the “mode-agnostic” simplicity of COO while reducing the bytes needed to represent the tensor and promoting data locality. We evaluate HiCOO by implementing a single-node, multicore-parallel version of the matricized tensor-times-Khatri-Rao product (MTTKRP) operation, which is the most expensive computational core in the widely used CANDECOMP/PARAFAC decomposition(CPD) algorithm. This MTTKRP implementation achieves up to 23.0× (6.8× on average) speedup over COO format and up to 15.6× (3.1× on average) speedup over another state-of-the-art format, compressed sparse fiber (CSF), by using less or comparable storage of them. When used within CPD, we also observe speedups against COO- and CSF-based implementations.</blockquote></div></div></div></div><a href="includes/files/pap511s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap273"></a><div class="slot-title">Distributed Memory Sparse Inverse Covariance Matrix Estimation on High-Performance Computing Architectures</div><div class="slot-authors">Aryan Eftekhari (University of Lugano), Matthias Bollhöfer (Braunschweig University of Technology), and Olaf Schenk (University of Lugano)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_912_1539825877_93" onclick="$('#vhsjs_view_912_1539825877_93').hide();
                $('#vhsjs_hide_912_1539825877_93').show();
                $('#911_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_912_1539825877_93" onclick="$('#911_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_912_1539825877_93').hide();
                $('#vhsjs_view_912_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="912_1539825877_93" id="911_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>We consider the problem of estimating sparse inverse covariance matrices for high-dimensional datasets using the l1-regularized Gaussian maximum likelihood method. This task is particularly challenging as the required computational resources increase superlinearly with the dimensionality of the dataset. We introduce a performant and scalable algorithm which builds on the current advancements of second-order, maximum likelihood methods. The routine leverages the intrinsic parallelism in the linear algebra operations and exploits the underlying sparsity of the problem. The computational bottlenecks are identified and the respective subroutines are parallelized using an MPI-OpenMP approach. Experiments conducted on a Cray XC50 system at the Swiss National Supercomputing Center show that, in comparison to the state-of-the-art algorithms, the proposed routine provides significant strong scaling speedup with ideal scalability up to 128 nodes. The developed framework is used to estimate the sparse inverse covariance matrix of both synthetic and real-world datasets with up to 10 million dimensions.</blockquote></div></div></div></div><a href="includes/files/pap273s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap466"></a><div class="slot-title">PruneJuice:  Pruning Trillion-Edge Graphs to a Precise Pattern-Matching Solution</div><div class="slot-authors">Tahsin Reza, Matei Ripeanu, and Nicolas Tripoul (University of British Columbia) and Geoffrey Sanders and Roger Pearce (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_914_1539825877_93" onclick="$('#vhsjs_view_914_1539825877_93').hide();
                $('#vhsjs_hide_914_1539825877_93').show();
                $('#913_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_914_1539825877_93" onclick="$('#913_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_914_1539825877_93').hide();
                $('#vhsjs_view_914_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="914_1539825877_93" id="913_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>Pattern matching is a powerful graph analysis tool. Unfortunately, existing solutions have limited scalability, support only a limited set of search patterns, and/or focus on only a subset of the real-world problems associated with pattern matching. This paper presents a new algorithmic pipeline that: (i) enables highly scalable pattern matching on labeled graphs, (ii) supports arbitrary patterns, (iii) enables trade-offs between precision and time-to-solution (while always selecting all vertices and edges that participate in matches, thus offering 100% recall), and (iv) supports a set of popular data analytics scenarios. We implement our approach on top of HavoqGT and demonstrate its advantages through strong and weak scaling experiments on massive-scale real-world (up to 257 billion edges) and synthetic (up to 4.4 trillion edges) graphs, respectively, and at scales (1,024 nodes / 36,864 cores) orders of magnitude larger than used in the past for similar problems.</blockquote></div></div></div></div><a href="includes/files/pap466s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Data Analytics, Performance, Programming Systems, Storage, Tools, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Performance Optimization Studies</div><div class="slot-entry"><a name="pap335"></a><div class="slot-title">Many-Core Graph Workload Analysis</div><div class="slot-authors">Stijn Eyerman, Wim Heirman, Kristof Du Bois, Joshua B. Fryman, and Ibrahim Hur (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_916_1539825877_93" onclick="$('#vhsjs_view_916_1539825877_93').hide();
                $('#vhsjs_hide_916_1539825877_93').show();
                $('#915_1539825877_93').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_916_1539825877_93" onclick="$('#915_1539825877_93').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_916_1539825877_93').hide();
                $('#vhsjs_view_916_1539825877_93').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="916_1539825877_93" id="915_1539825877_93" style="display: none"><div class="arrow-slidedown"><blockquote>Graph applications have specific characteristics that are not common in other application domains. In this paper, we analyze multiple graph applications on current multi- and many-core processors and provide conclusions and recommendations for future designs. We provide new insights on executing graph applications on many-core processors.<br><br>Our main novel observations are (i) some memory streams do show locality, while others show no locality, (ii) thread imbalance becomes a major problem with many threads, and (iii) many threads are required to saturate high-bandwidth memories. We recommend a selective memory access policy, where accesses with locality are cached and prefetched, while accesses without locality can remain uncached to save cache capacity. Additionally, more threads are needed, but they are not used efficiently due to thread imbalance. Our recommendation is to revise the graph analysis algorithms to provide more parallelism, and to provide a few high-performance cores that speedup sections with low parallelism.</blockquote></div></div></div></div><a href="includes/files/pap335s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap488"></a><div class="slot-title">Lessons Learned from Analyzing Dynamic Promotion for User-Level Threading</div><div class="slot-authors">Shintaro Iwasaki (University of Tokyo), Abdelhalim Amer (Argonne National Laboratory), Kenjiro Taura (University of Tokyo), and Pavan Balaji (Argonne National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_918_1539825877_94" onclick="$('#vhsjs_view_918_1539825877_94').hide();
                $('#vhsjs_hide_918_1539825877_94').show();
                $('#917_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_918_1539825877_94" onclick="$('#917_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_918_1539825877_94').hide();
                $('#vhsjs_view_918_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="918_1539825877_94" id="917_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>A performance vs. practicality trade-off exists between user-level threading techniques. The community has settled mostly on a black-and-white perspective; fully fledged threads assume that suspension is imminent and incur overheads when suspension does not take place, and run-to-completion threads are more lightweight but less practical since they cannot suspend. Gray areas exist, however, whereby threads can start with minimal capabilities and then can be dynamically promoted to acquire additional capabilities when needed. This paper investigates the full spectrum of threading techniques from a performance vs. practicality trade-off perspective on modern multicore and many-core systems. Our results indicate that achieving the best trade-off highly depends on the suspension likelihood; dynamic promotion is more appropriate when suspension is unlikely and represents a solid replacement for run to completion, thanks to its lower programming constraints, while fully fledged threads remain the technique of choice when suspension likelihood is high.</blockquote></div></div></div></div><a href="includes/files/pap488s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap484"></a><div class="slot-title">Topology-Aware Space-Shared Co-Analysis of Large-Scale Molecular Dynamics Simulations</div><div class="slot-authors">Preeti Malakar (Indian Institute of Technology Kanpur); Todd Munson, Christopher Knight, and Venkatram Vishwanath (Argonne National Laboratory); and Michael E. Papka (Argonne National Laboratory, Northern Illinois University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_920_1539825877_94" onclick="$('#vhsjs_view_920_1539825877_94').hide();
                $('#vhsjs_hide_920_1539825877_94').show();
                $('#919_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_920_1539825877_94" onclick="$('#919_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_920_1539825877_94').hide();
                $('#vhsjs_view_920_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="920_1539825877_94" id="919_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>Analysis of scientific simulation data can be concurrently executed with simulation either in time- or space-shared mode. This mitigates the I/O bottleneck.  However it results in either stalling the simulation for performing the analysis or transferring data for analysis. In this paper, we improve the throughput of space-shared in situ analysis of large-scale simulations by topology-aware mapping and optimal process decomposition. We propose node interconnect topology-aware process placement for simulation and analysis to reduce the data movement time. We also present an integer linear program for optimal 3D decompositions of simulation and analysis processes. We demonstrate our approach using molecular dynamics simulation on Mira, Cori and Theta supercomputers. Our mapping schemes, combined with optimal 3D process decomposition and code optimizations resulted in up to 30% lower execution times for space-shared in situ analysis than the default approach. Our mappings also reduce MPI collective I/O times by 10-40%.</blockquote></div></div></div></div><a href="includes/files/pap484s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, MPI, Networks, Performance, Programming Systems, State of the Practice, Tech Program Reg Pass</span><br /><div class="session-title">MPI Optimization and Characterization</div><div class="slot-entry"><a name="pap504"></a><div class="slot-title">Cooperative Rendezvous Protocols for Improved Performance and Overlap</div><div><span class="BSP award">Best Student Paper Finalists</span></div><div class="slot-authors">S. Chakraborty, M. Bayatpour, J. Hashmi, H. Subramoni, and D. K. Panda (Ohio State University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_922_1539825877_94" onclick="$('#vhsjs_view_922_1539825877_94').hide();
                $('#vhsjs_hide_922_1539825877_94').show();
                $('#921_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_922_1539825877_94" onclick="$('#921_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_922_1539825877_94').hide();
                $('#vhsjs_view_922_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="922_1539825877_94" id="921_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>With the emergence of larger multi-/many-core clusters, performance of large message communication is becoming more important. MPI libraries use different Rendezvous protocols to perform large message communication. However, existing Rendezvous protocols do not consider the overall communication pattern and make optimal use of the Sender and the Receiver CPUs. In this work, we propose a cooperative Rendezvous protocol that can provide up to 2x improvement in intra-node bandwidth and latency for large messages. We also propose a scheme to dynamically choose the best Rendezvous protocol for each message based on the communication pattern.  Finally, we show how these improvements can increase the overlap of computation with intra-node and inter-node communication, and lead to application level benefits. We evaluate proposed designs on three different architectures including Intel Xeon, Knights Landing, and OpenPOWER with different HPC applications and obtain benefits up to 19% with Graph500, 16% with CoMD, and 10% with MiniGhost.</blockquote></div></div></div></div><a href="includes/files/pap504s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap463"></a><div class="slot-title">Framework for Scalable Intra-Node Collective Operations Using Shared Memory</div><div class="slot-authors">Surabhi Jain, Rashid Kaleem, Marc Gamell Balmana, Akhil Langer, Dmitry Durnov, Alexander Sannikov, and Maria Garzaran (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_924_1539825877_94" onclick="$('#vhsjs_view_924_1539825877_94').hide();
                $('#vhsjs_hide_924_1539825877_94').show();
                $('#923_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_924_1539825877_94" onclick="$('#923_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_924_1539825877_94').hide();
                $('#vhsjs_view_924_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="924_1539825877_94" id="923_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>Collective operations are used in MPI programs to express common communication patterns, collective computations, or synchronizations. In many collectives, such as barrier or allreduce, the intra-node component of the collective is in the critical path, as the inter-node communication cannot start until the intra-node component has been executed. Thus, with increasing number of core counts in each node, intra-node optimizations that leverage the intra-node shared memory become increasingly important.<br><br>In this paper, we focus on the performance benefit of optimizing intra-node collectives using shared memory. We optimize several collectives using the primitives in broadcast and reduce as building blocks for other collectives. A comparison of our implementation on top of MPICH shows significant performance speedups with respect to the original MPICH implementation, MVAPICH, and OpenMPI, among others.</blockquote></div></div></div></div><a href="includes/files/pap463s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap537"></a><div class="slot-title">Characterization of MPI Usage on a Production Supercomputer</div><div class="slot-authors">Sudheer Chunduri, Scott Parker, Pavan Balaji, Kevin Harms, and Kalyan Kumaran (Argonne National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_926_1539825877_94" onclick="$('#vhsjs_view_926_1539825877_94').hide();
                $('#vhsjs_hide_926_1539825877_94').show();
                $('#925_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_926_1539825877_94" onclick="$('#925_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_926_1539825877_94').hide();
                $('#vhsjs_view_926_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="926_1539825877_94" id="925_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>MPI is the most prominent programming model used in scientific computing today.  Despite it's importance, however, how scientific applications use it in production is not very well understood due to the lack of low overhead profiling tools.   We used a lightweight profiling tool, called autoperf, to log the MPI usage characteristics of production applications on a large supercomputing system (Mira) and its corresponding development system (Cetus).  Autoperf limits the amount of information that it records in order to keep the overhead to a minimum while still storing enough data to derive useful insights.  MPI usage statistics have been collected for over 100K jobs that were run within a 2-year period and are analyzed.  The analysis of this data is intended as a mechanism to provide useful insights for MPI developers and network hardware developers for their next generation of improvements, and for supercomputing center operators for their next system procurements.</blockquote></div></div></div></div><a href="includes/files/pap537s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">GPUs, Memory, NVRAM, Performance, System Software, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Non-Volatile Memory</div><div class="slot-entry"><a name="pap203"></a><div class="slot-title">Runtime Data Management on Non-Volatile Memory-Based Heterogeneous Memory for Task-Parallel Programs</div><div class="slot-authors">Kai Wu, Jie Ren, and Dong Li (University of California, Merced)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_928_1539825877_94" onclick="$('#vhsjs_view_928_1539825877_94').hide();
                $('#vhsjs_hide_928_1539825877_94').show();
                $('#927_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_928_1539825877_94" onclick="$('#927_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_928_1539825877_94').hide();
                $('#vhsjs_view_928_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="928_1539825877_94" id="927_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>Non-volatile memory (NVM) provides a scalable solution to replace DRAM as main memory. Because of relatively high latency and low bandwidth of NVM (comparing with DRAM), NVM often pairs with DRAM to build a  heterogeneous main memory system (HMS). Deciding data placement on NVM-based HMS is critical to enable future NVM-based HPC. In this paper, we study task-parallel programs and introduce a runtime system to address the data placement problem on NVM-based HMS. Leveraging semantics and execution mode of task-parallel programs, we efficiently characterize memory access patterns of tasks and reduce data movement overhead. We also introduce a performance model to predict performance for tasks with various data placements on HMS. Evaluating with a set of HPC benchmarks, we show that our runtime system achieves higher performance than a conventional HMS-oblivious runtime (24% improvement on average) and two state-of-the-art HMS-aware solutions (16% and 11% improvement on average, respectively).</blockquote></div></div></div></div><a href="includes/files/pap203s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap194"></a><div class="slot-title">DRAGON: Breaking GPU Memory Capacity Limits with Direct NVM Access</div><div class="slot-authors">Pak Markthub (Tokyo Institute of Technology); Mehmet E. Belviranli, Seyong Lee, and Jeffrey S. Vetter (Oak Ridge National Laboratory); and Satoshi Matsuoka (RIKEN, Tokyo Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_930_1539825877_94" onclick="$('#vhsjs_view_930_1539825877_94').hide();
                $('#vhsjs_hide_930_1539825877_94').show();
                $('#929_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_930_1539825877_94" onclick="$('#929_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_930_1539825877_94').hide();
                $('#vhsjs_view_930_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="930_1539825877_94" id="929_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>Heterogeneous computing with accelerators is growing in importance in high performance computing (HPC). Recently, application datasets have expanded beyond the memory capacity of these accelerators, and often beyond the capacity of their hosts. Meanwhile, nonvolatile memory (NVM) storage has emerged as a pervasive component in HPC systems because NVM provides massive amounts of memory capacity at affordable cost. Currently, for accelerator applications to use NVM, they must manually orchestrate data movement across multiple memories and this approach only performs well for applications with simple access behaviors. To address this issue, we developed DRAGON, a solution that enables all classes of GP-GPU applications to transparently compute on terabyte datasets residing in NVM. DRAGON leverages the page-faulting mechanism on the recent NVIDIA GPUs by extending capabilities of CUDA Unified Memory (UM). Our experimental results show that DRAGON transparently expands memory capacity and obtain additional speedups via automated I/O and data transfer overlapping.</blockquote></div></div></div></div><a href="includes/files/pap194s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap355"></a><div class="slot-title">Siena: Exploring the Design Space of Heterogeneous Memory Systems</div><div class="slot-authors">Ivy B. Peng and Jeffrey S. Vetter (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_932_1539825877_94" onclick="$('#vhsjs_view_932_1539825877_94').hide();
                $('#vhsjs_hide_932_1539825877_94').show();
                $('#931_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_932_1539825877_94" onclick="$('#931_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_932_1539825877_94').hide();
                $('#vhsjs_view_932_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="932_1539825877_94" id="931_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>Memory systems are crucial to the performance, power, and cost of high-performance computing systems. Recently, multiple factors are driving the need for more complex, deep memory hierarchies. However, architects and customers are struggling to design memory systems that effectively balance multiple, often competing, factors in this large, multidimensional, and fast-moving design space. In this paper, we systematically explore the organization of heterogeneous memory systems on a framework, called Siena. Siena facilitates quick exploration of memory architectures with flexible configurations of memory systems and realistic memory workloads. We perform a design space exploration on 22 proposed memory systems using eight relevant workloads. Our results show that horizontal organizations of memories can achieve higher performance than that of vertical organizations when the distribution of memory traffic balances the performance gap between memories. However, the coupling effects through shared resources and application behaviors could negate the advantage of high-performance memory in horizontal organizations.</blockquote></div></div></div></div><a href="includes/files/pap355s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, Memory, Networks, Parallel Programming Languages, Libraries, and Models, Power, Programming Systems, Scheduling, Tech Program Reg Pass</span><br /><div class="session-title">Task-Based Programming</div><div class="slot-entry"><a name="pap490"></a><div class="slot-title">Dynamic Tracing: Memoization of Task Graphs for Dynamic Task-Based Runtimes</div><div class="slot-authors">Wonchan Lee (Stanford University), Elliott Slaughter (SLAC National Accelerator Laboratory), Michael Bauer and Sean Treichler (Nvidia Corporation), Todd Warszawski (Stanford University), Michael Garland (Nvidia Corporation), and Alex Aiken (Stanford University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_934_1539825877_94" onclick="$('#vhsjs_view_934_1539825877_94').hide();
                $('#vhsjs_hide_934_1539825877_94').show();
                $('#933_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_934_1539825877_94" onclick="$('#933_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_934_1539825877_94').hide();
                $('#vhsjs_view_934_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="934_1539825877_94" id="933_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>Many recent programming systems for both supercomputing and data center workloads generate task graphs to express computations that run on parallel and distributed machines. Due to the overhead associated with constructing these graphs the dependence analysis that generates them is often statically computed and memoized, and the resulting graph executed repeatedly at runtime. However, many applications require a dynamic dependence analysis due to data dependent behavior, but there are new challenges in capturing and re-executing task graphs at runtime. In this work, we introduce dynamic tracing, a technique to capture a dynamic dependence analysis of a trace that generates a task graph, and replay it. We show that an implementation of dynamic tracing improves strong scaling by an average of 4.9X and up to 7.0X on a suite of already optimized benchmarks.</blockquote></div></div></div></div><a href="includes/files/pap490s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap338"></a><div class="slot-title">Runtime-Assisted Cache Coherence Deactivation in Task Parallel Programs</div><div class="slot-authors">Paul Caheny (Barcelona Supercomputing Center, Polytechnic University of Catalonia); Lluc Alvarez (Barcelona Supercomputing Center); Mateo Valero and Miquel Moretó (Barcelona Supercomputing Center, Polytechnic University of Catalonia); and Marc Casas (Barcelona Supercomputing Center)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_936_1539825877_94" onclick="$('#vhsjs_view_936_1539825877_94').hide();
                $('#vhsjs_hide_936_1539825877_94').show();
                $('#935_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_936_1539825877_94" onclick="$('#935_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_936_1539825877_94').hide();
                $('#vhsjs_view_936_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="936_1539825877_94" id="935_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>With increasing core counts, the scalability of directory-based cache coherence has become a challenging problem. To reduce the area and power needs of the directory, recent proposals reduce its size by classifying data as private or shared, and disable coherence for private data. However, existing classification methods suffer from inaccuracies and require complex hardware support with limited scalability.<br><br>This paper proposes a hardware/software co-designed approach: the runtime system identifies data that is guaranteed by the programming model semantics to not require coherence and notifies the microarchitecture. The microarchitecture deactivates coherence for this private data and powers off unused directory capacity. Our proposal reduces directory accesses to just 26% of the baseline system and supports a 64× smaller directory with only 2.8% performance degradation. By dynamically calibrating the directory size, our proposal saves 86% of dynamic energy consumption in the directory without harming performance.</blockquote></div></div></div></div><a href="includes/files/pap338s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap547"></a><div class="slot-title">A Divide and Conquer Algorithm for DAG Scheduling Under Power Constraints</div><div class="slot-authors">Gökalp Demirci, Ivana Marincic, and Henry Hoffmann (University of Chicago)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_938_1539825877_94" onclick="$('#vhsjs_view_938_1539825877_94').hide();
                $('#vhsjs_hide_938_1539825877_94').show();
                $('#937_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_938_1539825877_94" onclick="$('#937_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_938_1539825877_94').hide();
                $('#vhsjs_view_938_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="938_1539825877_94" id="937_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>We consider the problem of scheduling a parallel computation–represented as a directed acyclic graph (DAG)–on a distributed parallel system with a global resource constraint–specifically a global power budget–and configurable resources, allowing a range of different power/performance tradeoffs. There is a rich body of literature on the independent problems of (1) scheduling DAGs and (2) scheduling independent applications under resource constraints. Very little, however, is known about the combined problem of scheduling DAGs under resource constraints. We present a novel approximation algorithm using a divide-and-conquer method for minimizing application execution time. We prove that the length of the schedule returned by our algorithm is always within O(log n)-factor of the optimum that can be achieved with selection of configurations for the tasks. We implement and test our algorithm on simulations of real application DAGs. We find that our divide-and-conquer method improves performance by up to 75% compared to greedy scheduling algorithms.</blockquote></div></div></div></div><a href="includes/files/pap547s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Computational Physics, Scientific Computing, Tech Program Reg Pass</span><br /><div class="session-title">Physics and Tensor Applications</div><div class="slot-entry"><a name="pap502"></a><div class="slot-title">Simulating the Wenchuan Earthquake with Accurate Surface Topography on Sunway TaihuLight</div><div class="slot-authors">Bingwei Chen, Haohuan Fu, Yanwen Wei, and Conghui He (Tsinghua University; National Supercomputing Center, Wuxi); Wenqiang Zhang (University of Science and Technology of China); Yuxuan Li (Tsinghua University; National Supercomputing Center, Wuxi); Wubin Wan and Wei Zhang (National Supercomputing Center, Wuxi); Lin Gan (Tsinghua University; National Supercomputing Center, Wuxi); Wei Zhang and Zhenguo Zhang (Southern University of Science and Technology, China); Guangwen Yang (Tsinghua University; National Supercomputing Center, Wuxi); and Xiaofei Chen (Southern University of Science and Technology, China)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_940_1539825877_94" onclick="$('#vhsjs_view_940_1539825877_94').hide();
                $('#vhsjs_hide_940_1539825877_94').show();
                $('#939_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_940_1539825877_94" onclick="$('#939_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_940_1539825877_94').hide();
                $('#vhsjs_view_940_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="940_1539825877_94" id="939_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>This paper reports our efforts on performing 50-m resolution earthquake simulation of the Wenchuan Earthquake (Ms 8.0, China) on Sunway TaihuLight. To accurately capture the surface topography, we adopt a curvilinear grid finite-difference method with a traction image free surface implementation and redesign the algorithm to reduce memory access costs for heterogeneous many-core architectures. We then derive a performance model of our algorithm to guide and drive the further optimization and tuning of various parameters using a genetic algorithm. A data layout transformation is also proposed to improve the direct memory access (DMA) efficiency further. Our efforts improve the simulation efficiency from 0.05% to 7.6%, with a sustained performance of 9.07 Pflops using the entire machine of the Sunway TaihuLight (over 10 million cores), and a large-scale simulation of the Wenchuan earthquake with accurate surface topography and improved coda wave effects.</blockquote></div></div></div></div><a href="includes/files/pap502s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap506"></a><div class="slot-title">Accelerating Quantum Chemistry with Vectorized and Batched Integrals</div><div class="slot-authors">Hua Huang and Edmond Chow (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_942_1539825877_94" onclick="$('#vhsjs_view_942_1539825877_94').hide();
                $('#vhsjs_hide_942_1539825877_94').show();
                $('#941_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_942_1539825877_94" onclick="$('#941_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_942_1539825877_94').hide();
                $('#vhsjs_view_942_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="942_1539825877_94" id="941_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>This paper presents the first quantum chemistry calculations using a recently developed vectorized library for computing electron repulsion integrals. To lengthen the SIMD loop and thus improve SIMD utilization, the approach used in this paper is to batch together the computation of multiple integrals that have the same code path. The standard approach is to compute integrals one at a time, and thus a batching procedure had to be developed. This paper shows proof-of-concept and demonstrates the performance gains possible when the batched approach is used. Batching also enables certain optimizations when the integrals are used to compute the Fock matrix. We further describe several other optimizations that were needed to obtain up to a 270% speedup over the no batching version of the code, making a compelling case for adopting the presented techniques in quantum chemistry software.</blockquote></div></div></div></div><a href="includes/files/pap506s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap133"></a><div class="slot-title">High-Performance Dense Tucker Decomposition on GPU Clusters</div><div class="slot-authors">Jee Choi (IBM), Xing Liu (Intel Corporation), and Venkatesan Chakaravarthy (IBM)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_944_1539825877_94" onclick="$('#vhsjs_view_944_1539825877_94').hide();
                $('#vhsjs_hide_944_1539825877_94').show();
                $('#943_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_944_1539825877_94" onclick="$('#943_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_944_1539825877_94').hide();
                $('#vhsjs_view_944_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="944_1539825877_94" id="943_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>The Tucker decomposition method is one of the most popular algorithms for analyzing and compressing data with multi-way relationship. Its execution time is typically dominated by dense matrix multiplication, which makes it well-suited for GPU acceleration. State-of-the-art distributed dense Tucker implementations for CPU clusters adopt multi-dimensional partitioning that optimizes for storage and communication. This, however, leads to smaller matrix dimensions that result in under-utilizing the GPU. <br><br>In this paper, we present our optimized implementation and performance analysis of dense Tucker decomposition on a multi-GPU cluster. We propose three optimizations: a new partitioning strategy that improves GPU performance, a new tensor matricization layout that halves the number of communication/matricization steps, and a variation of the randomized SVD algorithm to overcome the eigenvalue bottleneck that arises from the high speedups gained from GPU acceleration.  Our GPU implementation employing all three optimizations achieves up to 11.8x speedup on 64 nodes over state-of-the-art TuckerMPI.</blockquote></div></div></div></div><a href="includes/files/pap133s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Clouds and Distributed Computing, Resource Management, Scheduling, Tech Program Reg Pass</span><br /><div class="session-title">Clouds and Distributed Computing</div><div class="slot-entry"><a name="pap229"></a><div class="slot-title">A Reference Architecture for Datacenter Scheduling: Design, Validation, and Experiments</div><div class="slot-authors">Georgios Andreadis (Delft University of Technology, Vrije University Amsterdam); Laurens Versluis (Vrije University Amsterdam); Fabian Mastenbroek (Delft University of Technology); and Alexandru Iosup (Vrije University Amsterdam, Delft University of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_946_1539825877_94" onclick="$('#vhsjs_view_946_1539825877_94').hide();
                $('#vhsjs_hide_946_1539825877_94').show();
                $('#945_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_946_1539825877_94" onclick="$('#945_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_946_1539825877_94').hide();
                $('#vhsjs_view_946_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="946_1539825877_94" id="945_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>Datacenters act as cloud-infrastructure to stakeholders across industry, government, and academia. To meet growing demand yet operate efficiently, datacenter operators employ increasingly more sophisticated scheduling systems, mechanisms, and policies. Although many scheduling techniques already exist, relatively little research has gone into the abstraction of the scheduling process itself, hampering design, tuning, and comparison of existing techniques. In this work, we propose a reference architecture for datacenter schedulers. The architecture follows five design principles: components with clearly distinct responsibilities, grouping of related components where possible, separation of mechanism from policy, scheduling as complex workflow, and hierarchical multi-scheduler structure. To demonstrate the validity of the reference architecture, we map to it state-of-the-art datacenter schedulers. We find scheduler-stages are commonly underspecified in peer-reviewed publications. Through trace-based simulation and real-world experiments, we show underspecification of scheduler-stages can lead to significant variations in performance.</blockquote></div></div></div></div><a href="includes/files/pap229s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap356"></a><div class="slot-title">Dynamically Negotiating Capacity Between On-Demand and Batch Clusters</div><div class="slot-authors">Feng Liu (University of Minnesota), Kate Keahey (Argonne National Laboratory), Pierre Riteau (University of Chicago), and Jon Weissman (University of Minnesota)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_948_1539825877_94" onclick="$('#vhsjs_view_948_1539825877_94').hide();
                $('#vhsjs_hide_948_1539825877_94').show();
                $('#947_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_948_1539825877_94" onclick="$('#947_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_948_1539825877_94').hide();
                $('#vhsjs_view_948_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="948_1539825877_94" id="947_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>In the era of rapid experimental expansion data analysis needs are rapidly outpacing the capabilities of small institutional clusters and looking to integrate HPC resources into their workflow. We propose one way of reconciling on-demand needs of experimental analytics with the batch managed HPC resources within a system that dynamically moves nodes between an on-demand cluster configured with cloud technology (OpenStack) and a traditional HPC cluster managed by a batch scheduler (Torque). We evaluate this system experimentally both in the context of real-life traces representing two years of a specific institutional need, and via experiments in the context of synthetic traces that capture generalized characteristics of potential batch and on-demand workloads. Our results for the real-life scenario show that our approach could reduce the current investment in on-demand infrastructure by 82% while at the same time improving the mean batch wait time almost by an order of magnitude (8x).</blockquote></div></div></div></div><a href="includes/files/pap356s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap365"></a><div class="slot-title">A Lightweight Model for Right-Sizing Master-Worker Applications</div><div class="slot-authors">Nathaniel Kremer-Herman, Benjamin Tovar, and Douglas Thain (University of Notre Dame)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_950_1539825877_94" onclick="$('#vhsjs_view_950_1539825877_94').hide();
                $('#vhsjs_hide_950_1539825877_94').show();
                $('#949_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_950_1539825877_94" onclick="$('#949_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_950_1539825877_94').hide();
                $('#vhsjs_view_950_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="950_1539825877_94" id="949_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>When running a parallel application at scale, a resource provisioning policy should minimize over-commitment (idle resources) and under-commitment (resource contention). However, users seldom know the quantity of resources to appropriately execute their application. Even with such knowledge, over- and under-commitment of resources may still occur because the application does not run in isolation. It shares resources  such as network and filesystems.<br><br>We formally define the capacity of a parallel application as the quantity of resources that may effectively be provisioned for the best  execution time in an environment.  We present a model to compute an estimate of the capacity of master-worker applications as they run based on execution and data-transfer times. We demonstrate this model with two bioinformatics workflows, a machine learning application, and one synthetic application.  Our results show the model correctly tracks the known value of capacity in scaling,  dynamic task behavior, and with improvements in task throughput.</blockquote></div></div></div></div><a href="includes/files/pap365s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Performance, Resiliency, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Resilience II</div><div class="slot-entry"><a name="pap392"></a><div class="slot-title">Lessons Learned from Memory Errors Observed Over the Lifetime of Cielo</div><div class="slot-authors">Scott Levy and Kurt B. Ferreira (Sandia National Laboratories), Nathan DeBardeleben (Los Alamos National Laboratory), Taniya Siddiqua and Vilas Sridharan (Advanced Micro Devices Inc), and Elisabeth Baseman (Los Alamos National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_952_1539825877_94" onclick="$('#vhsjs_view_952_1539825877_94').hide();
                $('#vhsjs_hide_952_1539825877_94').show();
                $('#951_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_952_1539825877_94" onclick="$('#951_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_952_1539825877_94').hide();
                $('#vhsjs_view_952_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="952_1539825877_94" id="951_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>Maintaining the performance of high-performance computing (HPC) applications as failures increase is a major challenge for next-generation extreme-scale systems. Recent research demonstrates that hardware failures are expected to become more common due to increased component counts, reduced device-feature sizes, and tightly-constrained power budgets. Few existing studies, however, have examined failures in the context of the entire lifetime of a single platform. In this paper, we analyze failure data collected over the entire lifetime of Cielo, a leadership-class HPC system. Our analysis reveals several key findings, including: (i) Cielo’s memory (DRAM and SRAM) exhibited no discernible aging effects; (ii) correctable memory faults are not predictive of future uncorrectable memory faults; (iii) developing more comprehensive logging facilities will improve failure analysis on future machines; (iv) continued advances will be required to ensure current failure mitigation techniques remain a viable option for future platforms.</blockquote></div></div></div></div><a href="includes/files/pap392s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap381"></a><div class="slot-title">Partial Redundancy in HPC Systems with Non-Uniform Node Reliabilities</div><div class="slot-authors">Zaeem Hussain, Taieb Znati, and Rami Melhem (University of Pittsburgh)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_954_1539825877_94" onclick="$('#vhsjs_view_954_1539825877_94').hide();
                $('#vhsjs_hide_954_1539825877_94').show();
                $('#953_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_954_1539825877_94" onclick="$('#953_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_954_1539825877_94').hide();
                $('#vhsjs_view_954_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="954_1539825877_94" id="953_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>We study the usefulness of partial redundancy in HPC message passing systems where individual node failure distributions are not identical. Prior research works on fault tolerance have generally assumed identical failure distributions for the nodes of the system. In such settings, partial replication has never been shown to outperform the two extremes (full and no-replication) for any significant range of node counts. We argue that partial redundancy may provide the best performance under the more realistic assumption of non-identical node failure distributions. We provide theoretical results on arranging nodes with different reliability values among replicas such that system reliability is maximized. Moreover, using system reliability to compute MTTI (mean-time-to-interrupt) and expected completion time of a partially replicated system, we numerically determine the optimal partial replication degree. Our results indicate that partial replication can be a more efficient alternative to full replication at system scales where Checkpoint/Restart alone is not sufficient.</blockquote></div></div></div></div><a href="includes/files/pap381s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap386"></a><div class="slot-title">Evaluating and Accelerating High-Fidelity Error Injection for HPC</div><div class="slot-authors">Chun-Kai Chang, Sangkug Lym, and Nicholas Kelly (University of Texas); Michael B. Sullivan (Nvidia Corporation); and Mattan Erez (University of Texas)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_956_1539825877_94" onclick="$('#vhsjs_view_956_1539825877_94').hide();
                $('#vhsjs_hide_956_1539825877_94').show();
                $('#955_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_956_1539825877_94" onclick="$('#955_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_956_1539825877_94').hide();
                $('#vhsjs_view_956_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="956_1539825877_94" id="955_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>We address two important concerns in the analysis of the behavior of applications in the presence of hardware errors: (1) when is it important to model how hardware faults lead to erroneous values (instruction-level errors) with high fidelity, as opposed to using simple bit-flipping models, and (2) how to enable fast high-fidelity error injection campaigns, in particular when error detectors are employed. We present and verify a new nested Monte Carlo methodology for evaluating high-fidelity gate-level fault models and error-detector coverage, which is orders of magnitude faster than current approaches. We use that methodology to demonstrate that, without detectors, simple error models suffice for evaluating errors in 9 HPC benchmarks.</blockquote></div></div></div></div><a href="includes/files/pap386s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Networks, Performance, Scientific Computing, State of the Practice, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Large Scale System Deployments</div><div class="slot-entry"><a name="pap277"></a><div class="slot-title">The Design, Deployment, and Evaluation of the CORAL Pre-Exascale Systems</div><div class="slot-authors">Sudharshan S. Vazhkudai (Oak Ridge National Laboratory); Bronis R. de Supinski (Lawrence Livermore National Laboratory); Arthur S. Bland and Al Geist (Oak Ridge National Laboratory); James Sexton and Jim Kahle (IBM); Christopher J. Zimmer, Scott Atchley, Sarp H. Oral, Don E. Maxwell, and Veronica G. Vergara Larrea (Oak Ridge National Laboratory); Adam Bertsch and Robin Goldstone (Lawrence Livermore National Laboratory); Wayne Joubert (Oak Ridge National Laboratory); Chris Chambreau (Lawrence Livermore National Laboratory); David Appelhans and Robert Blackmore (IBM); Ben Casses (Lawrence Livermore National Laboratory); George Chochia and Gene Davison (IBM); Matthew A. Ezell (Oak Ridge National Laboratory); Tom Gooding (IBM); Elsa Gonsiorowski (Lawrence Livermore National Laboratory); Leopold Grinberg, Bill Hanson, and Bill Hartner (IBM); Ian Karlin and Matthew L. Leininger (Lawrence Livermore National Laboratory); Dustin Leverman (Oak Ridge National Laboratory); Chris Marroquin (IBM); Adam Moody (Lawrence Livermore National Laboratory); Martin Ohmacht (IBM); Ramesh Pankajakshan (Lawrence Livermore National Laboratory); Fernando Pizzano (IBM); James H. Rogers (Oak Ridge National Laboratory); Bryan Rosenburg (IBM); Drew Schmidt, Mallikarjun Shankar, and Feiyi Wang (Oak Ridge National Laboratory); Py Watson (Lawrence Livermore National Laboratory); Bob Walkup (IBM); Lance D. Weems (Lawrence Livermore National Laboratory); and Junqi Yin (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_958_1539825877_94" onclick="$('#vhsjs_view_958_1539825877_94').hide();
                $('#vhsjs_hide_958_1539825877_94').show();
                $('#957_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_958_1539825877_94" onclick="$('#957_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_958_1539825877_94').hide();
                $('#vhsjs_view_958_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="958_1539825877_94" id="957_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>CORAL, the Collaboration of Oak Ridge, Argonne and Livermore, is fielding two similar IBM systems, Summit and Sierra, with NVIDIA GPUs that will replace the existing Titan and Sequoia systems. Summit and Sierra are currently ranked No. 1 and No. 3, respectively, on the Top500 list. We discuss the design and key differences of the systems. Our evaluation of the systems highlights the following. Applications that fit in HBM see the most benefit and may prefer more GPUs; however, for some applications, the CPU-GPU bandwidth is more important than the number of GPUs. The node-local burst buffer scales linearly, and can achieve a 4X improvement over the parallel file system for large jobs; smaller jobs, however, may benefit from writing directly to the PFS. Finally, several CPU, network and memory bound analytics and GPU-bound deep learning codes achieve up to a 11X and 79X speedup/node, respectively over Titan.</blockquote></div></div></div></div><a href="includes/files/pap277s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap449"></a><div class="slot-title">Best Practices and Lessons from Deploying and Operating a Sustained-Petascale System: The Blue Waters Experience</div><div class="slot-authors">Gregory H. Bauer, Brett Bode, Jeremy Enos, William T. Kramer, Scott Lathrop, Celso L. Mendes, and Roberto R. Sisneros (University of Illinois, National Center for Supercomputing Applications)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_960_1539825877_94" onclick="$('#vhsjs_view_960_1539825877_94').hide();
                $('#vhsjs_hide_960_1539825877_94').show();
                $('#959_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_960_1539825877_94" onclick="$('#959_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_960_1539825877_94').hide();
                $('#vhsjs_view_960_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="960_1539825877_94" id="959_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>Building and operating versatile extreme-scale computing systems that work productively for a range of frontier research domains present many challenges and opportunities. Solutions created, experiences acquired, and lessons learned, while rarely published, could drive the development of new methods and practices and raise the bar for all organizations supporting research, scholarship, and education. This paper describes the methods and procedures developed for deploying, supporting, and continuously improving the Blue Waters system and its services during the last five years. Being the first US sustained-petascale computing platform available to the open-science community, the Blue Waters project pioneered various unique practices that we are sharing to be adopted and further improved by the community. We present our support and service methodologies, and the leadership practices employed for ensuring that the system stays highly efficient and productive. We also provide the return on investment summaries related to deploying and operating the system.</blockquote></div></div></div></div><a href="includes/files/pap449s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap346"></a><div class="slot-title">Performance Evaluation of a Vector Supercomputer SX-Aurora TSUBASA</div><div class="slot-authors">Kazuhiko Komatsu (Tohoku University); Shintaro Momose, Yoko Isobe, Osamu Watanabe, and Akihiro Musa (Tohoku University, NEC Corporation); Mitsuo Yokokawa (Kobe University, NEC Corporation); Toshikazu Aoyama (NEC Corporation); and Masayuki Sato and Hiroaki Kobayashi (Tohoku University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_962_1539825877_94" onclick="$('#vhsjs_view_962_1539825877_94').hide();
                $('#vhsjs_hide_962_1539825877_94').show();
                $('#961_1539825877_94').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_962_1539825877_94" onclick="$('#961_1539825877_94').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_962_1539825877_94').hide();
                $('#vhsjs_view_962_1539825877_94').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="962_1539825877_94" id="961_1539825877_94" style="display: none"><div class="arrow-slidedown"><blockquote>A new SX-Aurora TSUBASA vector supercomputer has been released with a new system architecture and a new execution model to achieve high sustained performance, especially for memory-intensive applications. In SX-Aurora TSUBASA, the vector host (VH) of a standard x86 Linux node is attached to the vector engine (VE) of a newly developed vector processor.  An application is executed on the VE, and only system calls are offloaded to the VH. This new execution model can avoid redundant data transfers between a VH and a VE that can easily become a bottleneck in the conventional execution model. This paper examines the potential of SX-Aurora TSUBASA. First, the basic performance of SX-Aurora TSUBASA is clarified by evaluating benchmark programs. Then, the effectiveness of the new execution model is examined by using a microbenchmark.  Finally, the high potential of SX-Aurora TSUBASA is clarified through evaluations of practical applications.</blockquote></div></div></div></div><a href="includes/files/pap346s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Architectures, Compiler Analysis and Optimization, Floating Point, Performance, Precision, Programming Systems, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Arithmetic and Optimization</div><div class="slot-entry"><a name="pap431"></a><div class="slot-title">Associative Instruction Reordering to Alleviate Register Pressure</div><div class="slot-authors">Prashant Singh Rawat, Aravind Sukumaran-Rajam, and Atanas Rountev (Ohio State University); Fabrice Rastello (French Institute for Research in Computer Science and Automation (INRIA)); Louis-Noel Pouchet (Colorado State University); and P. Sadayappan (Ohio State University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_964_1539825877_95" onclick="$('#vhsjs_view_964_1539825877_95').hide();
                $('#vhsjs_hide_964_1539825877_95').show();
                $('#963_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_964_1539825877_95" onclick="$('#963_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_964_1539825877_95').hide();
                $('#vhsjs_view_964_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="964_1539825877_95" id="963_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>Register allocation is generally considered a practically solved problem. For most applications, the register allocation strategies in production compilers are very effective in controlling the number of loads/stores and register spills. However, existing register allocation strategies are not effective and result in excessive register spilling for computation patterns with a high degree of many-to-many data reuse, e.g., high-order stencils and tensor contractions.  We develop a source-to-source instruction reordering strategy that exploits the flexibility of reordering associative operations to alleviate register pressure.  The developed transformation module implements an adaptable strategy that can appropriately control the degree of instruction-level parallelism, while relieving register pressure.  The effectiveness of the approach is demonstrated through experimental results using multiple production compilers (GCC, Clang/LLVM) and target platforms (Intel Xeon Phi, and Intel x86 multi-core).</blockquote></div></div></div></div><a href="includes/files/pap431s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap464"></a><div class="slot-title">Harnessing GPU's Tensor Cores Fast FP16 Arithmetic to Speedup Mixed-Precision Iterative Refinement Solvers</div><div class="slot-authors">Azzam Haidar (University of Tennessee, Innovative Computing Laboratory); Stan Tomov and Jack Dongarra (University of Tennessee); and Nicholas Higham (University of Manchester, School of Mathematics)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_966_1539825877_95" onclick="$('#vhsjs_view_966_1539825877_95').hide();
                $('#vhsjs_hide_966_1539825877_95').show();
                $('#965_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_966_1539825877_95" onclick="$('#965_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_966_1539825877_95').hide();
                $('#vhsjs_view_966_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="966_1539825877_95" id="965_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>The use of low-precision arithmetic in computing methods has been a powerful tool to accelerate numerous scientific computing applications including Artificial Intelligence. We present an investigation showing that other HPC applications can harness this power too, and in particular, the general HPC problem of solving Ax = b, where A is a large dense matrix, and the solution is needed in FP64 accuracy. Our approach is based on the mixed-precision (FP16->FP64) iterative refinement technique – we generalize and extend prior advances into a framework, for which we develop architecture-specific algorithms and highly-tuned implementations where we show how the use of FP16-TC (tensor cores) arithmetic can provide up to 4X speedup and improve the energy consumption by a factor of 5 achieving 74 Gflop/Watt. This is due to the performance boost that the FP16 (Tensor Cores) provide and to its better accuracy that outperforms the classical FP16.</blockquote></div></div></div></div><a href="includes/files/pap464s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap503"></a><div class="slot-title">ADAPT: Algorithmic Differentiation Applied to Floating-Point Precision Tuning</div><div class="slot-authors">Harshitha Menon (Lawrence Livermore National Laboratory); Michael O. Lam (James Madison University, Lawrence Livermore National Laboratory); and Daniel Osei-Kuffuor, Markus Schordan, Scott Lloyd, Kathryn Mohror, and Jeffrey Hittinger (Lawrence Livermore National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_968_1539825877_95" onclick="$('#vhsjs_view_968_1539825877_95').hide();
                $('#vhsjs_hide_968_1539825877_95').show();
                $('#967_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_968_1539825877_95" onclick="$('#967_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_968_1539825877_95').hide();
                $('#vhsjs_view_968_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="968_1539825877_95" id="967_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>HPC applications extensively use floating point arithmetic operations to solve computational problems in various domains. Mixed precision computing, use of lowest precision data type sufficient to achieve a desired accuracy, have been explored to improve performance, reduce power consumption and data movement. Manually optimizing the program to use mixed precision is challenging. In this work, we present ADAPT, an approach for mixed precision analysis on HPC workloads while providing guarantees about the final output error. Our approach uses algorithmic differentiation to accurately estimate the output error for mixed precision configuration. ADAPT provides floating-point precision sensitivity of programs, which highlights regions of the code that that can potentially be converted to lower precision, is used to make algorithmic choices and develop mixed precision configurations. We evaluate ADAPT on six benchmarks and a proxy application and show that we are able to achieve a speedup of 1.2x on the proxy application, LULESH.</blockquote></div></div></div></div><a href="includes/files/pap503s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Applications, Graph Algorithms, Security, Tech Program Reg Pass</span><br /><div class="session-title">Graph Algorithms and Systems</div><div class="slot-entry"><a name="pap115"></a><div class="slot-title">iSpan: Parallel Identification of Strongly Connected Components with Spanning Trees</div><div class="slot-authors">Yuede Ji (George Washington University); Hang Liu (University of Massachusetts, Lowell); and H. Howie Huang (George Washington University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_970_1539825877_95" onclick="$('#vhsjs_view_970_1539825877_95').hide();
                $('#vhsjs_hide_970_1539825877_95').show();
                $('#969_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_970_1539825877_95" onclick="$('#969_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_970_1539825877_95').hide();
                $('#vhsjs_view_970_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="970_1539825877_95" id="969_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>Detecting strongly connected components (SCCs) in a directed graph is crucial for understanding the structure of graphs. Most real-world graphs have one large SCC that contains the majority of the vertices, and many small SCCs whose sizes are reversely proportional to the frequency of their occurrence. For both types of SCCs, current approaches that rely on depth or breadth first search (DFS or BFS) face the challenges of strict synchronization requirement and high computation cost. In this paper, we advocate a new paradigm of identifying SCCs with simple spanning trees, since SCC detection requires only the knowledge of connectivity among the vertices. We have developed a prototype called iSpan which consists of parallel, relaxed synchronization construction of spanning trees for detecting the large and small SCCs. The evaluations show that iSpan is able to significantly outperform current state-of-the-art DFS and BFS- based methods by average 18× and 4×, respectively.</blockquote></div></div></div></div><a href="includes/files/pap115s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap468"></a><div class="slot-title">Adaptive Anonymization of Data with b-Edge Covers</div><div class="slot-authors">Arif Khan (Pacific Northwest National Laboratory), Krzysztof Choromanski (Google LLC), Alex Pothen and S M Ferdous (Purdue University), and Mahantesh Halappanavar and Antonino Tumeo (Pacific Northwest National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_972_1539825877_95" onclick="$('#vhsjs_view_972_1539825877_95').hide();
                $('#vhsjs_hide_972_1539825877_95').show();
                $('#971_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_972_1539825877_95" onclick="$('#971_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_972_1539825877_95').hide();
                $('#vhsjs_view_972_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="972_1539825877_95" id="971_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>We explore the problem of sharing data that pertains to individuals with anonymity guarantees, where each user requires a desired level of privacy.  We propose the first shared-memory as well as distributed memory parallel algorithms for the adaptive anonymity problem that achieves this goal, and produces high quality anonymized datasets.  <br><br>The new algorithm is based on an optimization procedure that iteratively computes weights on the edges of a dissimilarity matrix, and at each iteration computes a minimum weighted b-Edge cover in the graph. We are able to solve adaptive anonymity problems with hundreds of thousands of instances and hundreds of features on a leadership-class supercomputer in under five minutes. Our algorithm scales up to 4K cores on a distributed memory supercomputer, while also providing good speedups on shared memory multiprocessors. On smaller problems, where an algorithm based on Belief Propagation is feasible, our algorithm is two orders of magnitude faster.</blockquote></div></div></div></div><a href="includes/files/pap468s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap167"></a><div class="slot-title">faimGraph: High Performance Management of Fully-Dynamic Graphs Under Tight Memory Constraints on the GPU</div><div class="slot-authors">Martin Winter and Daniel Mlakar (Graz University of Technology); Rhaleb Zayer and Hans-Peter Seidel (Max Planck Institute for Informatics); and Markus Steinberger (Graz University of Technology, Max Planck Institute for Informatics)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_974_1539825877_95" onclick="$('#vhsjs_view_974_1539825877_95').hide();
                $('#vhsjs_hide_974_1539825877_95').show();
                $('#973_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_974_1539825877_95" onclick="$('#973_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_974_1539825877_95').hide();
                $('#vhsjs_view_974_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="974_1539825877_95" id="973_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>In this paper, we present a fully-dynamic graph data structure for the Graphics Processing Unit (GPU). It delivers high update rates while keeping a low memory footprint using autonomous memory management directly on the GPU. The data structure is fully-dynamic, allowing not only for edge but also vertex updates. Performing the memory management on the GPU allows for fast initialization times and efficient update procedures without additional intervention or reallocation procedures from the host.  faimGraph is the first GPU graph framework that fully reclaims unused memory, permitting long time application with highly changing graph structures. Performance evaluations show that our approach outperforms that previous state-of-the-art in for all types of graph updates. Furthermore, evaluate algorithmic performance using a PageRank and a Static Triangle Counting (STC) implementation, demonstrating the suitability of the framework even for memory access intensive algorithms.</blockquote></div></div></div></div><a href="includes/files/pap167s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Linear Algebra, Memory, MPI, OpenMP, Programming Systems, Tools, Tech Program Reg Pass</span><br /><div class="session-title">Programming Systems Tools</div><div class="slot-entry"><a name="pap179"></a><div class="slot-title">Dynamic Data Race Detection for OpenMP Programs</div><div class="slot-authors">Yizi Gu and John Mellor-Crummey (Rice University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_976_1539825877_95" onclick="$('#vhsjs_view_976_1539825877_95').hide();
                $('#vhsjs_hide_976_1539825877_95').show();
                $('#975_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_976_1539825877_95" onclick="$('#975_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_976_1539825877_95').hide();
                $('#vhsjs_view_976_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="976_1539825877_95" id="975_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>Two concurrent accesses to a shared variable that are unordered by synchronization are said to be a data race if at least one access is a write. Data races cause shared memory parallel programs to behave unpredictably. This paper describes ROMP -- a tool for detecting data races in executions of scalable parallel applications that employ OpenMP for node-level parallelism. The complexity of OpenMP, which includes primitives for managing data environments, SPMD and SIMD parallelism, work sharing, tasking, mutual exclusion, and ordering, presents a formidable challenge for data race detection. ROMP is a hybrid data race detector that tracks accesses, access orderings, and mutual exclusion. Unlike other OpenMP race detectors, ROMP detects races with respect to logical parallelism rather than implementation threads. Experiments show that ROMP yields precise race reports for a broader set of OpenMP constructs than prior state-of-the-art race detectors.</blockquote></div></div></div></div><a href="includes/files/pap179s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap256"></a><div class="slot-title">ParSy: Inspection and Transformation of Sparse Matrix Computations for Parallelism</div><div class="slot-authors">Kazem Cheshmi (University of Toronto), Shoaib Kamil (Adobe Research), Michelle Mills Strout (University of Arizona), and Maryam Mehri Dehnavi (University of Toronto)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_978_1539825877_95" onclick="$('#vhsjs_view_978_1539825877_95').hide();
                $('#vhsjs_hide_978_1539825877_95').show();
                $('#977_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_978_1539825877_95" onclick="$('#977_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_978_1539825877_95').hide();
                $('#vhsjs_view_978_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="978_1539825877_95" id="977_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>In this work, we describe ParSy, a framework that uses a novel inspection strategy along with a simple code transformation to optimize parallel sparse algorithms for shared memory processors. Unlike existing approaches that can suffer from load imbalance and excessive synchronization, ParSy uses a novel task coarsening strategy to create well-balanced tasks that can execute in parallel, while maintaining locality of memory accesses. Code using the ParSy inspector and transformation outperforms existing highly-optimized sparse matrix algorithms such as Cholesky factorization on multi-core processors with speedups of 2.8× and 3.1× over the MKL Pardiso and PaStiX libraries respectively.</blockquote></div></div></div></div><a href="includes/files/pap256s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap382"></a><div class="slot-title">Detecting MPI Usage Anomalies via Partial Program Symbolic Execution</div><div class="slot-authors">Fangke Ye, Jisheng Zhao, and Vivek Sarkar (Georgia Institute of Technology)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_980_1539825877_95" onclick="$('#vhsjs_view_980_1539825877_95').hide();
                $('#vhsjs_hide_980_1539825877_95').show();
                $('#979_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_980_1539825877_95" onclick="$('#979_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_980_1539825877_95').hide();
                $('#vhsjs_view_980_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="980_1539825877_95" id="979_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>MPI is a message passing based programming model for distributed-memory parallelism that has been had been widely used for programming supercomputers for over 25 years. However, debugging and verification of MPI programs is widely recognized to be a deep technical challenge. This challenge is further exacerbated by a recent increase in the use of nonblocking MPI operations that bring new classes of bugs related to data races.<br><br>In this paper, we introduce a new MPI program debugging approach based on partial symbolic execution so as to avoid the false alarms inherent in the static analysis based methodology. Compared with the dynamic approach, our approach can be applied to incomplete programs and explore multiple execution paths, thereby bringing more flexibility and precision. By comparing with well known static/dynamic tools on real-world MPI applications, our approach shows same precision as the dynamic tool and avoids false positive produced by the static tool.</blockquote></div></div></div></div><a href="includes/files/pap382s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Applications, Cosmology, Data Analytics, Deep Learning, Machine Learning, Programming Systems, Storage, Visualization, Tech Program Reg Pass</span><br /><div class="session-title">Deep Learning</div><div class="slot-entry"><a name="pap425"></a><div class="slot-title">Exploring Flexible Communications for Streamlining DNN Ensemble Training Pipelines</div><div class="slot-authors">Randall Pittman, Hui Guan, and Xipeng Shen (North Carolina State University) and Seung-Hwan Lim and Robert M. Patton (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_982_1539825877_95" onclick="$('#vhsjs_view_982_1539825877_95').hide();
                $('#vhsjs_hide_982_1539825877_95').show();
                $('#981_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_982_1539825877_95" onclick="$('#981_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_982_1539825877_95').hide();
                $('#vhsjs_view_982_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="982_1539825877_95" id="981_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>Parallel training of a Deep Neural Network (DNN) ensemble on a cluster of nodes is a common practice to train multiple models in order to construct a model with a higher prediction accuracy. Existing ensemble training pipelines can perform a great deal of redundant operations, resulting in unnecessary CPU usage, or even poor pipeline performance.  In order to remove these redundancies, we need pipelines with more communication flexibility than existing DNN frameworks provide.<br><br>This project investigates a series of designs to improve pipeline flexibility and adaptivity, while also increasing performance. We implement our designs using Tensorflow with Horovod, and test it using several large DNNs. Our results show that the CPU time spent during training is reduced by 2-11X. Furthermore, our implementation can achieve up to 10X speedups when CPU core limits are imposed. Our best pipeline also reduces the average power draw of the ensemble training process by 5-16%.</blockquote></div></div></div></div><a href="includes/files/pap425s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap429"></a><div class="slot-title">CosmoFlow: Using Deep Learning to Learn the Universe at Scale</div><div class="slot-authors">Amrita Mathuriya (Intel Corporation); Deborah Bard (National Energy Research Scientific Computing Center (NERSC), Lawrence Berkeley National Laboratory); Pete Mendygral (Cray Inc); Lawrence Meadows (Intel Corporation); James Arnemann (University of California, Berkeley); Lei Shao (Intel Corporation); Siyu He (Carnegie Mellon University); Tuomas Karna (Intel Corporation); Diana Moise (Cray Inc); Simon J. Pennycook (Intel Corporation); Kristyn Maschhoff (Cray Inc); Jason Sewall and Nalini Kumar (Intel Corporation); Shirley Ho (Lawrence Berkeley National Laboratory, Carnegie Mellon University); Michael F. Ringenburg (Cray Inc); Mr Prabhat (Lawrence Berkeley National Laboratory, National Energy Research Scientific Computing Center (NERSC)); and Victor Lee (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_984_1539825877_95" onclick="$('#vhsjs_view_984_1539825877_95').hide();
                $('#vhsjs_hide_984_1539825877_95').show();
                $('#983_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_984_1539825877_95" onclick="$('#983_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_984_1539825877_95').hide();
                $('#vhsjs_view_984_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="984_1539825877_95" id="983_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>Deep learning is a promising tool to determine the physical model that describes our universe.   To handle the considerable computational cost of this problem, we present CosmoFlow: a highly scalable deep learning application built on top of the TensorFlow framework.<br><br>CosmoFlow uses efficient implementations of 3D convolution and pooling primitives, together with improvements in threading for many element-wise operations, to improve training performance on Intel Xeon Phi processors.  We also utilize the Cray PE Machine Learning Plugin for efficient scaling to multiple nodes. We demonstrate fully synchronous data-parallel training on 8192 nodes of Cori with 77% parallel efficiency, achieving 3.5 Pflop/s sustained performance. <br><br>To our knowledge, this is the first large-scale science application of the TensorFlow framework at supercomputer scale with fully-synchronous training. These enhancements enable us to process large 3D dark matter distribution and predict the cosmological parameters Omega_M, sigma_8 and N_s with unprecedented accuracy.</blockquote></div></div></div></div><a href="includes/files/pap429s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap322"></a><div class="slot-title">Anatomy of High-Performance Deep Learning Convolutions on SIMD Architectures</div><div class="slot-authors">Evangelos Georganas, Sasikanth Avancha, Kunal Banerjee, Dhiraj Kalamkar, Greg Henry, Hans Pabst, and Alexander Heinecke (Intel Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_986_1539825877_95" onclick="$('#vhsjs_view_986_1539825877_95').hide();
                $('#vhsjs_hide_986_1539825877_95').show();
                $('#985_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_986_1539825877_95" onclick="$('#985_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_986_1539825877_95').hide();
                $('#vhsjs_view_986_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="986_1539825877_95" id="985_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>Convolution layers are prevalent in many classes of deep neural networks, including Convolutional Neural Networks (CNNs) which provide state-of-the-art results for tasks like image recognition, neural machine translation, and speech recognition. The computationally expensive nature of a convolution operation has led to the proliferation of implementations including matrix-matrix multiplication formulation, and direct convolution primarily targeting GPUs. In this paper, we introduce direct convolution kernels for x86 architectures, in particular for Xeon and Xeon Phi systems, which are implemented via a dynamic compilation approach. Our JIT-based implementation shows close to theoretical peak performance, depending on the setting and the CPU architecture at hand. We additionally demonstrate how these JIT-optimized kernels can be integrated into a light-weight multi-node graph execution model. This illustrates that single- and multi-node runs yield high efficiencies and high image-throughputs  when executing state of the art image recognition tasks on CPUs.</blockquote></div></div></div></div><a href="includes/files/pap322s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Architectures, GPUs, Linear Algebra, Networks, Resiliency, Tech Program Reg Pass</span><br /><div class="session-title">Resilience III: GPUs</div><div class="slot-entry"><a name="pap247"></a><div class="slot-title">Optimizing Software-Directed Instruction Replication for GPU Error Detection</div><div class="slot-authors">Abdulrahman Mahmoud (University of Illinois) and Siva Kumar Sastry Hari, Michael B. Sullivan, Timothy Tsai, and Stephen W. Keckler (Nvidia Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_988_1539825877_95" onclick="$('#vhsjs_view_988_1539825877_95').hide();
                $('#vhsjs_hide_988_1539825877_95').show();
                $('#987_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_988_1539825877_95" onclick="$('#987_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_988_1539825877_95').hide();
                $('#vhsjs_view_988_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="988_1539825877_95" id="987_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>Application execution on safety-critical and high-performance computer systems must be resilient to transient errors. As GPUs become more pervasive in such systems, they must supplement ECC/parity for major storage structures with reliability techniques that cover more of the GPU hardware logic.  Instruction duplication has been explored for CPU resilience; however, it has never been studied in the context of GPUs, and it is unclear whether the performance and design choices it presents makes it a feasible GPU solution. This paper describes a practical methodology to employ instruction duplication for GPUs and identifies implementation challenges that can incur high overheads (69% on average). It explores GPU-specific software optimizations that trade fine-grained recoverability for performance. It also proposes simple ISA extensions with limited hardware changes and area costs to further improve performance, cutting the runtime overheads by more than half to an average of 30%.</blockquote></div></div></div></div><a href="includes/files/pap247s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap244"></a><div class="slot-title">Fault Tolerant One-Sided Matrix Decompositions on Heterogeneous Systems with GPUs</div><div class="slot-authors">Jieyang Chen, Hongbo Li, Sihuan Li, and Xin Liang (University of California, Riverside); Panruo Wu (University of Houston); Dingwen Tao (University of Alabama); Kaiming Ouyang, Yuanlai Liu, and Kai Zhao (University of California, Riverside); Qiang Guan (Kent State University); and Zizhong Chen (University of California, Riverside)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_990_1539825877_95" onclick="$('#vhsjs_view_990_1539825877_95').hide();
                $('#vhsjs_hide_990_1539825877_95').show();
                $('#989_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_990_1539825877_95" onclick="$('#989_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_990_1539825877_95').hide();
                $('#vhsjs_view_990_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="990_1539825877_95" id="989_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>Current algorithm-based fault tolerance (ABFT) approach for one-sided matrix decomposition on heterogeneous systems with GPUs have following limitations: (1) they do not provide sufficient protection as most of them only maintain checksum in one dimension; (2) their checking scheme is not efficient due to redundant checksum verifications; (3) they fail to protect PCIe communication; (4) the checksum calculation based on a special type of matrix multiplication is far from efficient. By overcoming the above limitations, we design an efficient ABFT approach providing stronger protection for one-sided matrix decomposition methods on heterogeneous systems. First, we provide full matrix protection by using checksums in two dimensions. Second, our checking scheme is more efficient by prioritizing the checksum verification according to the sensitivity of matrix operations to soft errors. Third, we protect PCIe communication by reordering checksum verifications and decomposition steps. Fourth, we accelerate the checksum calculation by 1.7x via better utilizing GPUs.</blockquote></div></div></div></div><a href="includes/files/pap244s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap430"></a><div class="slot-title">PRISM: Predicting Resilience of GPU Applications Using Statistical Methods</div><div class="slot-authors">Charu Kalra, Fritz Previlon, and Xiangyu Li (Northeastern University); Norman Rubin (Nvidia Corporation); and David Kaeli (Northeastern University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_992_1539825877_95" onclick="$('#vhsjs_view_992_1539825877_95').hide();
                $('#vhsjs_hide_992_1539825877_95').show();
                $('#991_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_992_1539825877_95" onclick="$('#991_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_992_1539825877_95').hide();
                $('#vhsjs_view_992_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="992_1539825877_95" id="991_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>As Graphics Processing Units (GPUs) become more pervasive in HPC and safety-critical domains, ensuring that GPU applications can be protected from data corruption grows in importance. Despite prior efforts to mitigate errors, we still lack a clear understanding of how resilient these applications are in the presence of transient faults.  Due to the random nature of these faults, predicting whether they will alter the program output is a challenging problem. In this paper, we build a framework named PRISM, which uses a systematic approach to predict failures in GPU programs. PRISM extracts micro-architecture agnostic features to characterize program resiliency, which serve as predictors in our statistical model. PRISM enables us to predict failures in applications without running exhaustive fault-injection campaigns on a GPU, thereby reducing the error estimation effort. PRISM can also be used to gain insight into potential architectural support required to improve the reliability of GPU applications.</blockquote></div></div></div></div><a href="includes/files/pap430s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Algorithms, Applications, Computational Physics, Scientific Computing, Tech Program Reg Pass</span><br /><div class="session-title">Astrophysics Applications</div><div class="slot-entry"><a name="pap239"></a><div class="slot-title">Phase Asynchronous AMR Execution for Productive and Performant Astrophysical Flows</div><div class="slot-authors">Muhammad Nufail Farooqi (Koc University); Tan Nguyen, Weiqun Zhang, Ann S. Almgren, and John Shalf (Lawrence Berkeley National Laboratory); and Didem Unat (Koc University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_994_1539825877_95" onclick="$('#vhsjs_view_994_1539825877_95').hide();
                $('#vhsjs_hide_994_1539825877_95').show();
                $('#993_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_994_1539825877_95" onclick="$('#993_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_994_1539825877_95').hide();
                $('#vhsjs_view_994_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="994_1539825877_95" id="993_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>Adaptive Mesh Refinement (AMR) is an approach to solving PDEs that reduces the computational and memory requirements at the expense of increased communication. Although adopting asynchronous execution can overcome communication issues, manually restructuring an AMR application to realize asynchrony is extremely complicated and hinders readability and long-term maintainability. To balance performance against productivity, we design a user-friendly API and adopt phase asynchronous execution model where all subgrids at an AMR level can be computed asynchronously. <br><br>We apply the phase asynchrony to transform a real-world AMR application, CASTRO, which solves multicomponent compressible hydrodynamic equations for astrophysical flows. We evaluate the performance and programming effort required to use our carefully designed API and execution model for transitioning large legacy codes from synchronous to asynchronous execution up to 278,528 Intel-KNL cores. CASTRO is about 100K lines of code but less than 0.2% code changes are required to achieve significant performance improvement.</blockquote></div></div></div></div><a href="includes/files/pap239s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap294"></a><div class="slot-title">Computing Planetary Interior Normal Modes with a Highly Parallel Polynomial Filtering Eigensolver</div><div class="slot-authors">Jia Shi (Rice University), Ruipeng Li (Lawrence Livermore National Laboratory), Yuanzhe Xi and Yousef Saad (University of Minnesota), and Maarten V. de Hoop (Rice University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_996_1539825877_95" onclick="$('#vhsjs_view_996_1539825877_95').hide();
                $('#vhsjs_hide_996_1539825877_95').show();
                $('#995_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_996_1539825877_95" onclick="$('#995_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_996_1539825877_95').hide();
                $('#vhsjs_view_996_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="996_1539825877_95" id="995_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>A highly parallel algorithm has been developed and exploited to compute the planetary normal modes of the elastic-gravitational system, which is approximated via the mixed finite element method on unstructured tetrahedral meshes. The eigenmodes of the relevant generalized eigenvalue problem were extracted by a Lanczos approach combined with polynomial filtering. In contrast with the standard shift-and-invert and the full-mode coupling algorithms, the polynomial filtering technique is ideally suited for solving large-scale 3-D interior eigenvalue problems since it significantly enhances the memory and computational efficiency without loss of accuracy.  The parallel efficiency and scalability of this approach are demonstrated on Stampede2 at the Texas Advanced Computing Center. To our knowledge, this is the first time that the direct calculation of the normal modes of 3-D strongly heterogeneous planets, in particular, Earth and Mars, is made feasible via a combination of multiple matrix-free methods and a separation of the essential spectra.</blockquote></div></div></div></div><a href="includes/files/pap294s4-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">Paper</span><span class="type-track-spacer">&nbsp;&middot;&nbsp;</span><span class="program-track">Architectures, Data Management, File Systems, Networks, State of the Practice, System Software, Workflows, Tech Program Reg Pass</span><br /><div class="session-title">File Systems: Data Movement and Provenance</div><div class="slot-entry"><a name="pap407"></a><div class="slot-title">Dac-Man: Data Change Management for Scientific Datasets on HPC Systems</div><div class="slot-authors">Devarshi Ghoshal, Lavanya Ramakrishnan, and Deborah Agarwal (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_998_1539825877_95" onclick="$('#vhsjs_view_998_1539825877_95').hide();
                $('#vhsjs_hide_998_1539825877_95').show();
                $('#997_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_998_1539825877_95" onclick="$('#997_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_998_1539825877_95').hide();
                $('#vhsjs_view_998_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="998_1539825877_95" id="997_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>Scientific data is growing rapidly and often changes due to instrument configurations, software updates, or quality assessments. These changes in datasets can result in significant waste of compute and storage resources on HPC systems as downstream pipelines are reprocessed. Data changes need to be detected, tracked, and analyzed for understanding the impact of data change, managing data provenance, and making efficient and effective decisions about reprocessing and use of HPC resources. Existing methods for identifying and capturing change are often manual, domain-specific, and error-prone and do not scale to large scientific datasets. In this paper, we describe the design and implementation of Dac-Man framework, which identifies, captures, and manages change in large scientific datasets, and enables plug-in of domain-specific change analysis with minimal user effort. Our evaluations show that it can retrieve file changes from directories containing millions of files and terabytes of data in less than a minute.</blockquote></div></div></div></div><a href="includes/files/pap407s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap521"></a><div class="slot-title">Stacker: An Autonomic Data Movement Engine for Extreme-Scale Data Staging-Based In Situ Workflows</div><div class="slot-authors">Pradeep Subedi, Philip Davis, and Shaohua Duan (Rutgers University); Scott Klasky (Oak Ridge National Laboratory); Hemanth Kolla (Sandia National Laboratories); and Manish Parashar (Rutgers University)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_1000_1539825877_95" onclick="$('#vhsjs_view_1000_1539825877_95').hide();
                $('#vhsjs_hide_1000_1539825877_95').show();
                $('#999_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_1000_1539825877_95" onclick="$('#999_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_1000_1539825877_95').hide();
                $('#vhsjs_view_1000_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="1000_1539825877_95" id="999_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>Data staging and in situ workflows are being explored extensively as an approach to address data-related costs at very large scales. However, the impact of emerging storage architectures (e.g., deep memory hierarchies and burst buffers) upon data staging solutions remains a challenge. In this paper, we investigate how burst buffers can be effectively used by data staging solutions, for example, as a persistence storage tier of the memory hierarchy. Furthermore, we use machine learning based prefetching techniques to move data between the storage levels in an autonomous manner. We also present Stacker, a prototype of the proposed solutions implemented within the Data\-Spaces data staging service, and experimentally evaluate its performance and scalability using the S3D combustion workflow on current leadership class platforms. Our experiments demonstrate that Stacker achieves low latency, high volume data-staging with low overhead as compared to in-memory staging services for production scientific workflows.</blockquote></div></div></div></div><a href="includes/files/pap521s4-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="pap206"></a><div class="slot-title">A Year in the Life of a Parallel File System</div><div class="slot-authors">Glenn K. Lockwood (Lawrence Berkeley National Laboratory), Shane Snyder (Argonne National Laboratory), Teng Wang and Suren Byna (Lawrence Berkeley National Laboratory), Philip Carns (Argonne National Laboratory), and Nicholas J. Wright (Lawrence Berkeley National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_1002_1539825877_95" onclick="$('#vhsjs_view_1002_1539825877_95').hide();
                $('#vhsjs_hide_1002_1539825877_95').show();
                $('#1001_1539825877_95').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_1002_1539825877_95" onclick="$('#1001_1539825877_95').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_1002_1539825877_95').hide();
                $('#vhsjs_view_1002_1539825877_95').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="1002_1539825877_95" id="1001_1539825877_95" style="display: none"><div class="arrow-slidedown"><blockquote>I/O performance is a critical aspect of data-intensive scientific computing.  We seek to advance the state of the practice in understanding and diagnosing I/O performance issues through investigation of a comprehensive I/O performance data set that captures a full year of production storage activity at two leadership-scale computing facilities.  We demonstrate techniques to identify regions of interest, perform focused investigations of both long-term trends and transient anomalies, and uncover the contributing factors that lead to performance fluctuation.<br><br>We find that a year in the life of a parallel file system is comprised of distinct regions of long-term performance variation in addition to short-term performance transients.  We demonstrate how systematic identification of these performance regions, combined with comprehensive analysis, allows us to isolate the factors contributing to different performance maladies at different time scales.  From this, we present specific lessons learned and important considerations for HPC storage practitioners.</blockquote></div></div></div></div><a href="includes/files/pap206s4-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="area-section"><div class="centered"><a name="other"></a><div class="section-title">Other</div></div><div class="section-entry"><div class="session-entry"><span class="session-event-type">ACM Gordon Bell Finalist</span><br /><div class="session-title">Gordon Bell Prize Finalist #1</div><div class="slot-entry"><a name="gb102"></a><div class="slot-title">A Fast Scalable Implicit Solver for Nonlinear Time-Evolution Earthquake City Problem on Low-Ordered Unstructured Finite Elements with Artificial Intelligence and Transprecision Computing</div><div class="slot-authors">Tsuyoshi Ichimura, Kohei Fujita, and Takuma Yamaguchi (University of Tokyo); Akira Naruse (Nvidia Corporation); Jack C. Wells (Oak Ridge National Laboratory); Thomas C. Schulthess (Swiss National Supercomputing Centre); Tjerk P. Straatsma and Christopher J. Zimmer (Oak Ridge National Laboratory); Maxime Martinasso (Swiss National Supercomputing Centre); and Kengo Nakajima, Muneo Hori, and Lalith Maddegedara (University of Tokyo)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_1004_1539825877_96" onclick="$('#vhsjs_view_1004_1539825877_96').hide();
                $('#vhsjs_hide_1004_1539825877_96').show();
                $('#1003_1539825877_96').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_1004_1539825877_96" onclick="$('#1003_1539825877_96').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_1004_1539825877_96').hide();
                $('#vhsjs_view_1004_1539825877_96').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="1004_1539825877_96" id="1003_1539825877_96" style="display: none"><div class="arrow-slidedown"><blockquote>To address problems that occur due to earthquakes in urban areas, we propose a method that utilizes artificial intelligence (AI) and transprecision computing to accelerate a nonlinear dynamic low-order unstructured finite-element solver. The AI is used to improve the convergence of iterative solver leading to 5.56-fold reduction in arithmetic count from a standard solver, and FP16-FP21-FP32-FP64 computing is used to accelerate the sparse matrix-vector product kernel, which demonstrated 71.4% peak FP64 performance on Summit. This is 25.3 times faster than a standard solver and 3.99 times faster than the state-of-the-art SC14 Gordon Bell Finalist solver. Furthermore, the proposed solver demonstrated high scalability (88.8% on the K computer and 89.5% on Piz Daint), leading to 14.7% peak FP64 performance on 4096 nodes of Summit. The proposed approach utilizing AI and FP16 arithmetic has implications for accelerating other implicit solvers used for earthquake city simulations as well as various fields.</blockquote></div></div></div></div><a href="includes/files/gb102s3-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="gb103"></a><div class="slot-title">167-PFlops Deep Learning for Electron Microscopy: From Learning Physics to Atomic Manipulation</div><div class="slot-authors">Robert M. Patton, J. Travis Johnston, Steven R. Young, Catherine D. Schuman, Don D. March, Thomas E. Potok, Derek C. Rose, Seung-Hwan Lim, Thomas P. Karnowski, Maxim A. Ziatdinov, and Sergei V. Kalinin (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_1006_1539825877_96" onclick="$('#vhsjs_view_1006_1539825877_96').hide();
                $('#vhsjs_hide_1006_1539825877_96').show();
                $('#1005_1539825877_96').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_1006_1539825877_96" onclick="$('#1005_1539825877_96').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_1006_1539825877_96').hide();
                $('#vhsjs_view_1006_1539825877_96').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="1006_1539825877_96" id="1005_1539825877_96" style="display: none"><div class="arrow-slidedown"><blockquote>An artificial intelligence system called MENNDL, which used 25,200 Nvidia Volta GPUs on Oak Ridge National Laboratory’s Summit machine, automatically designed an optimal deep learning network in order to extract structural information from raw atomic-resolution microscopy data. In a few hours, MENNDL creates and evaluates millions of networks using a scalable, parallel, asynchronous genetic algorithm augmented with a support vector machine to automatically find a superior deep learning network topology and hyper-parameter set than a human expert can find in months. For the application of electron microscopy, the system furthers the goal of improving our understanding of the electron-beam-matter interactions and real-time image-based feedback, which enables a huge step beyond human capacity toward nanofabricating materials automatically. MENNDL has been scaled to the 4,200 available nodes of Summit achieving a measured 152.5 PFlops, with an estimated sustained performance of 167 PFlops when the entire machine is available.</blockquote></div></div></div></div><a href="includes/files/gb103s3-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="gb105"></a><div class="slot-title">Exascale Deep Learning for Climate Analytics</div><div class="slot-authors">Thorsten Kurth (Lawrence Berkeley National Laboratory), Sean Treichler and Joshua Romero (Nvidia Corporation), Mayur Mudigonda (Lawrence Berkeley National Laboratory), Nathan Luehr and Everett Phillips (Nvidia Corporation), Ankur Mahesh (Lawrence Berkeley National Laboratory), Michael Matheson (Oak Ridge National Laboratory), Jack Deslippe (Lawrence Berkeley National Laboratory), Massimiliano Fatica (Nvidia Corporation), Mr Prabhat (Lawrence Berkeley National Laboratory), and Michael Houston (Nvidia Corporation)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_1008_1539825877_96" onclick="$('#vhsjs_view_1008_1539825877_96').hide();
                $('#vhsjs_hide_1008_1539825877_96').show();
                $('#1007_1539825877_96').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_1008_1539825877_96" onclick="$('#1007_1539825877_96').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_1008_1539825877_96').hide();
                $('#vhsjs_view_1008_1539825877_96').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="1008_1539825877_96" id="1007_1539825877_96" style="display: none"><div class="arrow-slidedown"><blockquote>We extract pixel-level masks of extreme weather patterns using variants of Tiramisu and DeepLabv3+ neural networks. We describe improvements to the software frameworks, input pipeline, and the network training algorithms necessary to efficiently scale deep learning on the Piz Daint and Summit systems. The Tiramisu network scales to 5300 P100 GPUs with a sustained throughput of 21.0 PF/s and parallel efficiency of 79.0%. DeepLabv3+ scales up to 27360 V100 GPUs with a sustained throughput of 325.8 PF/s and a parallel efficiency of 90.7% in single precision. By taking advantage of the FP16 Tensor Cores, a half-precision version of the DeepLabv3+ network achieves a peak and sustained throughput of 1.13 EF/s and 999.0 PF/s respectively.</blockquote></div></div></div></div><a href="includes/files/gb105s3-file1.pdf" target="_blank">pdf</a><br /></div></div><div class="session-entry"><span class="session-event-type">ACM Gordon Bell Finalist</span><br /><div class="session-title">Gordon Bell Prize Finalist #2</div><div class="slot-entry"><a name="gb101"></a><div class="slot-title">Simulating the Weak Death of the Neutron in a Femtoscale Universe with Near-Exascale Computing</div><div class="slot-authors">Evan Berkowitz (Forschungszentrum Juelich); M.A. Clark (Nvidia Corporation); Arjun Gambhir (Lawrence Livermore National Laboratory, Lawrence Berkeley National Laboratory); Ken McElvain (University of California, Berkeley; Lawrence Berkeley National Laboratory); Amy Nicholson (University of North Carolina); Enrico Rinaldi (RIKEN BNL Research Center, Lawrence Berkeley National Laboratory); Pavlos Vranas (Lawrence Livermore National Laboratory, Lawrence Berkeley National Laboratory); André Walker-Loud (Lawrence Berkeley National Laboratory, Lawrence Livermore National Laboratory); Chia Cheng Chang (Lawrence Berkeley National Laboratory, RIKEN); Bálint Joó (Thomas Jefferson National Accelerator Facility); Thorsten Kurth (Lawrence Berkeley National Laboratory); and Kostas Orginos (College of William & Mary, Thomas Jefferson National Accelerator Facility)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_1010_1539825877_96" onclick="$('#vhsjs_view_1010_1539825877_96').hide();
                $('#vhsjs_hide_1010_1539825877_96').show();
                $('#1009_1539825877_96').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_1010_1539825877_96" onclick="$('#1009_1539825877_96').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_1010_1539825877_96').hide();
                $('#vhsjs_view_1010_1539825877_96').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="1010_1539825877_96" id="1009_1539825877_96" style="display: none"><div class="arrow-slidedown"><blockquote>The fundamental particle theory called Quantum Chromodynamics (QCD) dictates everything about protons and neutrons, from their intrinsic properties to interactions that bind them into atomic nuclei.  Quantities that cannot be fully resolved through experiment, such as the neutron lifetime (whose precise value is important for the existence of light-atomic elements that make the sun shine and life possible), may be understood through numerical solutions to QCD.  We directly solve QCD using Lattice Gauge Theory and calculate nuclear observables such as neutron lifetime.  We have developed an improved algorithm that exponentially decreases the time-to-solution and applied it on the new CORAL supercomputers, Sierra and Summit.  We use run-time autotuning to distribute GPU resources, achieving 20% performance at low node count.  We also developed optimal application mapping through a job manager, which allows CPU and GPU jobs to be interleaved, yielding 15% of peak performance when deployed across large fractions of CORAL.</blockquote></div></div></div></div><a href="includes/files/gb101s3-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="gb106"></a><div class="slot-title">ShenTu: Processing Multi-Trillion Edge Graphs on Millions of Cores in Seconds</div><div class="slot-authors">Heng Lin (Tsinghua University, Fma Technology); Xiaowei Zhu (Tsinghua University, Qatar Computing Research Institute); Bowen Yu (Tsinghua University); Xiongchao Tang (Tsinghua University, Qatar Computing Research Institute); Wei Xue and Wenguang Chen (Tsinghua University); Lufei Zhang (State Key Laboratory of Mathematical Engineering and Advanced Computing); Torsten Hoefler (ETH Zurich); Xiaosong Ma (Qatar Computing Research Institute); Xin Liu (National Research Centre of Parallel Computer Engineering and Technology); Weimin Zheng (Tsinghua University); and Jingfang Xu (Beijing Sogou Technology Development Company)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_1012_1539825877_96" onclick="$('#vhsjs_view_1012_1539825877_96').hide();
                $('#vhsjs_hide_1012_1539825877_96').show();
                $('#1011_1539825877_96').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_1012_1539825877_96" onclick="$('#1011_1539825877_96').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_1012_1539825877_96').hide();
                $('#vhsjs_view_1012_1539825877_96').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="1012_1539825877_96" id="1011_1539825877_96" style="display: none"><div class="arrow-slidedown"><blockquote>Graphs are an important abstraction used in many scientific fields. With the magnitude of graph-structured data constantly increasing, effective data analytics requires efficient and scalable graph processing systems. Although HPC systems have long been used for scientific computing, people have only recently started to assess their potential for graph processing, a workload with inherent load imbalance, lack of locality, and access irregularity. We propose ShenTu, the first general-purpose graph processing framework that can efficiently utilize an entire petascale system to process multi-trillion edge graphs in seconds. ShenTu embodies four key innovations: hardware specializing, supernode routing, on-chip sorting, and degree-aware messaging, which together enable its unprecedented performance and scalability. It can traverse an unprecedented 70-trillion-edge graph in seconds. Furthermore, ShenTu enables the processing of a spam detection problem on a 12-trillion edge Internet graph, making it possible to identify trustworthy and spam web pages directly at the fine-grained page level.</blockquote></div></div></div></div><a href="includes/files/gb106s3-file1.pdf" target="_blank">pdf</a><br /></div><div class="slot-entry"><a name="gb104"></a><div class="slot-title">Attacking the Opioid Epidemic: Determining the Epistatic and Pleiotropic Genetic Architectures for Chronic Pain and Opioid Addiction</div><div class="slot-authors">Wayne Joubert (Oak Ridge National Laboratory); Deborah Weighill (Oak Ridge National Laboratory, University of Tennessee); David Kainer (Oak Ridge National Laboratory); Sharlee Climer (University of Missouri, St Louis); Amy Justice (Yale University, US Department of Veterans Affairs); Kjiersten Fagnan (Lawrence Berkeley National Laboratory, US Department of Energy Joint Genome Institute); and Daniel Jacobson (Oak Ridge National Laboratory)</div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_1014_1539825877_96" onclick="$('#vhsjs_view_1014_1539825877_96').hide();
                $('#vhsjs_hide_1014_1539825877_96').show();
                $('#1013_1539825877_96').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_1014_1539825877_96" onclick="$('#1013_1539825877_96').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_1014_1539825877_96').hide();
                $('#vhsjs_view_1014_1539825877_96').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="1014_1539825877_96" id="1013_1539825877_96" style="display: none"><div class="arrow-slidedown"><blockquote>We describe the CoMet application for large-scale epistatic Genome-Wide Association Studies (eGWAS) and pleiotropy studies. High performance is attained by transforming the underlying vector comparison methods into highly performant generalized distributed dense linear algebra operations. The 2-way and 3-way Proportional Similarity metric and Custom Correlation Coefficient are implemented using native or adapted GEMM kernels optimized for GPU architectures. By aggressive overlapping of communications, transfers and computations, high efficiency with respect to single GPU kernel performance is maintained up to the full Titan and Summit systems. Nearly 300 quadrillion element comparisons per second and over 2.3 mixed precision ExaOps are reached on Summit by use of Tensor Core hardware on the Nvidia Volta GPUs. Performance is four to five orders of magnitude beyond comparable state of the art. CoMet is currently being used in projects ranging from bioenergy to clinical genomics, including for the genetics of chronic pain and opioid addiction.</blockquote></div></div></div></div><a href="includes/files/gb104s3-file1.pdf" target="_blank">pdf</a><br /></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div></td></tr></table></div></div><div class="created-date righted">Created 2018-10-17 20:24</div></body></html>

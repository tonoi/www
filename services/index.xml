<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Services on tonoi株式会社</title>
    <link>/services/</link>
    <description>Recent content in Services on tonoi株式会社</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-JP</language>
    <lastBuildDate>Fri, 05 Oct 2018 14:40:08 +0900</lastBuildDate>
    
	<atom:link href="/services/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Service3</title>
      <link>/services/service3/</link>
      <pubDate>Fri, 05 Oct 2018 14:40:08 +0900</pubDate>
      
      <guid>/services/service3/</guid>
      <description>宅配便の再配達が運送業者の負担になっています。家庭向け宅配ボックスもありますが、本人確認や盗難の危険性もあり配達者側に抵抗があるようです。ドアホンと宅配ボックスを組み合わせ、本人確認をしながら遠隔地で荷物を受け取れる仕組みを開発しました。&amp;rdquo; detail = &amp;ldquo;宅配便の再配達が運送業者の負担になっています。家庭向け宅配ボックスもありますが、本人確認や盗難の危険性もあり配達者側に抵抗があるようです。ドアホンと宅配ボックスを組み合わせ、本人確認をしながら遠隔地で荷物を受け取れる仕組みを開発しました。ホームゲートウェイ宅配ボックス (実証実験)対面型ドアホン (POC)配布用資料対面型ドアホン (開発機)</description>
    </item>
    
    <item>
      <title>Service2</title>
      <link>/services/service2/</link>
      <pubDate>Fri, 05 Oct 2018 14:40:01 +0900</pubDate>
      
      <guid>/services/service2/</guid>
      <description>S3Cubed 様の依頼により、NYRIAD の思想をもとにした新しい RAID Storage Software の企画・設計を行いました。大容量・高速化とともに、低消費電力を狙ったものです。</description>
    </item>
    
    <item>
      <title>Service1</title>
      <link>/services/service1/</link>
      <pubDate>Thu, 04 Oct 2018 18:17:40 +0900</pubDate>
      
      <guid>/services/service1/</guid>
      <description>Deep Learning の勘所現代の AI の代表格である Deep Learning は従来手法ではプログラミングしづらかった案件を Neural Network を利用して自動化します。案件に合わせて Neural Network を学習させ推論可能な AI ロジックを生成するためには膨大な元データが必要となります。元データギガ級のファイルを多数集めたテラ ~ ペタ級の容量。対象空間の前後情報である時間軸を含む四次元強化学習GPUが持つ 16 ~ 32ギガのメモリ上に Neural Network を構築し元データをストリームプロセッシングして学習。空間を再現する三次元推論エンジンUSB メモリに入る程度のキロ ~ メガ級の AI ロジック を用いてビデオ動画などを処理。イメージセンサーの二次元元データはテラ ~ ペタ級と非常に大容量であるため、強化学習用の GPU サーバーを設置したクラウドにネットワークで転送することが出来ず、Amazon Snowball のようなストレージ機器にデータを保存し宅配便でクラウドに輸送しています。
強化学習に対して推論エンジンは軽い処理です。学習済みの AI ロジックは数キロ程度の大きさであり、組み込みプロセッサーに AI ロジックをすべて読み込んだ状態で自動運転などの AI 処理が可能です。ただしレスポンスが必要とされるため、軽量ながら並列性の高い専用のチップや FPGA などが利用されます。強化学習と推論エンジンの負荷の違いを Deep Learing の非対称性と呼びます。組み込みにおける AI の高速化技術が各社より発表されておりますが、推論エンジン側であるため元々高速化しやすい特性があります。一方、強化学習は推論に比べ高次元の処理 (Hi-Dimenstion Compute) であり高速化はハードウェアの強化に頼っている事が現状です。
高次元処理である強化学習を高速化するため、Hybrid Computingを利用して分散処理をさせるミドルウェアを弊社にて開発中です。
動作イメージ   動作環境Mirantis Cloud Platform (MCP)Intel Xeon processor または nVidia GPU構成要素OpenCL AI ロジックを HC で分散処理するための Python インターフェースCeph ストレージ上で分散処理を実現する MCP 拡張キット優位点開発の容易性: Dockerによるマイクロサービスの分散化と異なり、一つのアプリケーションの中でAI処理を記述できるため、分散処理のための余分な開発が不要となる。 高効率処理: 実行コードをデータ場所に転送して実行するため、データ通信が削減され消費電力が下がり、処理速度が向上する。 多種環境対応: OpenCLで記述するためGPUやIntel CPUなど複数環境で動作 超高速処理: AI処理がOSを介さずに直接ストレージにアクセス する特別な高速化を実装提供時期 (見込み)2018&amp;frasl;11 弊社サーバー上における動作デモ</description>
    </item>
    
  </channel>
</rss>